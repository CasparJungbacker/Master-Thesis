@article{choquetteNVIDIAA100Tensor2021,
  title = {{{NVIDIA A100 Tensor Core GPU}}: {{Performance}} and {{Innovation}}},
  shorttitle = {{{NVIDIA A100 Tensor Core GPU}}},
  author = {Choquette, J. and Gandhi, W. and Giroux, O. and Stam, N. and Krashinsky, R.},
  year = {2021},
  journal = {IEEE Micro},
  volume = {41},
  number = {2},
  pages = {29--35},
  issn = {0272-1732},
  doi = {10.1109/MM.2021.3061394},
  abstract = {NVIDIA A100 Tensor Core GPU is NVIDIA's latest flagship GPU. It has been designed with many new innovative features to provide performance and capabilities for HPC, AI, and data analytics workloads. Feature enhancements include a Third-Generation Tensor Core, new asynchronous data movement and programming model, enhanced L2 cache, HBM2 DRAM, and third-generation NVIDIA NVLink I/O. \textcopyright{} 1981-2012 IEEE.},
  langid = {english},
  keywords = {A100,C++20,CUDA,Deep Learning,GPU,NVLink,Tensor Core},
  file = {/Users/caspar/Zotero/storage/R42XCV4P/display.html}
}

@article{costaFFTbasedFinitedifferenceSolver2018,
  title = {A {{FFT-based}} Finite-Difference Solver for Massively-Parallel Direct Numerical Simulations of Turbulent Flows},
  author = {Costa, Pedro},
  year = {2018},
  month = oct,
  journal = {Computers \& Mathematics with Applications},
  volume = {76},
  number = {8},
  pages = {1853--1862},
  issn = {0898-1221},
  doi = {10.1016/j.camwa.2018.07.034},
  urldate = {2023-05-25},
  abstract = {We present an efficient solver for massively-parallel direct numerical simulations of incompressible turbulent flows. The method uses a second-order, finite-volume pressure-correction scheme, where the pressure Poisson equation is solved with the method of eigenfunction expansions. This approach allows for very efficient FFT-based solvers in problems with different combinations of homogeneous pressure boundary conditions. Our algorithm explores all combinations of pressure boundary conditions valid for such a solver, in a single, general framework. The method is implemented in a 2D pencil-like domain decomposition, which enables efficient massively-parallel simulations. The implementation was validated against different canonical flows, and its computational performance was examined. Excellent strong scaling performance up to 104 cores is demonstrated for a domain with 109 spatial degrees of freedom, corresponding to a very small wall-clock time/time step. The resulting tool, CaNS, has been made freely available and open-source.},
  langid = {english},
  keywords = {Direct numerical simulations,Fast Poisson solver,High-performance computing,Turbulent flows},
  file = {/Users/caspar/Zotero/storage/WCD9NJJK/Costa - 2018 - A FFT-based finite-difference solver for massively.pdf;/Users/caspar/Zotero/storage/K8TVR7Q5/S089812211830405X.html}
}

@article{elsterNvidiaHopperGPU2022,
  title = {Nvidia {{Hopper GPU}} and {{Grace CPU Highlights}}},
  author = {Elster, Anne C. and Haugdahl, Tor A.},
  year = {2022},
  month = mar,
  journal = {Computing in Science \& Engineering},
  volume = {24},
  number = {2},
  pages = {95--100},
  issn = {1558-366X},
  doi = {10.1109/MCSE.2022.3163817},
  abstract = {At GTC 2022, Nvidia announced a new product family that aims to cover from small enterprise workloads through exascale high performance computing (HPC) and trillion-parameter AI models. This column highlights the most interesting features of their new Hopper graphical processing unit (GPU) and Grace central processing unit (CPU) computer chips and the Hopper product family. We also discuss some of the history behind Nvidia technologies and their most useful features for computational scientists, such as the Hopper DPX dynamic programming (DP) instruction set, increased number of SMs, and FP 8 tensor core availability. Also included are descriptions of the new Hopper Clustered SMs architecture and updated NVSwitch technologies that integrate their new ARM-based Grace CPU.},
  keywords = {Central processing units,Companies,Graphics processing units,Product design,Product development},
  file = {/Users/caspar/Zotero/storage/SXV5H73R/Elster and Haugdahl - 2022 - Nvidia Hopper GPU and Grace CPU Highlights.pdf;/Users/caspar/Zotero/storage/KL3DKPXC/stamp.html}
}

@techreport{FFTW97,
  title = {The Fastest {{Fourier}} Transform in the West},
  author = {Frigo, Matteo and Johnson, Steven G.},
  year = {1997},
  month = sep,
  number = {MIT-LCS-TR-728},
  institution = {{Massachusetts Institute of Technology}}
}

@article{herdmanAcceleratingHydrocodesOpenACC2012,
  title = {Accelerating Hydrocodes with {{OpenACC}}, {{OpenCL}} and {{CUDA}}},
  author = {Herdman, J. A. and Gaudin, W. P. and {McIntosh-Smith}, S. and Boulton, M. and Beckingsale, D. A. and Mallinson, A. C. and Jarvis, S. A.},
  year = {2012},
  journal = {Proceedings - 2012 SC Companion: High Performance Computing, Networking Storage and Analysis, SCC 2012},
  pages = {465--471},
  doi = {10.1109/SC.COMPANION.2012.66},
  abstract = {Hardware accelerators such as GPGPUs are becoming increasingly common in HPC platforms and their use is widely recognised as being one of the most promising approaches for reaching exascale levels of performance.Large HPC centres, such as AWE, have made huge investments in maintaining their existing scientific software codebases, the vast majority of which were not designed to effectively utilise accelerator devices.Consequently, HPC centres will have to decide how to develop their existing applications to take best advantage of future HPC system architectures.Given limited development and financial resources, it is unlikely that all potential approaches will be evaluated for each application. We are interested in how this decision making can be improved, and this work seeks to directly evaluate three candidate technologies - -OpenACC, OpenCL and CUDA - -in terms of performance, programmer productivity, and portability using a recently developed Lagrangian-Eulerian explicit hydrodynamics mini-Application. We find that OpenACC is an extremely viable programming model for accelerator devices, improving programmer productivity and achieving better performance than OpenCL and CUDA. \textcopyright{} 2012 IEEE.},
  isbn = {9780769549569},
  keywords = {CUDA,High Performance Computing,Hydrodynamics,OpenACC,OpenCL},
  file = {/Users/caspar/Zotero/storage/KFESNJ6F/Herdman et al. - 2012 - Accelerating hydrocodes with OpenACC, OpenCL and C.pdf}
}

@article{heusFormulationDutchAtmospheric2010,
  title = {Formulation of the {{Dutch Atmospheric Large-Eddy Simulation}} ({{DALES}}) and Overview of Its Applications},
  author = {Heus, T. and Van Heerwaarden, C. C. and Jonker, H. J. J. and Pier Siebesma, A. and Axelsen, S. and Van Den Dries, K. and Geoffroy, O. and Moene, A. F. and Pino, D. and De Roode, S. R. and {Vil{\`a}-Guerau De Arellano}, J.},
  year = {2010},
  month = sep,
  journal = {Geoscientific Model Development},
  volume = {3},
  number = {2},
  pages = {415--444},
  issn = {1991-9603},
  doi = {10.5194/gmd-3-415-2010},
  urldate = {2023-05-23},
  abstract = {Abstract. The current version of the Dutch Atmospheric Large-Eddy Simulation (DALES) is presented. DALES is a large-eddy simulation code designed for studies of the physics of the atmospheric boundary layer, including convective and stable boundary layers as well as cloudy boundary layers. In addition, DALES can be used for studies of more specific cases, such as flow over sloping or heterogeneous terrain, and dispersion of inert and chemically active species. This paper contains an extensive description of the physical and numerical formulation of the code, and gives an overview of its applications and accomplishments in recent years.},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/B4CER9JR/Heus et al. - 2010 - Formulation of the Dutch Atmospheric Large-Eddy Si.pdf}
}

@article{masonLargeEddySimulationConvective1989,
  title = {Large-{{Eddy Simulation}} of the {{Convective Atmospheric Boundary Layer}}},
  author = {Mason, P. J.},
  year = {1989},
  month = jun,
  journal = {Journal of the Atmospheric Sciences},
  volume = {46},
  number = {11},
  pages = {1492--1516},
  publisher = {{American Meteorological Society}},
  issn = {0022-4928, 1520-0469},
  doi = {10.1175/1520-0469(1989)046<1492:LESOTC>2.0.CO;2},
  urldate = {2023-05-26},
  abstract = {Abstract Large-eddy simulations of a free convective atmospheric boundary layer with an overlying capping inversion are considered. Attention is given to the dependence of the results upon the various factors influencing the simulation: the subgrid model, the domain size, and the mesh resolution. By providing artificial constraints upon the convection the results also provide extra insight into the underlying dynamics. The gross features of the boundary layer, such as the overall energy budget, are not sensitive to the details of the simulations but a number of important factors are revealed. It has been found that near the surface the subgrid diffusivity must be larger than is usually supposed, in order for the vertical velocity skewness to have the correct sign. This region of the flow has a significant subgrid-scale heat flux and it seems that the subgrid model requires improvement in such cases. A revised model which under statically unstable conditions allows the mixing-length of the subgrid-scale turbulence to depend on the flow stability is found to give improved results. The domain size and mesh spacings have a significant influence upon the results and need a setting which allows resolution of the main, freely occurring scales of motion. The entrainment at the capping inversion is remarkable in its insensitivity to all factors. Finally, the higher resolution simulations provide a detailed view of the flow structure of the convective boundary layer. Downdrafts cover a large fraction of the surface area, and near the surface the flow converges into smaller areas comprising long narrow regions of updrafts. The plumes which penetrate through the depth of the boundary layer to the inversion mainly occur over the inter-sections of these long narrow regions of updrafts.},
  chapter = {Journal of the Atmospheric Sciences},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/E65ZN4TT/Mason - 1989 - Large-Eddy Simulation of the Convective Atmospheri.pdf}
}

@misc{nvidiaNVIDIANsightSystems,
  title = {{{NVIDIA Nsight Systems}}},
  author = {{NVIDIA}},
  urldate = {2023-05-25}
}

@misc{RefactoringOptimizingWRF,
  title = {Refactoring and {{Optimizing WRF Model}} on {{Sunway TaihuLight}} | {{Proceedings}} of the 48th {{International Conference}} on {{Parallel Processing}}},
  urldate = {2023-05-10},
  howpublished = {https://dl.acm.org/doi/10.1145/3337821.3337923},
  file = {/Users/caspar/Zotero/storage/KMETDULS/Refactoring and Optimizing WRF Model on Sunway Tai.pdf;/Users/caspar/Zotero/storage/453T3UG2/3337821.html}
}

@article{schalkwijkHighPerformanceSimulationsTurbulent2012,
  title = {High-{{Performance Simulations}} of {{Turbulent Clouds}} on a {{Desktop PC}}: {{Exploiting}} the {{GPU}}},
  shorttitle = {High-{{Performance Simulations}} of {{Turbulent Clouds}} on a {{Desktop PC}}},
  author = {Schalkwijk, Jer{\^o}me and Griffith, Eric J. and Post, Frits H. and Jonker, Harm J. J.},
  year = {2012},
  month = mar,
  journal = {Bulletin of the American Meteorological Society},
  volume = {93},
  number = {3},
  pages = {307--314},
  issn = {1520-0477},
  doi = {10.1175/BAMS-D-11-00059.1},
  urldate = {2023-05-22},
  abstract = {No abstract available.},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/VI465BX5/Schalkwijk et al. - 2012 - High-Performance Simulations of Turbulent Clouds o.pdf}
}

@article{schumannFastFourierTransforms1988,
  title = {Fast {{Fourier}} Transforms for Direct Solution of Poisson's Equation with Staggered Boundary Conditions},
  author = {Schumann, Ulrich and Sweet, Roland A},
  year = {1988},
  month = mar,
  journal = {Journal of Computational Physics},
  volume = {75},
  number = {1},
  pages = {123--137},
  issn = {0021-9991},
  doi = {10.1016/0021-9991(88)90102-7},
  urldate = {2023-05-24},
  abstract = {This paper describes pre- and postprocessing algorithms used to incorporate the fast Fourier transform (FFT) into the solution of finite difference approximations to multi-dimensional Poisson's equation on a staggered grid where the boundary is located midway between two grid points. All frequently occurring boundary conditions (Neumann, Dirichlet, or cyclic) are considered including the combination of staggered Neumann boundary condition on one side with nonstaggered Dirichlet boundary condition on the other side. Experiences from implementing these algorithms in vectorized coding in Fortran subroutines are reported.},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/MZZ46CWY/Schumann and Sweet - 1988 - Fast Fourier transforms for direct solution of poi.pdf;/Users/caspar/Zotero/storage/PE69MKIZ/0021999188901027.html}
}

@article{siebesmaLargeEddySimulation2003,
  title = {A {{Large Eddy Simulation Intercomparison Study}} of {{Shallow Cumulus Convection}}},
  author = {Siebesma, A. Pier and Bretherton, Christopher S. and Brown, Andrew and Chlond, Andreas and Cuxart, Joan and Duynkerke, Peter G. and Jiang, Hongli and Khairoutdinov, Marat and Lewellen, David and Moeng, Chin-Hoh and Sanchez, Enrique and Stevens, Bjorn and Stevens, David E.},
  year = {2003},
  month = may,
  journal = {Journal of the Atmospheric Sciences},
  volume = {60},
  number = {10},
  pages = {1201--1219},
  doi = {10.1175/1520-0469(2003)60<1201:ALESIS>2.0.CO;2}
}
