@book{aamondtGeneralPurposeGraphicsProcessing2018,
  title = {General-{{Purpose Graphics Processing Architecture}}},
  author = {Aamondt, Tor M. and Wai Lun Fung, Wilson and Rogers, Timothy G.},
  year = {2018},
  series = {Synthesis {{Lectures}} on {{Computer Architecture}}},
  number = {44},
  publisher = {{Morgan \& Claypool}},
  isbn = {978-1-62705-923-7}
}

@inproceedings{amdahlValiditySingleProcessor1967,
  title = {Validity of the Single Processor Approach to Achieving Large Scale Computing Capabilities},
  booktitle = {Proceedings of the {{April}} 18-20, 1967, Spring Joint Computer Conference on - {{AFIPS}} '67 ({{Spring}})},
  author = {Amdahl, Gene M.},
  year = {1967},
  pages = {483},
  publisher = {{ACM Press}},
  address = {{Atlantic City, New Jersey}},
  doi = {10.1145/1465482.1465560},
  url = {http://portal.acm.org/citation.cfm?doid=1465482.1465560},
  urldate = {2023-09-05},
  langid = {english}
}

@techreport{amdAMDEPYC7H12,
  title = {{{AMD EPYC}}™ {{7H12}} Specifications},
  author = {{AMD}},
  url = {https://www.amd.com/en/product/9131}
}

@misc{amdHipFFT2023,
  title = {{{hipFFT}}},
  author = {{AMD}},
  year = {2023},
  url = {https://hipfft.readthedocs.io/en/rocm-5.7.0/}
}

@misc{amdRocFFT2023,
  title = {{{rocFFT}}},
  author = {{AMD}},
  year = {2023},
  url = {https://rocfft.readthedocs.io/en/rocm-5.7.0/index.html}
}

@inproceedings{antaoOffloadingSupportOpenMP2016,
  title = {Offloading {{Support}} for {{OpenMP}} in {{Clang}} and {{LLVM}}},
  booktitle = {2016 {{Third Workshop}} on the {{LLVM Compiler Infrastructure}} in {{HPC}} ({{LLVM-HPC}})},
  author = {Antao, Samuel F. and Bataev, Alexey and Jacob, Arpith C. and Bercea, Gheorghe-Teodor and Eichenberger, Alexandre E. and Rokos, Georgios and Martineau, Matt and Jin, Tian and Ozen, Guray and Sura, Zehra and Chen, Tong and Sung, Hyojin and Bertolli, Carlo and O'Brien, Kevin},
  year = {2016},
  month = nov,
  pages = {1--11},
  publisher = {{IEEE}},
  address = {{Salt Lake City, UT, USA}},
  doi = {10.1109/LLVM-HPC.2016.006},
  url = {http://ieeexplore.ieee.org/document/7839455/},
  urldate = {2024-02-21},
  isbn = {978-1-5090-3878-7}
}

@incollection{arakawaComputationalDesignBasic1977,
  title = {Computational {{Design}} of the {{Basic Dynamical Processes}} of the {{UCLA General Circulation Model}}},
  booktitle = {Methods in {{Computational Physics}}: {{Advances}} in {{Research}} and {{Applications}}},
  author = {Arakawa, Akio and Lamb, Vivian R.},
  year = {1977},
  volume = {17},
  pages = {173--265},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-0-12-460817-7.50009-4},
  url = {https://linkinghub.elsevier.com/retrieve/pii/B9780124608177500094},
  urldate = {2023-11-28},
  isbn = {978-0-12-460817-7},
  langid = {english}
}

@inproceedings{bardinaImprovedSubgridscaleModels1980,
  title = {Improved Subgrid-Scale Models for Large-Eddy Simulation},
  booktitle = {13th {{Fluid}} and {{PlasmaDynamics Conference}}},
  author = {Bardina, J. and Ferziger, J. and Reynolds, W.},
  year = {1980},
  month = jul,
  publisher = {{American Institute of Aeronautics and Astronautics}},
  address = {{Snowmass,CO,U.S.A.}},
  doi = {10.2514/6.1980-1357},
  url = {https://arc.aiaa.org/doi/10.2514/6.1980-1357},
  urldate = {2024-01-09},
  langid = {english}
}

@book{barlasMulticoreGPUProgramming2023,
  title = {Multicore and {{GPU}} Programming: An Integrated Approach},
  shorttitle = {Multicore and {{GPU}} Programming},
  author = {Barlas, Gerassimos},
  year = {2023},
  edition = {Second edition},
  publisher = {{Morgan Kaufmann Publishers}},
  address = {{Cambridge, MA}},
  isbn = {978-0-12-814120-5},
  lccn = {QA76.642 .B375 2023},
  keywords = {Graphics processing units,Parallel programming (Computer science),Programming}
}

@article{bauerQuietRevolutionNumerical2015,
  title = {The Quiet Revolution of Numerical Weather Prediction},
  author = {Bauer, Peter and Thorpe, Alan and Brunet, Gilbert},
  year = {2015},
  month = sep,
  journal = {Nature},
  volume = {525},
  number = {7567},
  pages = {47--55},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14956},
  url = {https://www.nature.com/articles/nature14956},
  urldate = {2024-01-29},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/KLSFFQQS/Bauer et al. - 2015 - The quiet revolution of numerical weather predicti.pdf}
}

@article{beareIntercomparisonLargeEddySimulations2006,
  title = {An {{Intercomparison}} of {{Large-Eddy Simulations}} of the {{Stable Boundary Layer}}},
  author = {Beare, Robert J. and Macvean, Malcolm K. and Holtslag, Albert A. M. and Cuxart, Joan and Esau, Igor and Golaz, Jean-Christophe and Jimenez, Maria A. and Khairoutdinov, Marat and Kosovic, Branko and Lewellen, David and Lund, Thomas S. and Lundquist, Julie K. and Mccabe, Anne and Moene, Arnold F. and Noh, Yign and Raasch, Siegfried and Sullivan, Peter},
  year = {2006},
  month = feb,
  journal = {Boundary-Layer Meteorology},
  volume = {118},
  number = {2},
  pages = {247--272},
  issn = {0006-8314, 1573-1472},
  doi = {10.1007/s10546-004-2820-6},
  url = {http://link.springer.com/10.1007/s10546-004-2820-6},
  urldate = {2024-02-01},
  langid = {english}
}

@article{boingInfluenceSubcloudLayer2012,
  title = {Influence of the {{Subcloud Layer}} on the {{Development}} of a {{Deep Convective Ensemble}}},
  author = {B{\"o}ing, Steven J. and Jonker, Harm J. J. and Siebesma, A. Pier and Grabowski, Wojciech W.},
  year = {2012},
  month = sep,
  journal = {Journal of the Atmospheric Sciences},
  volume = {69},
  number = {9},
  pages = {2682--2698},
  issn = {0022-4928, 1520-0469},
  doi = {10.1175/JAS-D-11-0317.1},
  url = {https://journals.ametsoc.org/doi/10.1175/JAS-D-11-0317.1},
  urldate = {2024-02-12},
  abstract = {Abstract             The rapid transition from shallow to deep convection is investigated using large-eddy simulations. The role of cold pools, which occur due to the evaporation of rainfall, is explored using a series of experiments in which their formation is suppressed. A positive feedback occurs: the presence of cold pools promotes deeper, wider, and more buoyant clouds with higher precipitation rates, which in turn lead to stronger cold pools. To assess the influence of the subcloud layer on the development of deep convection, the coupling between the cloud layer and the subcloud layer is explored using Lagrangian particle trajectories. As shown in previous studies, particles that enter clouds have properties that deviate significantly from the mean state. However, the differences between particles that enter shallow and deep clouds are remarkably small in the subcloud layer, and become larger in the cloud layer, indicating different entrainment rates. The particles that enter the deepest clouds also correspond to the widest cloud bases, which points to the importance of convective organization within the subcloud layer.},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/JY7FKJLC/Böing et al. - 2012 - Influence of the Subcloud Layer on the Development.pdf}
}

@article{bruntonMachineLearningFluid2020,
  title = {Machine {{Learning}} for {{Fluid Mechanics}}},
  author = {Brunton, Steven L. and Noack, Bernd R. and Koumoutsakos, Petros},
  year = {2020},
  month = jan,
  journal = {Annual Review of Fluid Mechanics},
  volume = {52},
  number = {1},
  pages = {477--508},
  issn = {0066-4189, 1545-4479},
  doi = {10.1146/annurev-fluid-010719-060214},
  url = {https://www.annualreviews.org/doi/10.1146/annurev-fluid-010719-060214},
  urldate = {2024-01-24},
  abstract = {The field of fluid mechanics is rapidly advancing, driven by unprecedented volumes of data from experiments, field measurements, and large-scale simulations at multiple spatiotemporal scales. Machine learning (ML) offers a wealth of techniques to extract information from data that can be translated into knowledge about the underlying fluid mechanics. Moreover, ML algorithms can augment domain knowledge and automate tasks related to flow control and optimization. This article presents an overview of past history, current developments, and emerging opportunities of ML for fluid mechanics. We outline fundamental ML methodologies and discuss their uses for understanding, modeling, optimizing, and controlling fluid flows. The strengths and limitations of these methods are addressed from the perspective of scientific inquiry that considers data as an inherent part of modeling, experiments, and simulations. ML provides a powerful information-processing framework that can augment, and possibly even transform, current lines of fluid mechanics research and industrial applications.},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/UHYUKINQ/Brunton et al. - 2020 - Machine Learning for Fluid Mechanics.pdf}
}

@book{chandrasekaranOpenACCProgrammersConcepts2018,
  title = {{{OpenACC}} for Programmers: Concepts and Strategies},
  shorttitle = {{{OpenACC}} for Programmers},
  editor = {Chandrasekaran, Sunita and Juckeland, Guido},
  year = {2018},
  publisher = {{Addison-Wesley}},
  address = {{Boston}},
  isbn = {978-0-13-469434-4},
  langid = {english},
  annotation = {OCLC: 1003645622}
}

@misc{choiBeatingHartWorld2022,
  title = {The {{Beating Hart}} of the {{World}}'s {{First Exascale Supercomputer}}},
  author = {Choi, Charles Q.},
  year = {2022},
  month = jun,
  journal = {IEEE Spectrum},
  url = {https://spectrum.ieee.org/frontier-exascale-supercomputer}
}

@article{choquetteNVIDIAA100Tensor2021,
  title = {{{NVIDIA A100 Tensor Core GPU}}: {{Performance}} and {{Innovation}}},
  shorttitle = {{{NVIDIA A100 Tensor Core GPU}}},
  author = {Choquette, J. and Gandhi, W. and Giroux, O. and Stam, N. and Krashinsky, R.},
  year = {2021},
  journal = {IEEE Micro},
  volume = {41},
  number = {2},
  pages = {29--35},
  issn = {0272-1732},
  doi = {10.1109/MM.2021.3061394},
  abstract = {NVIDIA A100 Tensor Core GPU is NVIDIA's latest flagship GPU. It has been designed with many new innovative features to provide performance and capabilities for HPC, AI, and data analytics workloads. Feature enhancements include a Third-Generation Tensor Core, new asynchronous data movement and programming model, enhanced L2 cache, HBM2 DRAM, and third-generation NVIDIA NVLink I/O. {\copyright} 1981-2012 IEEE.},
  langid = {english},
  keywords = {A100,C++20,CUDA,Deep Learning,GPU,NVLink,Tensor Core},
  file = {/Users/caspar/Zotero/storage/R42XCV4P/display.html}
}

@article{chorinNumericalSolutionNavierStokes1967,
  title = {The Numerical Solution of the {{Navier-Stokes}} Equations for an Incompressible Fluid},
  author = {Chorin, Alexandre Joel},
  year = {1967},
  journal = {Bulletin of the American Mathematical Society},
  volume = {73},
  number = {6},
  pages = {928--931},
  issn = {0273-0979, 1088-9485},
  doi = {10.1090/S0002-9904-1967-11853-6},
  url = {https://www.ams.org/bull/1967-73-06/S0002-9904-1967-11853-6/},
  urldate = {2023-12-06},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/UB4U6L6U/Chorin - 1967 - The numerical solution of the Navier-Stokes equati.pdf}
}

@inproceedings{corbalanEnergyOptimizationAnalysis2020,
  title = {Energy {{Optimization}} and {{Analysis}} with {{EAR}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Cluster Computing}} ({{CLUSTER}})},
  author = {Corbalan, Julita and Alonso, Lluis and Aneas, Jordi and Brochard, Luigi},
  year = {2020},
  month = sep,
  pages = {464--472},
  publisher = {{IEEE}},
  address = {{Kobe, Japan}},
  doi = {10.1109/CLUSTER49012.2020.00067},
  url = {https://ieeexplore.ieee.org/document/9229570/},
  urldate = {2023-09-05},
  isbn = {978-1-72816-677-3},
  file = {/Users/caspar/Zotero/storage/9R2PK5MA/Corbalan et al. - 2020 - Energy Optimization and Analysis with EAR.pdf;/Users/caspar/Zotero/storage/G5UWX5XX/Corbalan et al. - 2020 - Energy Optimization and Analysis with EAR.pdf}
}

@article{costaFFTbasedFinitedifferenceSolver2018,
  title = {A {{FFT-based}} Finite-Difference Solver for Massively-Parallel Direct Numerical Simulations of Turbulent Flows},
  author = {Costa, Pedro},
  year = {2018},
  month = oct,
  journal = {Computers \& Mathematics with Applications},
  volume = {76},
  number = {8},
  pages = {1853--1862},
  issn = {0898-1221},
  doi = {10.1016/j.camwa.2018.07.034},
  url = {https://www.sciencedirect.com/science/article/pii/S089812211830405X},
  urldate = {2023-05-25},
  abstract = {We present an efficient solver for massively-parallel direct numerical simulations of incompressible turbulent flows. The method uses a second-order, finite-volume pressure-correction scheme, where the pressure Poisson equation is solved with the method of eigenfunction expansions. This approach allows for very efficient FFT-based solvers in problems with different combinations of homogeneous pressure boundary conditions. Our algorithm explores all combinations of pressure boundary conditions valid for such a solver, in a single, general framework. The method is implemented in a 2D pencil-like domain decomposition, which enables efficient massively-parallel simulations. The implementation was validated against different canonical flows, and its computational performance was examined. Excellent strong scaling performance up to 104 cores is demonstrated for a domain with 109 spatial degrees of freedom, corresponding to a very small wall-clock time/time step. The resulting tool, CaNS, has been made freely available and open-source.},
  langid = {english},
  keywords = {Direct numerical simulations,Fast Poisson solver,High-performance computing,Turbulent flows},
  file = {/Users/caspar/Zotero/storage/WCD9NJJK/Costa - 2018 - A FFT-based finite-difference solver for massively.pdf;/Users/caspar/Zotero/storage/K8TVR7Q5/S089812211830405X.html}
}

@article{costaGPUAccelerationCaNS2021,
  title = {{{GPU}} Acceleration of {{CaNS}} for Massively-Parallel Direct Numerical Simulations of Canonical Fluid Flows},
  author = {Costa, Pedro and Phillips, Everett and Brandt, Luca and Fatica, Massimiliano},
  year = {2021},
  month = jan,
  journal = {Computers \& Mathematics with Applications},
  volume = {81},
  pages = {502--511},
  issn = {08981221},
  doi = {10.1016/j.camwa.2020.01.002},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0898122120300092},
  urldate = {2023-05-26},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/BP4P5UHX/Costa et al. - 2021 - GPU acceleration of CaNS for massively-parallel di.pdf}
}

@article{dallyEvolutionGraphicsProcessing2021,
  title = {Evolution of the {{Graphics Processing Unit}} ({{GPU}})},
  author = {Dally, William J. and Keckler, Stephen W. and Kirk, David B.},
  year = {2021},
  month = nov,
  journal = {IEEE Micro},
  volume = {41},
  number = {6},
  pages = {42--51},
  issn = {0272-1732, 1937-4143},
  doi = {10.1109/MM.2021.3113475},
  url = {https://ieeexplore.ieee.org/document/9623445/},
  urldate = {2024-02-13}
}

@article{deardorffStratocumuluscappedMixedLayers1980,
  title = {Stratocumulus-Capped Mixed Layers Derived from a Three-Dimensional Model},
  author = {Deardorff, James W.},
  year = {1980},
  month = jun,
  journal = {Boundary-Layer Meteorology},
  volume = {18},
  number = {4},
  pages = {495--527},
  issn = {0006-8314, 1573-1472},
  doi = {10.1007/BF00119502},
  url = {http://link.springer.com/10.1007/BF00119502},
  urldate = {2024-01-05},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/Z4PRF4VU/Deardorff - 1980 - Stratocumulus-capped mixed layers derived from a t.pdf}
}

@article{deardorffThreedimensionalNumericalStudy1974,
  title = {Three-Dimensional Numerical Study of the Height and Mean Structure of a Heated Planetary Boundary Layer},
  author = {Deardorff, J. W.},
  year = {1974},
  month = aug,
  journal = {Boundary-Layer Meteorology},
  volume = {7},
  number = {1},
  pages = {81--106},
  issn = {0006-8314, 1573-1472},
  doi = {10.1007/BF00224974},
  url = {http://link.springer.com/10.1007/BF00224974},
  urldate = {2024-02-01},
  langid = {english}
}

@article{debruineExplicitAerosolCloud2019,
  title = {Explicit Aerosol{\textendash}Cloud Interactions in the {{Dutch Atmospheric Large-Eddy Simulation}} Model {{DALES4}}.1-{{M7}}},
  author = {De Bruine, Marco and Krol, Maarten and {Vil{\`a}-Guerau De Arellano}, Jordi and R{\"o}ckmann, Thomas},
  year = {2019},
  month = dec,
  journal = {Geoscientific Model Development},
  volume = {12},
  number = {12},
  pages = {5177--5196},
  issn = {1991-9603},
  doi = {10.5194/gmd-12-5177-2019},
  url = {https://gmd.copernicus.org/articles/12/5177/2019/},
  urldate = {2024-02-11},
  abstract = {Abstract. Large-eddy simulation (LES) models are an excellent tool to improve our understanding of aerosol{\textendash}cloud interactions (ACI). We introduce a prognostic aerosol scheme with multiple aerosol species in the Dutch Atmospheric Large-Eddy Simulation model (DALES), especially focused on simulating the impact of cloud microphysical processes on the aerosol population. The numerical treatment of aerosol activation is a crucial element for simulating both cloud and aerosol characteristics. Two methods are implemented and discussed: an explicit activation scheme based on {$\kappa$}-K{\"o}hler theory and a more classic approach using updraught strength. Sample model simulations are based on the Rain in Shallow Cumulus over the Ocean (RICO) campaign, characterized by rapidly precipitating warm-phase shallow cumulus clouds. We find that in this pristine ocean environment virtually all aerosol mass in cloud droplets is the result of the activation process, while in-cloud scavenging is relatively inefficient. Despite the rapid formation of precipitation, most of the in-cloud aerosol mass is returned to the atmosphere by cloud evaporation. The strength of aerosol processing through subsequent cloud cycles is found to be particularly sensitive to the activation scheme and resulting cloud characteristics. However, the precipitation processes are considerably less sensitive. Scavenging by precipitation is the dominant source for in-rain aerosol mass. About half of the in-rain aerosol reaches the surface, while the rest is released by evaporation of falling precipitation. The effect of cloud microphysics on the average aerosol size depends on the balance between the evaporation of clouds and rain and ultimate removal by precipitation. Analysis of typical aerosol size associated with the different microphysical processes shows that aerosols resuspended by cloud evaporation have a radius that is only 5\,\% to 10\,\% larger than the originally activated aerosols. In contrast, aerosols released by evaporating precipitation are an order of magnitude larger.},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/9S3UUEQP/De Bruine et al. - 2019 - Explicit aerosol–cloud interactions in the Dutch A.pdf}
}

@article{dufresneAssessmentPrimarySources2008,
  title = {An {{Assessment}} of the {{Primary Sources}} of {{Spread}} of {{Global Warming Estimates}} from {{Coupled Atmosphere}}{\textendash}{{Ocean Models}}},
  author = {Dufresne, Jean-Louis and Bony, Sandrine},
  year = {2008},
  month = oct,
  journal = {Journal of Climate},
  volume = {21},
  number = {19},
  pages = {5135--5144},
  issn = {1520-0442, 0894-8755},
  doi = {10.1175/2008JCLI2239.1},
  url = {http://journals.ametsoc.org/doi/10.1175/2008JCLI2239.1},
  urldate = {2024-01-31},
  abstract = {Abstract             Climate feedback analysis constitutes a useful framework for comparing the global mean surface temperature responses to an external forcing predicted by general circulation models (GCMs). Nevertheless, the contributions of the different radiative feedbacks to global warming (in equilibrium or transient conditions) and their comparison with the contribution of other processes (e.g., the ocean heat uptake) have not been quantified explicitly. Here these contributions from the classical feedback analysis framework are defined and quantified for an ensemble of 12 third phase of the Coupled Model Intercomparison Project (CMIP3)/Intergovernmental Panel on Climate Change (IPCC) Fourth Assessment Report (AR4) coupled atmosphere{\textendash}ocean GCMs. In transient simulations, the multimodel mean contributions to global warming associated with the combined water vapor{\textendash}lapse-rate feedback, cloud feedback, and ocean heat uptake are comparable. However, intermodel differences in cloud feedbacks constitute by far the most primary source of spread of both equilibrium and transient climate responses simulated by GCMs. The spread associated with intermodel differences in cloud feedbacks appears to be roughly 3 times larger than that associated either with the combined water vapor{\textendash}lapse-rate feedback, the ocean heat uptake, or the radiative forcing.},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/HR9X4M7D/Dufresne and Bony - 2008 - An Assessment of the Primary Sources of Spread of .pdf}
}

@article{elsterNvidiaHopperGPU2022,
  title = {Nvidia {{Hopper GPU}} and {{Grace CPU Highlights}}},
  author = {Elster, Anne C. and Haugdahl, Tor A.},
  year = {2022},
  month = mar,
  journal = {Computing in Science \& Engineering},
  volume = {24},
  number = {2},
  pages = {95--100},
  issn = {1558-366X},
  doi = {10.1109/MCSE.2022.3163817},
  abstract = {At GTC 2022, Nvidia announced a new product family that aims to cover from small enterprise workloads through exascale high performance computing (HPC) and trillion-parameter AI models. This column highlights the most interesting features of their new Hopper graphical processing unit (GPU) and Grace central processing unit (CPU) computer chips and the Hopper product family. We also discuss some of the history behind Nvidia technologies and their most useful features for computational scientists, such as the Hopper DPX dynamic programming (DP) instruction set, increased number of SMs, and FP 8 tensor core availability. Also included are descriptions of the new Hopper Clustered SMs architecture and updated NVSwitch technologies that integrate their new ARM-based Grace CPU.},
  keywords = {Central processing units,Companies,Graphics processing units,Product design,Product development},
  file = {/Users/caspar/Zotero/storage/SXV5H73R/Elster and Haugdahl - 2022 - Nvidia Hopper GPU and Grace CPU Highlights.pdf;/Users/caspar/Zotero/storage/KL3DKPXC/stamp.html}
}

@book{farberParallelProgrammingOpenACC2017,
  title = {Parallel {{Programming}} with {{OpenACC}}},
  author = {Farber, Rob},
  year = {2017},
  publisher = {{Elsevier}},
  isbn = {978-0-12-410397-9},
  langid = {english}
}

@techreport{FFTW97,
  title = {The Fastest {{Fourier}} Transform in the West},
  author = {Frigo, Matteo and Johnson, Steven G.},
  year = {1997},
  month = sep,
  number = {MIT-LCS-TR-728},
  institution = {{Massachusetts Institute of Technology}}
}

@article{flynnVeryHighspeedComputing1966,
  title = {Very High-Speed Computing Systems},
  author = {Flynn, M.J.},
  year = {1966},
  journal = {Proceedings of the IEEE},
  volume = {54},
  number = {12},
  pages = {1901--1909},
  issn = {0018-9219},
  doi = {10.1109/PROC.1966.5273},
  url = {http://ieeexplore.ieee.org/document/1447203/},
  urldate = {2023-06-04},
  abstract = {Very high-speed computers maybe clnssified as follows: 1) Single Jktmction S t r d i n g l e Data Stream (SISD) 2) S i l e Imbnctioa Stream-Multiple Data Stream(SIMD) 3) Multiplehstmcth StrePntSingle Data Stream(MSD) 4) Mnltiple Instroctioo Stream-Multiple Data Stream (''D).},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/RMSPUSJH/Flynn - 1966 - Very high-speed computing systems.pdf}
}

@inproceedings{frigoFFTWAdaptiveSoftware1998a,
  title = {{{FFTW}}: An Adaptive Software Architecture for the {{FFT}}},
  shorttitle = {{{FFTW}}},
  booktitle = {Proceedings of the 1998 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}, {{ICASSP}} '98 ({{Cat}}. {{No}}.{{98CH36181}})},
  author = {Frigo, M. and Johnson, S.G.},
  year = {1998},
  volume = {3},
  pages = {1381--1384},
  publisher = {{IEEE}},
  address = {{Seattle, WA, USA}},
  doi = {10.1109/ICASSP.1998.681704},
  url = {http://ieeexplore.ieee.org/document/681704/},
  urldate = {2023-10-03},
  isbn = {978-0-7803-4428-0}
}

@article{gustafsonReevaluatingAmdahlLaw1988,
  title = {Reevaluating {{Amdahl}}'s Law},
  author = {Gustafson, John L.},
  year = {1988},
  month = may,
  journal = {Communications of the ACM},
  volume = {31},
  number = {5},
  pages = {532--533},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/42411.42415},
  url = {https://dl.acm.org/doi/10.1145/42411.42415},
  urldate = {2023-09-14},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/3N2W5BTG/Gustafson - 1988 - Reevaluating Amdahl's law.pdf}
}

@inproceedings{hanReducingBranchDivergence2011,
  title = {Reducing Branch Divergence in {{GPU}} Programs},
  booktitle = {Proceedings of the {{Fourth Workshop}} on {{General Purpose Processing}} on {{Graphics Processing Units}}},
  author = {Han, Tianyi David and Abdelrahman, Tarek S.},
  year = {2011},
  month = mar,
  pages = {1--8},
  publisher = {{ACM}},
  address = {{Newport Beach California USA}},
  doi = {10.1145/1964179.1964184},
  url = {https://dl.acm.org/doi/10.1145/1964179.1964184},
  urldate = {2023-11-30},
  isbn = {978-1-4503-0569-3},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/HJVTCRUS/Han and Abdelrahman - 2011 - Reducing branch divergence in GPU programs.pdf}
}

@book{hennessyComputerArchitectureQuantitative2012,
  title = {Computer Architecture: A Quantitative Approach},
  shorttitle = {Computer Architecture},
  author = {Hennessy, John L. and Patterson, David A. and Asanovi{\'c}, Krste},
  year = {2012},
  edition = {5th ed},
  publisher = {{Morgan Kaufmann/Elsevier}},
  address = {{Waltham, MA}},
  isbn = {978-0-12-383872-8},
  lccn = {QA76.9.A73 P377 2012},
  keywords = {Computer architecture},
  annotation = {OCLC: ocn755102367}
}

@article{herdmanAcceleratingHydrocodesOpenACC2012,
  title = {Accelerating Hydrocodes with {{OpenACC}}, {{OpenCL}} and {{CUDA}}},
  author = {Herdman, J. A. and Gaudin, W. P. and {McIntosh-Smith}, S. and Boulton, M. and Beckingsale, D. A. and Mallinson, A. C. and Jarvis, S. A.},
  year = {2012},
  journal = {Proceedings - 2012 SC Companion: High Performance Computing, Networking Storage and Analysis, SCC 2012},
  pages = {465--471},
  doi = {10.1109/SC.COMPANION.2012.66},
  abstract = {Hardware accelerators such as GPGPUs are becoming increasingly common in HPC platforms and their use is widely recognised as being one of the most promising approaches for reaching exascale levels of performance.Large HPC centres, such as AWE, have made huge investments in maintaining their existing scientific software codebases, the vast majority of which were not designed to effectively utilise accelerator devices.Consequently, HPC centres will have to decide how to develop their existing applications to take best advantage of future HPC system architectures.Given limited development and financial resources, it is unlikely that all potential approaches will be evaluated for each application. We are interested in how this decision making can be improved, and this work seeks to directly evaluate three candidate technologies - -OpenACC, OpenCL and CUDA - -in terms of performance, programmer productivity, and portability using a recently developed Lagrangian-Eulerian explicit hydrodynamics mini-Application. We find that OpenACC is an extremely viable programming model for accelerator devices, improving programmer productivity and achieving better performance than OpenCL and CUDA. {\copyright} 2012 IEEE.},
  isbn = {9780769549569},
  keywords = {CUDA,High Performance Computing,Hydrodynamics,OpenACC,OpenCL},
  file = {/Users/caspar/Zotero/storage/KFESNJ6F/Herdman et al. - 2012 - Accelerating hydrocodes with OpenACC, OpenCL and C.pdf}
}

@article{heusFormulationDutchAtmospheric2010,
  title = {Formulation of the {{Dutch Atmospheric Large-Eddy Simulation}} ({{DALES}}) and Overview of Its Applications},
  author = {Heus, T. and Van Heerwaarden, C. C. and Jonker, H. J. J. and Pier Siebesma, A. and Axelsen, S. and Van Den Dries, K. and Geoffroy, O. and Moene, A. F. and Pino, D. and De Roode, S. R. and {Vil{\`a}-Guerau De Arellano}, J.},
  year = {2010},
  month = sep,
  journal = {Geoscientific Model Development},
  volume = {3},
  number = {2},
  pages = {415--444},
  issn = {1991-9603},
  doi = {10.5194/gmd-3-415-2010},
  url = {https://gmd.copernicus.org/articles/3/415/2010/},
  urldate = {2023-05-23},
  abstract = {Abstract. The current version of the Dutch Atmospheric Large-Eddy Simulation (DALES) is presented. DALES is a large-eddy simulation code designed for studies of the physics of the atmospheric boundary layer, including convective and stable boundary layers as well as cloudy boundary layers. In addition, DALES can be used for studies of more specific cases, such as flow over sloping or heterogeneous terrain, and dispersion of inert and chemically active species. This paper contains an extensive description of the physical and numerical formulation of the code, and gives an overview of its applications and accomplishments in recent years.},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/B4CER9JR/Heus et al. - 2010 - Formulation of the Dutch Atmospheric Large-Eddy Si.pdf}
}

@article{hockneyFastDirectSolution1965,
  title = {A {{Fast Direct Solution}} of {{Poisson}}'s {{Equation Using Fourier Analysis}}},
  author = {Hockney, R. W.},
  year = {1965},
  month = jan,
  journal = {Journal of the ACM},
  volume = {12},
  number = {1},
  pages = {95--113},
  issn = {0004-5411, 1557-735X},
  doi = {10.1145/321250.321259},
  url = {https://dl.acm.org/doi/10.1145/321250.321259},
  urldate = {2023-12-19},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/JUZPLBBU/Hockney - 1965 - A Fast Direct Solution of Poisson's Equation Using.pdf}
}

@article{hollandMeasurementsAtmosphericMass1973,
  title = {Measurements of the {{Atmospheric Mass}}, {{Energy}}, and {{Momentum Budgets Over}} a 500-{{Kilometer Square}} of {{Tropical Ocean}}},
  author = {Holland, Joshua Z. and Rasmusson, Eugene M.},
  year = {1973},
  month = jan,
  journal = {Monthly Weather Review},
  volume = {101},
  pages = {44},
  issn = {0027-0644},
  doi = {10.1175/1520-0493(1973)101<0044:MOTAME>2.3.CO;2},
  url = {https://ui.adsabs.harvard.edu/abs/1973MWRv..101...44H},
  urldate = {2023-09-19},
  annotation = {ADS Bibcode: 1973MWRv..101...44H},
  file = {/Users/caspar/Zotero/storage/NQE9M6MX/Holland and Rasmusson - 1973 - Measurements of the Atmospheric Mass, Energy, and .pdf}
}

@article{hundsdorferPositiveFiniteDifferenceAdvection1995,
  title = {A {{Positive Finite-Difference Advection Scheme}}},
  author = {Hundsdorfer, W. and Koren, B. and {vanLoon}, M. and Verwer, J.G.},
  year = {1995},
  month = mar,
  journal = {Journal of Computational Physics},
  volume = {117},
  number = {1},
  pages = {35--46},
  issn = {00219991},
  doi = {10.1006/jcph.1995.1042},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S002199918571042X},
  urldate = {2024-02-09},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/25YTWWDK/Hundsdorfer et al. - 1995 - A Positive Finite-Difference Advection Scheme.pdf}
}

@article{janssonCloudBotanyShallow2023,
  title = {Cloud {{Botany}}: {{Shallow Cumulus Clouds}} in an {{Ensemble}} of {{Idealized Large}}-{{Domain Large}}-{{Eddy Simulations}} of the {{Trades}}},
  shorttitle = {Cloud {{Botany}}},
  author = {Jansson, Fredrik and Janssens, Martin and Gr{\"o}nqvist, Johanna H. and Siebesma, A. Pier and Glassmeier, Franziska and Attema, Jisk and Azizi, Victor and Satoh, Masaki and Sato, Yousuke and Schulz, Hauke and K{\"o}lling, Tobias},
  year = {2023},
  month = nov,
  journal = {Journal of Advances in Modeling Earth Systems},
  volume = {15},
  number = {11},
  pages = {e2023MS003796},
  issn = {1942-2466, 1942-2466},
  doi = {10.1029/2023MS003796},
  url = {https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2023MS003796},
  urldate = {2023-11-23},
  abstract = {Abstract                            Small shallow cumulus clouds ({$<$}1~km) over the tropical oceans appear to possess the ability to self-organize into mesoscale (10{\textendash}100~km) patterns. To better understand the processes leading to such self-organized convection, we present Cloud Botany, an ensemble of 103 large-eddy simulations on domains of 150~km, produced by the Dutch Atmospheric Large Eddy Simulation model on supercomputer Fugaku. Each simulation is run in an idealized, fixed, larger-scale environment, controlled by six free parameters. We vary these over characteristic ranges for the winter trades, including parameter combinations observed during the EUREC               4               A (Elucidating the role of clouds{\textendash}circulation coupling in climate) field campaign. In contrast to simulation setups striving for maximum realism, Cloud Botany provides a platform for studying idealized, and therefore more clearly interpretable causal relationships between conditions in the larger-scale environment and patterns in mesoscale, self-organized shallow convection. We find that any simulation that supports cumulus clouds eventually develops mesoscale patterns in their cloud fields. We also find a rich variety in these patterns as our control parameters change, including cold pools lined by cloudy arcs, bands of cross-wind clouds and aggregated patches, sometimes topped by thin anvils. Many of these features are similar to cloud patterns found in nature. The published data set consists of raw simulation output on full 3D grids and 2D cross-sections, as well as post-processed quantities aggregated over the vertical (2D), horizontal (1D) and all spatial dimensions (time-series). The data set is directly accessible from Python through the use of the EUREC               4               A intake catalog.                        ,              Plain Language Summary             The organization of shallow cumulus clouds over the tropical ocean has recently received a lot of attention. This type of organization is potentially important for how the clouds are affected by a changing climate and also for how they modulate further warming. We present a collection of 103 detailed simulations of shallow cumulus clouds in idealized atmospheric environments. These environments are described by six parameters, and our collection is formed by systematically simulating different parameter combinations. This way an ensemble is created that spans up a multidimensional phase space of environmental conditions typical for the wintertime subtropical Atlantic Ocean. This approach allows us to form a picture of how the environmental conditions relate to the cloud organization that develops in the simulations. At a glance, most simulations evolve similarly: They quickly form small cumulus clouds, which then grow in size and organize into patterns. Often this leads to rainfall, which then causes further heterogeneity and pattern formation. The data is openly available online, and will serve future studies of cumulus clouds, their organization, and how they interact with the climate.           ,              Key Points                                                                We present Cloud Botany, an ensemble of idealized large-eddy simulations of the winter trade wind regions, controlled by six varied parameters                                                     The parameter ranges are chosen to match the climatology of the trade wind region                                                     The simulations show a variety of cloud organization patterns: small cumulus, stripes, cold pools, cloud arcs, and anvils},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/WDVJLKI5/Jansson et al. - 2023 - Cloud Botany Shallow Cumulus Clouds in an Ensembl.pdf}
}

@article{janssonRegionalSuperparameterizationGlobal2019,
  title = {Regional {{Superparameterization}} in a {{Global Circulation Model Using Large Eddy Simulations}}},
  author = {Jansson, Fredrik and {van~den~Oord}, Gijs and Pelupessy, Inti and Gr{\"o}nqvist, Johanna H. and Siebesma, A. Pier and Crommelin, Daan},
  year = {2019},
  month = sep,
  journal = {Journal of Advances in Modeling Earth Systems},
  volume = {11},
  number = {9},
  pages = {2958--2979},
  issn = {1942-2466, 1942-2466},
  doi = {10.1029/2018MS001600},
  url = {https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2018MS001600},
  urldate = {2024-02-11},
  abstract = {Abstract             As a computationally attractive alternative for global large eddy simulations (LESs), we investigate the possibility of using comprehensive three-dimensional LESs as a superparameterization that can replace all traditional parameterizations of atmospheric processes that are currently used in global models. We present the technical design for a replacement of the parameterization for clouds, convection, and turbulence of the global atmospheric model of the European Centre for Medium-Range Weather Forecasts by the Dutch Atmospheric Large Eddy Simulation model. The model coupling consists of bidirectional data exchange between the global model and the high-resolution LES models embedded within the columns of the global model. Our setup allows for selective superparameterization, that is, for applying superparameterization in local regions selected by the user, while keeping the standard parameterization of the global model intact outside this region. Computationally, this setup can result in major geographic load imbalance, because of the large difference in computational load between superparameterized and nonsuperparameterized model columns. To resolve this issue, we use a modular design where the local and global models are kept as distinct model codes and organize the model coupling such that all the local models run in parallel, separate from the global model. First simulation results, employing this design, demonstrate the potential of our approach.           ,              Key Points                                                                We present an efficient implementation of a large eddy simulation (LES)-based superparameterization in a global circulation model                                                     Superparameterization can be enabled for grid points in a selected area                                                     The initial results of an LES-based superparameterized simulation are evaluated and found promising},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/VP8CY9XE/Jansson et al. - 2019 - Regional Superparameterization in a Global Circula.pdf}
}

@article{kedwardStateFortran2022,
  title = {The {{State}} of {{Fortran}}},
  author = {Kedward, Laurence J. and Aradi, Balint and Certik, Ondrej and Curcic, Milan and Ehlert, Sebastian and Engel, Philipp and Goswami, Rohit and Hirsch, Michael and {Lozada-Blanco}, Asdrubal and Magnin, Vincent and Markus, Arjen and Pagone, Emanuele and Pribec, Ivan and Richardson, Brad and Snyder, Harris and Urban, John and Vandenplas, Jeremie},
  year = {2022},
  month = mar,
  journal = {Computing in Science \& Engineering},
  volume = {24},
  number = {2},
  pages = {63--72},
  issn = {1521-9615, 1558-366X},
  doi = {10.1109/MCSE.2022.3159862},
  url = {https://ieeexplore.ieee.org/document/9736688/},
  urldate = {2023-10-06},
  file = {/Users/caspar/Zotero/storage/B9R5HI9W/Kedward et al. - 2022 - The State of Fortran.pdf}
}

@techreport{khronosgroupSYCL2020Specification2023,
  title = {{{SYCL}} 2020 {{Specification}} (Revision 8)},
  author = {{Khronos Group}},
  year = {2023},
  month = oct,
  url = {https://registry.khronos.org/SYCL/specs/sycl-2020/pdf/sycl-2020.pdf}
}

@inproceedings{larsenFastMatrixMultiplies2001,
  title = {Fast Matrix Multiplies Using Graphics Hardware},
  booktitle = {Proceedings of the 2001 {{ACM}}/{{IEEE}} Conference on {{Supercomputing}}},
  author = {Larsen, E. Scott and McAllister, David},
  year = {2001},
  month = nov,
  pages = {55--55},
  publisher = {{ACM}},
  address = {{Denver Colorado}},
  doi = {10.1145/582034.582089},
  url = {https://dl.acm.org/doi/10.1145/582034.582089},
  urldate = {2024-02-13},
  isbn = {978-1-58113-293-9},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/D96NBQI5/Larsen and McAllister - 2001 - Fast matrix multiplies using graphics hardware.pdf}
}

@incollection{leonardEnergyCascadeLargeEddy1975,
  title = {Energy {{Cascade}} in {{Large-Eddy Simulations}} of {{Turbulent Fluid Flows}}},
  booktitle = {Advances in {{Geophysics}}},
  author = {Leonard, A.},
  year = {1975},
  volume = {18},
  pages = {237--248},
  publisher = {{Elsevier}},
  doi = {10.1016/S0065-2687(08)60464-1},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0065268708604641},
  urldate = {2024-02-01},
  isbn = {978-0-12-018818-5},
  langid = {english}
}

@inproceedings{lilly1967representation,
  title = {The Representation of Small-Scale Turbulence in Numerical Simulation Experiments},
  booktitle = {Proc. {{IBM}} Sci. {{Comput}}. {{Symp}}. on Environmental Science},
  author = {Lilly, Douglas K},
  year = {1967},
  pages = {195--210}
}

@techreport{liquilungOpenBoundaryConditions2023,
  type = {Preprint},
  title = {Open {{Boundary Conditions}} for {{Atmospheric Large Eddy Simulations}} and the {{Implementation}} in {{DALES4}}.4},
  author = {Liqui Lung, Franciscus P. A. and Jakob, Christian and Siebesma, A. Pier and Jansson, Fredrik R.},
  year = {2023},
  month = nov,
  institution = {{Atmospheric sciences}},
  doi = {10.5194/gmd-2023-196},
  url = {https://gmd.copernicus.org/preprints/gmd-2023-196/},
  urldate = {2023-11-15},
  abstract = {Abstract. Open boundary conditions were developed for atmospheric large eddy simulation (LES) models and implemented into the Dutch Atmospheric Large Eddy simulation model. The implementation was tested in a "Big Brother"-like setup, in which the simulation with open boundary conditions was forced by an identical control simulation with periodic boundary conditions. The results show that the open boundary implementation has minimal influence on the solution. Both the mean state and the turbulent structures are close to the control simulation and disturbances at the in- and outflow boundaries are negligible. To emulate a setup in which the LES is coupled to a coarser model, the influence of coarse boundary input was tested by smoothing the output of the periodic control simulation both temporally and spatially before feeding it as input to the simulation with open boundary conditions. The results show that when the ratio between input and model resolution increases, disturbances start to form at the inflow boundary and an area exists where turbulence needs to develop. Adding synthetic turbulence to the smoothed input reduces the size of this area and the magnitude of the disturbances.},
  file = {/Users/caspar/Zotero/storage/DIX8W5ZS/Liqui Lung et al. - 2023 - Open Boundary Conditions for Atmospheric Large Edd.pdf}
}

@article{lorenzDeterministicNonperiodicFlow1963,
  title = {Deterministic {{Nonperiodic Flow}}},
  author = {Lorenz, Edward N.},
  year = {1963},
  month = jan,
  journal = {Journal of the Atmospheric Sciences},
  volume = {20},
  number = {2},
  pages = {130--141},
  doi = {10.1175/1520-0469(1963)020%3C0130:DNF%3E2.0.CO;2}
}

@article{masonLargeEddySimulationConvective1989,
  title = {Large-{{Eddy Simulation}} of the {{Convective Atmospheric Boundary Layer}}},
  author = {Mason, P. J.},
  year = {1989},
  month = jun,
  journal = {Journal of the Atmospheric Sciences},
  volume = {46},
  number = {11},
  pages = {1492--1516},
  publisher = {{American Meteorological Society}},
  issn = {0022-4928, 1520-0469},
  doi = {10.1175/1520-0469(1989)046<1492:LESOTC>2.0.CO;2},
  url = {https://journals.ametsoc.org/view/journals/atsc/46/11/1520-0469_1989_046_1492_lesotc_2_0_co_2.xml},
  urldate = {2023-05-26},
  abstract = {Abstract Large-eddy simulations of a free convective atmospheric boundary layer with an overlying capping inversion are considered. Attention is given to the dependence of the results upon the various factors influencing the simulation: the subgrid model, the domain size, and the mesh resolution. By providing artificial constraints upon the convection the results also provide extra insight into the underlying dynamics. The gross features of the boundary layer, such as the overall energy budget, are not sensitive to the details of the simulations but a number of important factors are revealed. It has been found that near the surface the subgrid diffusivity must be larger than is usually supposed, in order for the vertical velocity skewness to have the correct sign. This region of the flow has a significant subgrid-scale heat flux and it seems that the subgrid model requires improvement in such cases. A revised model which under statically unstable conditions allows the mixing-length of the subgrid-scale turbulence to depend on the flow stability is found to give improved results. The domain size and mesh spacings have a significant influence upon the results and need a setting which allows resolution of the main, freely occurring scales of motion. The entrainment at the capping inversion is remarkable in its insensitivity to all factors. Finally, the higher resolution simulations provide a detailed view of the flow structure of the convective boundary layer. Downdrafts cover a large fraction of the surface area, and near the surface the flow converges into smaller areas comprising long narrow regions of updrafts. The plumes which penetrate through the depth of the boundary layer to the inversion mainly occur over the inter-sections of these long narrow regions of updrafts.},
  chapter = {Journal of the Atmospheric Sciences},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/E65ZN4TT/Mason - 1989 - Large-Eddy Simulation of the Convective Atmospheri.pdf}
}

@article{mittalSurveyTechniquesOptimizing2019,
  title = {A Survey of Techniques for Optimizing Deep Learning on {{GPUs}}},
  author = {Mittal, Sparsh and Vaishay, Shraiysh},
  year = {2019},
  month = oct,
  journal = {Journal of Systems Architecture},
  volume = {99},
  pages = {101635},
  issn = {13837621},
  doi = {10.1016/j.sysarc.2019.101635},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1383762119302656},
  urldate = {2024-02-21},
  langid = {english}
}

@incollection{moengNUMERICALMODELSLargeEddy2015,
  title = {{{NUMERICAL MODELS}} {\textbar} {{Large-Eddy Simulation}}},
  booktitle = {Encyclopedia of {{Atmospheric Sciences}}},
  author = {Moeng, C.-H. and Sullivan, P.P.},
  year = {2015},
  pages = {232--240},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-0-12-382225-3.00201-2},
  url = {https://linkinghub.elsevier.com/retrieve/pii/B9780123822253002012},
  urldate = {2023-05-26},
  isbn = {978-0-12-382225-3},
  langid = {english}
}

@article{mullerSelfAggregationConvection2020,
  title = {Self-{{Aggregation}} of {{Convection}} in {{Spatially Varying Sea Surface Temperatures}}},
  author = {M{\"u}ller, Sebastian K. and Hohenegger, Cathy},
  year = {2020},
  month = jan,
  journal = {Journal of Advances in Modeling Earth Systems},
  volume = {12},
  number = {1},
  pages = {e2019MS001698},
  issn = {1942-2466, 1942-2466},
  doi = {10.1029/2019MS001698},
  url = {https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2019MS001698},
  urldate = {2023-10-18},
  abstract = {Abstract             The phenomenon of self-aggregation of convection was first identified in convection-permitting simulations of radiative convective equilibrium, characterized by homogeneous boundary conditions and in the absence of planetary rotation. In this study, we expose self-aggregation of convection to more complex, nonhomogeneous boundary conditions and investigate its interaction with convective aggregation, as forced by large-scale variations in sea surface temperatures (SSTs). We do this by conducting radiative convective equilibrium simulations on a spherical domain, with SST patterns that are zonally homogeneous but meridionally varying. Due to the meridional contrast in SST, a convergence line first forms, mimicking the Intertropical Convergence Zone. We nevertheless find that the convergence line breaks up and contracts zonally as a result of the self-aggregation of convection. The contraction is significant, being here more than 50\% of the original extent. The stability of the convergence line is controlled by the strength of the meridional circulation, which depends upon the imposed SST contrast. However, the process of self-aggregation, once it is initiated, is insensitive to the strength of the SST contrast. The zonal contraction is accompanied by a slight meridional expansion and a moistening of the high latitudes, where SSTs are low. The moistening of the high latitudes can be understood from the fact that the convective cluster intensifies and expands its moist meridional low-level outflow when it self-aggregates zonally. Overall, our results suggest that the Intertropical Convergence Zone may be unstable to the self-aggregation of convection, that self-aggregation may serve as a precursor to the formation of atmospheric rivers, and that longer convergence lines are more likely to exist in regimes with strong SST gradients.           ,              Plain Language Summary             Storm clouds over the tropical oceans are found to organize in large systems. They tend to form where the temperatures of the sea surface (SSTs) are highest and that is most often along the equator. With idealized computer model experiments we investigate how different sea surface temperature patterns, along with the natural tendency of clouds to aggregate, control the properties of the storm cloud system. We find that a contrast in SSTs immediately acts to organize the storm clouds. In our simulations, this means the formation of a long line of clouds, oriented along the equator. With time however, this line breaks up and then contracts just by itself. Here we find that the spatial difference in SSTs controls the stability of the cloud system against the break up and contraction: The stronger the difference in SST, the more stable is the line of clouds. Also we find that the storm clouds export moisture only in one direction: from the equator to the higher latitudes.           ,              Key Points                                                                A convergence line, forced by an SST contrast and comprising deep convection, is unstable to the self-aggregation of convection                                                     The zonally elongated convective clusters export moisture only meridionally and only between 1 and 5~km height},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/9W28M57R/Müller and Hohenegger - 2020 - Self‐Aggregation of Convection in Spatially Varyin.pdf;/Users/caspar/Zotero/storage/B3ZNYBUK/Müller and Hohenegger - 2020 - Self‐Aggregation of Convection in Spatially Varyin.pdf}
}

@article{niemeyerRecentProgressChallenges2014,
  title = {Recent Progress and Challenges in Exploiting Graphics Processors in Computational Fluid Dynamics},
  author = {Niemeyer, Kyle E. and Sung, Chih-Jen},
  year = {2014},
  month = feb,
  journal = {The Journal of Supercomputing},
  volume = {67},
  number = {2},
  pages = {528--564},
  issn = {1573-0484},
  doi = {10.1007/s11227-013-1015-7},
  url = {https://doi.org/10.1007/s11227-013-1015-7},
  urldate = {2023-05-29},
  abstract = {The progress made in accelerating simulations of fluid flow using GPUs, and the challenges that remain, are surveyed. The review first provides an introduction to GPU computing and programming, and discusses various considerations for improved performance. Case studies comparing the performance of CPU- and GPU-based solvers for the Laplace and incompressible Navier{\textendash}Stokes equations are performed in order to demonstrate the potential improvement even with simple codes. Recent efforts to accelerate CFD simulations using GPUs are reviewed for laminar, turbulent, and reactive flow solvers. Also, GPU implementations of the lattice Boltzmann method are reviewed. Finally, recommendations for implementing CFD codes on GPUs are given and remaining challenges are discussed, such as the need to develop new strategies and redesign algorithms to enable GPU acceleration.},
  langid = {english},
  keywords = {Computational fluid dynamics (CFD),CUDA,Graphics processing unit (GPU),Laminar flows,Reactive flow,Turbulent flow},
  file = {/Users/caspar/Zotero/storage/324IWT3H/Niemeyer and Sung - 2014 - Recent progress and challenges in exploiting graph.pdf}
}

@misc{nvidiaCUDAProgrammingGuide2023,
  title = {{{CUDA C}}++ {{Programming Guide}}},
  author = {{NVIDIA}},
  year = {2023},
  month = jul,
  url = {https://docs.nvidia.com/cuda/pdf/CUDA_C_Programming_Guide.pdf}
}

@misc{nvidiaCuFFT,
  title = {{{cuFFT}}},
  author = {{Nvidia}},
  url = {https://docs.nvidia.com/cuda/cufft/index.html}
}

@misc{nvidiadeveloperCuFFTMp,
  title = {{{cuFFTMp}}},
  author = {{NVIDIA Developer}},
  url = {https://docs.nvidia.com/hpc-sdk/cufftmp/index.html}
}

@misc{nvidiadeveloperNVSHMEM,
  title = {{{NVSHMEM}}},
  author = {{NVIDIA Developer}},
  url = {https://docs.nvidia.com/nvshmem/api/index.html}
}

@techreport{nvidiaNVIDIAA100Tensor2020,
  title = {{{NVIDIA A100 Tensor Core GPU Overview}}},
  author = {{NVIDIA}},
  year = {2020},
  url = {https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf}
}

@techreport{nvidiaNVIDIAAmpereGA1022021,
  title = {{{NVIDIA Ampere GA102 GPU Architecture}}},
  author = {{NVIDIA}},
  year = {2021},
  url = {https://www.nvidia.com/content/PDF/nvidia-ampere-ga-102-gpu-architecture-whitepaper-v2.1.pdf}
}

@misc{nvidiaNVIDIANsightSystems,
  title = {{{NVIDIA Nsight Systems}}},
  author = {{NVIDIA}},
  url = {https://developer.nvidia.com/nsight-systems},
  urldate = {2023-05-25}
}

@article{ouwerslootLargeEddySimulationComparison2017,
  title = {Large-{{Eddy Simulation Comparison}} of {{Neutral Flow Over}} a {{Canopy}}: {{Sensitivities}} to {{Physical}} and {{Numerical Conditions}}, and {{Similarity}} to {{Other Representations}}},
  shorttitle = {Large-{{Eddy Simulation Comparison}} of {{Neutral Flow Over}} a {{Canopy}}},
  author = {Ouwersloot, H. G. and Moene, A. F. and Attema, J. J. and De Arellano, J. Vil{\`a}-Guerau},
  year = {2017},
  month = jan,
  journal = {Boundary-Layer Meteorology},
  volume = {162},
  number = {1},
  pages = {71--89},
  issn = {0006-8314, 1573-1472},
  doi = {10.1007/s10546-016-0182-5},
  url = {http://link.springer.com/10.1007/s10546-016-0182-5},
  urldate = {2023-11-24},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/KQWI42AV/Ouwersloot et al. - 2017 - Large-Eddy Simulation Comparison of Neutral Flow O.pdf}
}

@article{ouwerslootQuantifyingTransportSubcloud2013,
  title = {Quantifying the Transport of Subcloud Layer Reactants by Shallow Cumulus Clouds over the {{Amazon}}},
  author = {Ouwersloot, H. G. and De Arellano, J. Vil{\`a}-Guerau and H. Van Stratum, B. J. and Krol, M. C. and Lelieveld, J.},
  year = {2013},
  month = dec,
  journal = {Journal of Geophysical Research: Atmospheres},
  volume = {118},
  number = {23},
  issn = {2169-897X, 2169-8996},
  doi = {10.1002/2013JD020431},
  url = {https://agupubs.onlinelibrary.wiley.com/doi/10.1002/2013JD020431},
  urldate = {2024-02-11},
  abstract = {We investigate the vertical transport of atmospheric chemical reactants from the subcloud layer to the cumulus cloud layer driven by shallow convection over the Amazon during the dry season. The dynamical and chemical assumptions needed for mesoscale and global chemistry transport model parametrizations are systematically analyzed using a Large Eddy Simulation model. We quantify the mass flux transport contribution to the temporal evolution of reactants. Isoprene, a key atmospheric compound over the tropical rain forest, decreases by 8.5\% h               -1               on average and 15\% h               -1               at maximum due to mass-flux-induced removal. We apply mass flux parametrizations for the transport of chemical reactants and obtain satisfactory agreement with numerically resolved transport, except for some reactants like O               3               , NO, and NO               2               . The latter is caused by the local partitioning of reactants, influenced by UV radiation extinction by clouds and small-scale variability of ambient atmospheric compounds. By considering the longer-lived NO                                x                              (NO + NO               2               ), the transport is well represented by the parametrization. Finally, by considering heterogeneous surface exchange conditions, it is demonstrated that the parametrizations are sensitive to boundary conditions due to changes in the boundary layer dynamics.                        ,              Key Points                                                                Transport by shallow convection can significantly affect atmospheric reactants                                                     This transport of atmospheric compounds by active clouds can be parametrized                                                     Heterogeneous surface conditions affect clouds and the needed parametrizations},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/LN8BEEJZ/Ouwersloot et al. - 2013 - Quantifying the transport of subcloud layer reacta.pdf}
}

@article{owensGPUComputing2008,
  title = {{{GPU Computing}}},
  author = {Owens, J.D. and Houston, M. and Luebke, D. and Green, S. and Stone, J.E. and Phillips, J.C.},
  year = {2008},
  month = may,
  journal = {Proceedings of the IEEE},
  volume = {96},
  number = {5},
  pages = {879--899},
  issn = {0018-9219, 1558-2256},
  doi = {10.1109/JPROC.2008.917757},
  url = {http://ieeexplore.ieee.org/document/4490127/},
  urldate = {2023-06-04},
  abstract = {The graphics processing unit (GPU) has become an integral part of today's mainstream computing systems. Over the past six years, there has been a marked increase in the performance and capabilities of GPUs. The modern GPU is not only a powerful graphics engine but also a highly parallel programmable processor featuring peak arithmetic and memory bandwidth that substantially outpaces its CPU counterpart. The GPU's rapid increase in both programmability and capability has spawned a research community that has successfully mapped a broad range of computationally demanding, complex problems to the GPU. This effort in generalpurpose computing on the GPU, also known as GPU computing, has positioned the GPU as a compelling alternative to traditional microprocessors in high-performance computer systems of the future. We describe the background, hardware, and programming model for GPU computing, summarize the state of the art in tools and techniques, and present four GPU computing successes in game physics and computational biophysics that deliver order-of-magnitude performance gains over optimized CPU applications.},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/TQKRD2TY/Owens et al. - 2008 - GPU Computing.pdf}
}

@book{popeTurbulentFlows2000,
  title = {Turbulent Flows},
  author = {Pope, S. B.},
  year = {2000},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge ; New York}},
  isbn = {978-0-521-59125-6 978-0-521-59886-6},
  lccn = {QA913 .P64 2000},
  keywords = {Turbulence}
}

@inproceedings{potluriEfficientInternodeMPI2013,
  title = {Efficient {{Inter-node MPI Communication Using GPUDirect RDMA}} for {{InfiniBand Clusters}} with {{NVIDIA GPUs}}},
  booktitle = {2013 42nd {{International Conference}} on {{Parallel Processing}}},
  author = {Potluri, Sreeram and Hamidouche, Khaled and Venkatesh, Akshay and Bureddy, Devendar and Panda, Dhabaleswar K.},
  year = {2013},
  month = oct,
  pages = {80--89},
  publisher = {{IEEE}},
  address = {{Lyon, France}},
  doi = {10.1109/ICPP.2013.17},
  url = {http://ieeexplore.ieee.org/document/6687341/},
  urldate = {2023-11-03},
  isbn = {978-0-7695-5117-3},
  file = {/Users/caspar/Zotero/storage/FGDQTQSN/Potluri et al. - 2013 - Efficient Inter-node MPI Communication Using GPUDi.pdf}
}

@book{rauberParallelProgrammingMulticore2023,
  title = {Parallel Programming: For Multicore and Cluster Systems},
  shorttitle = {Parallel Programming},
  author = {Rauber, Thomas and R{\"u}nger, Gudula},
  year = {2023},
  edition = {Third edition},
  publisher = {{Springer}},
  address = {{Cham}},
  abstract = {This textbook covers the new development in processor architecture and parallel hardware. It provides detailed descriptions of parallel programming techniques that are necessary for developing efficient programs for multicore processors as well as for parallel cluster systems and supercomputers. The book is structured in three main parts, covering all areas of parallel computing: the architecture of parallel systems, parallel programming models and environments, and the implementation of efficient application algorithms. The emphasis lies on parallel programming techniques needed for different architectures. In particular, this third edition includes an extended update of the chapter on computer architecture and performance analysis taking new developments such as the aspect of energy consumption into consideration. The description of OpenMP has been extended and now also captures the task concept of OpenMP. The chapter on message-passing programming has been extended and updated to include new features of MPI such as extended reduction operations and non-blocking collective communication operations. The chapter on GPU programming also has been updated. All other chapters also have been revised carefully. The main goal of this book is to present parallel programming techniques that can be used in many situations for many application areas and to enable the reader to develop correct and efficient parallel programs. Many example programs and exercises are provided to support this goal and to show how the techniques can be applied to further applications. The book can be used as a textbook for students as well as a reference book for professionals. The material of the book has been used for courses in parallel programming at different universities for many years},
  isbn = {978-3-031-28924-8 978-3-031-28923-1},
  langid = {english}
}

@misc{RefactoringOptimizingWRF,
  title = {Refactoring and {{Optimizing WRF Model}} on {{Sunway TaihuLight}} {\textbar} {{Proceedings}} of the 48th {{International Conference}} on {{Parallel Processing}}},
  url = {https://dl.acm.org/doi/10.1145/3337821.3337923},
  urldate = {2023-05-10},
  file = {/Users/caspar/Zotero/storage/KMETDULS/Refactoring and Optimizing WRF Model on Sunway Tai.pdf;/Users/caspar/Zotero/storage/453T3UG2/3337821.html}
}

@book{richardson1922weather,
  title = {Weather Prediction by Numerical Process},
  author = {Richardson, Lewis F},
  year = {1922},
  publisher = {{University Press}}
}

@incollection{Rodriguez2019,
  title = {{{LES}} and {{DNS}} Turbulence Modeling},
  booktitle = {Applied Computational Fluid Dynamics and Turbulence Modeling: {{Practical}} Tools, Tips and Techniques},
  author = {Rodriguez, Sal},
  year = {2019},
  pages = {197--223},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-28691-0_5},
  url = {https://doi.org/10.1007/978-3-030-28691-0_5},
  abstract = {This chapter is divided into two parts, LES and DNS. In the first part, the LES turbulence model is derived from first principles, and its terms are described in detail. The usage of LES filters is described, along with various recommendations. The LIKE algorithm is applied to show how to model large eddies properly by applying the appropriate node-to-node computational distances. LES-specific boundary and initial conditions are described, and dozens of practical recommendations are provided. In the second part, analogous discussions and recommendations for DNS are included as well.},
  isbn = {978-3-030-28691-0}
}

@inproceedings{romeroDistributedmemorySimulationsTurbulent2022,
  title = {Distributed-Memory Simulations of Turbulent Flows on Modern {{GPU}} Systems Using an Adaptive Pencil Decomposition Library},
  booktitle = {Proceedings of the {{Platform}} for {{Advanced Scientific Computing Conference}}},
  author = {Romero, Joshua and Costa, Pedro and Fatica, Massimiliano},
  year = {2022},
  month = jul,
  series = {{{PASC}} '22},
  pages = {1--11},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3539781.3539797},
  url = {https://dl.acm.org/doi/10.1145/3539781.3539797},
  urldate = {2023-06-02},
  abstract = {This paper presents a performance analysis of pencil domain decomposition methodologies for three-dimensional Computational Fluid Dynamics (CFD) codes for turbulence simulations, on several large GPU-accelerated clusters. The performance was assessed for the numerical solution of the Navier-Stokes equations in two codes which require the calculation of Fast-Fourier Transforms (FFT): a tri-periodic pseudo-spectral solver for isotropic turbulence, and a finite-difference solver for canonical turbulent flows, where the FFTs are used in its Poisson solver. Both codes use a newly developed transpose library that automatically determines the optimal domain decomposition and communication backend on each system. We compared the performance across systems with very different node topologies and available network bandwidth, to show how these characteristics impact decomposition selection for best performance. Additionally, we assessed the performance of several communication libraries available on these systems, such as Open-MPI, IBM Spectrum MPI, Cray MPI, the NVIDIA Collective Communication Library (NCCL), and NVSHMEM. Our results show that the optimal combination of communication backend and domain decomposition is highly system-dependent, and that the adaptive decomposition library is key in ensuring efficient resource usage with minimal user effort.},
  isbn = {978-1-4503-9410-9},
  keywords = {computational fluid dynamics,direct numerical simulation,GPU accelerated systems,parallel transpose},
  file = {/Users/caspar/Zotero/storage/XENEL2NG/Romero et al. - 2022 - Distributed-memory simulations of turbulent flows .pdf}
}

@article{schalkwijkHighPerformanceSimulationsTurbulent2012,
  title = {High-{{Performance Simulations}} of {{Turbulent Clouds}} on a {{Desktop PC}}: {{Exploiting}} the {{GPU}}},
  shorttitle = {High-{{Performance Simulations}} of {{Turbulent Clouds}} on a {{Desktop PC}}},
  author = {Schalkwijk, Jer{\^o}me and Griffith, Eric J. and Post, Frits H. and Jonker, Harm J. J.},
  year = {2012},
  month = mar,
  journal = {Bulletin of the American Meteorological Society},
  volume = {93},
  number = {3},
  pages = {307--314},
  issn = {1520-0477},
  doi = {10.1175/BAMS-D-11-00059.1},
  url = {https://journals.ametsoc.org/doi/10.1175/BAMS-D-11-00059.1},
  urldate = {2023-05-22},
  abstract = {No abstract available.},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/VI465BX5/Schalkwijk et al. - 2012 - High-Performance Simulations of Turbulent Clouds o.pdf}
}

@article{schalkwijkWeatherForecastingUsing2015,
  title = {Weather {{Forecasting Using GPU-Based Large-Eddy Simulations}}},
  author = {Schalkwijk, Jer{\^o}me and Jonker, Harmen J. J. and Siebesma, A. Pier and Van Meijgaard, Erik},
  year = {2015},
  month = may,
  journal = {Bulletin of the American Meteorological Society},
  volume = {96},
  number = {5},
  pages = {715--723},
  issn = {0003-0007, 1520-0477},
  doi = {10.1175/BAMS-D-14-00114.1},
  url = {https://journals.ametsoc.org/doi/10.1175/BAMS-D-14-00114.1},
  urldate = {2024-01-31},
  abstract = {Abstract             Since the advent of computers midway through the twentieth century, computational resources have increased exponentially. It is likely they will continue to do so, especially when accounting for recent trends in multicore processors. History has shown that such an increase tends to directly lead to weather and climate models that readily exploit the extra resources, improving model quality and resolution. We show that Large-Eddy Simulation (LES) models that utilize modern, accelerated (e.g., by GPU or coprocessor), parallel hardware systems can now provide turbulence-resolving numerical weather forecasts over a region the size of the Netherlands at 100-m resolution. This approach has the potential to speed the development of turbulence-resolving numerical weather prediction models.},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/EEZU7EB3/Schalkwijk et al. - 2015 - Weather Forecasting Using GPU-Based Large-Eddy Sim.pdf}
}

@article{scharKilometerScaleClimateModels2020,
  title = {Kilometer-{{Scale Climate Models}}: {{Prospects}} and {{Challenges}}},
  shorttitle = {Kilometer-{{Scale Climate Models}}},
  author = {Sch{\"a}r, Christoph and Fuhrer, Oliver and Arteaga, Andrea and Ban, Nikolina and Charpilloz, Christophe and Di Girolamo, Salvatore and Hentgen, Laureline and Hoefler, Torsten and Lapillonne, Xavier and Leutwyler, David and Osterried, Katherine and Panosetti, Davide and R{\"u}dis{\"u}hli, Stefan and Schlemmer, Linda and Schulthess, Thomas C. and Sprenger, Michael and Ubbiali, Stefano and Wernli, Heini},
  year = {2020},
  month = may,
  journal = {Bulletin of the American Meteorological Society},
  volume = {101},
  number = {5},
  pages = {E567-E587},
  issn = {0003-0007, 1520-0477},
  doi = {10.1175/BAMS-D-18-0167.1},
  url = {https://journals.ametsoc.org/view/journals/bams/101/5/bams-d-18-0167.1.xml},
  urldate = {2024-01-30},
  abstract = {Abstract             Currently major efforts are underway toward refining the horizontal resolution (or grid spacing) of climate models to about 1 km, using both global and regional climate models (GCMs and RCMs). Several groups have succeeded in conducting kilometer-scale multiweek GCM simulations and decadelong continental-scale RCM simulations. There is the well-founded hope that this increase in resolution represents a quantum jump in climate modeling, as it enables replacing the parameterization of moist convection by an explicit treatment. It is expected that this will improve the simulation of the water cycle and extreme events and reduce uncertainties in climate change projections. While kilometer-scale resolution is commonly employed in limited-area numerical weather prediction, enabling it on global scales for extended climate simulations requires a concerted effort. In this paper, we exploit an RCM that runs entirely on graphics processing units (GPUs) and show examples that highlight the prospects of this approach. A particular challenge addressed in this paper relates to the growth in output volumes. It is argued that the data avalanche of high-resolution simulations will make it impractical or impossible to store the data. Rather, repeating the simulation and conducting online analysis will become more efficient. A prototype of this methodology is presented. It makes use of a bit-reproducible model version that ensures reproducible simulations across hardware architectures, in conjunction with a data virtualization layer as a common interface for output analyses. An assessment of the potential of these novel approaches will be provided.},
  file = {/Users/caspar/Zotero/storage/3NVC57RC/Schär et al. - 2020 - Kilometer-Scale Climate Models Prospects and Chal.pdf}
}

@article{schneiderExascaleEraUs2022,
  title = {The {{Exascale Era}} Is {{Upon Us}}: {{The Frontier}} Supercomputer May Be the First to Reach 1,000,000,000,000,000,000 Operations per Second},
  shorttitle = {The {{Exascale Era}} Is {{Upon Us}}},
  author = {Schneider, David},
  year = {2022},
  month = jan,
  journal = {IEEE Spectrum},
  volume = {59},
  number = {1},
  pages = {34--35},
  issn = {0018-9235, 1939-9340},
  doi = {10.1109/MSPEC.2022.9676353},
  url = {https://ieeexplore.ieee.org/document/9676353/},
  urldate = {2024-02-20}
}

@article{schumannFastFourierTransforms1988,
  title = {Fast {{Fourier}} Transforms for Direct Solution of Poisson's Equation with Staggered Boundary Conditions},
  author = {Schumann, Ulrich and Sweet, Roland A},
  year = {1988},
  month = mar,
  journal = {Journal of Computational Physics},
  volume = {75},
  number = {1},
  pages = {123--137},
  issn = {0021-9991},
  doi = {10.1016/0021-9991(88)90102-7},
  url = {https://www.sciencedirect.com/science/article/pii/0021999188901027},
  urldate = {2023-05-24},
  abstract = {This paper describes pre- and postprocessing algorithms used to incorporate the fast Fourier transform (FFT) into the solution of finite difference approximations to multi-dimensional Poisson's equation on a staggered grid where the boundary is located midway between two grid points. All frequently occurring boundary conditions (Neumann, Dirichlet, or cyclic) are considered including the combination of staggered Neumann boundary condition on one side with nonstaggered Dirichlet boundary condition on the other side. Experiences from implementing these algorithms in vectorized coding in Fortran subroutines are reported.},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/MZZ46CWY/Schumann and Sweet - 1988 - Fast Fourier transforms for direct solution of poi.pdf;/Users/caspar/Zotero/storage/PE69MKIZ/0021999188901027.html}
}

@article{siebesmaEvaluationParametricAssumptions1995,
  title = {Evaluation of {{Parametric Assumptions}} for {{Shallow Cumulus Convection}}},
  author = {Siebesma, A. P. and Cuijpers, J. W. M.},
  year = {1995},
  month = mar,
  journal = {Journal of the Atmospheric Sciences},
  volume = {52},
  number = {6},
  pages = {650--666},
  doi = {10.1175/1520-0469(1995)052%3C0650:EOPAFS%3E2.0.CO;2}
}

@article{siebesmaLargeEddySimulation2003,
  title = {A {{Large Eddy Simulation Intercomparison Study}} of {{Shallow Cumulus Convection}}},
  author = {Siebesma, A. Pier and Bretherton, Christopher S. and Brown, Andrew and Chlond, Andreas and Cuxart, Joan and Duynkerke, Peter G. and Jiang, Hongli and Khairoutdinov, Marat and Lewellen, David and Moeng, Chin-Hoh and Sanchez, Enrique and Stevens, Bjorn and Stevens, David E.},
  year = {2003},
  month = may,
  journal = {Journal of the Atmospheric Sciences},
  volume = {60},
  number = {10},
  pages = {1201--1219},
  doi = {10.1175/1520-0469(2003)60<1201:ALESIS>2.0.CO;2},
  url = {https://journals.ametsoc.org/view/journals/atsc/60/10/1520-0469_2003_60_1201_alesis_2.0.co_2.xml},
  file = {/Users/caspar/Zotero/storage/GFB4N7PB/Siebesma et al. - 2003 - A Large Eddy Simulation Intercomparison Study of S.pdf}
}

@article{slingoUncertaintyWeatherClimate2011,
  title = {Uncertainty in Weather and Climate Prediction},
  author = {Slingo, Julia and Palmer, Tim},
  year = {2011},
  month = dec,
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {369},
  number = {1956},
  pages = {4751--4767},
  issn = {1364-503X, 1471-2962},
  doi = {10.1098/rsta.2011.0161},
  url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2011.0161},
  urldate = {2024-02-06},
  abstract = {Following Lorenz's seminal work on chaos theory in the 1960s, probabilistic approaches to prediction have come to dominate the science of weather and climate forecasting. This paper gives a perspective on Lorenz's work and how it has influenced the ways in which we seek to represent uncertainty in forecasts on all lead times from hours to decades. It looks at how model uncertainty has been represented in probabilistic prediction systems and considers the challenges posed by a changing climate. Finally, the paper considers how the uncertainty in projections of climate change can be addressed to deliver more reliable and confident assessments that support decision-making on adaptation and mitigation.},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/JKWA3DPC/Slingo and Palmer - 2011 - Uncertainty in weather and climate prediction.pdf}
}

@article{smagorinskyGeneralCirculationExperiments1963,
  title = {General {{Circulation Experiments}} with the {{Primitive Equations}}},
  author = {Smagorinsky, Joseph},
  year = {1963},
  month = jan,
  journal = {Monthly Weather Review},
  volume = {91},
  pages = {99--164},
  doi = {10.1175/1520-0493(1963)091%3C0099:GCEWTP%3E2.3.CO;2},
  file = {/Users/caspar/Downloads/mwre-1520-0493_1963_091_0099_gcewtp_2_3_co_2.pdf}
}

@article{spiegel1960boussinesq,
  title = {On the {{Boussinesq}} Approximation for a Compressible Fluid.},
  author = {Spiegel, Edward A and Veronis, G},
  year = {1960},
  journal = {Astrophysical Journal, vol. 131, p. 442},
  volume = {131},
  pages = {442}
}

@book{stullIntroductionBoundaryLayer1988,
  title = {An {{Introduction}} to {{Boundary Layer Meteorology}}},
  author = {Stull, Roland. B},
  year = {1988},
  publisher = {{Kluwer Academic Publishers}},
  address = {{Dordrecht}},
  isbn = {978-90-277-2769-5}
}

@misc{surfSnelliusHardwareFile,
  title = {Snellius Hardware and File Systems},
  author = {{SURF}},
  url = {https://servicedesk.surf.nl/wiki/display/WIKI/Snellius+hardware+and+file+systems}
}

@incollection{swarztrauberVectorizingFFTs1982,
  title = {Vectorizing the {{FFTs}}},
  booktitle = {Parallel {{Computations}}},
  author = {Swarztrauber, Paul N.},
  year = {1982},
  pages = {51--83},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-0-12-592101-5.50007-5},
  url = {https://linkinghub.elsevier.com/retrieve/pii/B9780125921015500075},
  isbn = {978-0-12-592101-5},
  langid = {english}
}

@article{tolmachevVkFFTAPerformantCrossPlatform2023,
  title = {{{VkFFT-A Performant}}, {{Cross-Platform}} and {{Open-Source GPU FFT Library}}},
  author = {Tolmachev, Dmitrii},
  year = {2023},
  journal = {IEEE Access},
  volume = {11},
  pages = {12039--12058},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2023.3242240},
  url = {https://ieeexplore.ieee.org/document/10036080/},
  urldate = {2023-10-03},
  file = {/Users/caspar/Zotero/storage/SRKCLCWM/Tolmachev - 2023 - VkFFT-A Performant, Cross-Platform and Open-Source.pdf}
}

@misc{top500TOP500Lists2023,
  title = {{{TOP500 Lists}}},
  author = {{TOP500}},
  year = {2023},
  url = {https://www.top500.org/lists/top500/}
}

@article{trottKokkosProgrammingModel2022,
  title = {Kokkos 3: {{Programming Model Extensions}} for the {{Exascale Era}}},
  shorttitle = {Kokkos 3},
  author = {Trott, Christian R. and {Lebrun-Grandie}, Damien and Arndt, Daniel and Ciesko, Jan and Dang, Vinh and Ellingwood, Nathan and Gayatri, Rahulkumar and Harvey, Evan and Hollman, Daisy S. and Ibanez, Dan and Liber, Nevin and Madsen, Jonathan and Miles, Jeff and Poliakoff, David and Powell, Amy and Rajamanickam, Sivasankaran and Simberg, Mikael and Sunderland, Dan and Turcksin, Bruno and Wilke, Jeremiah},
  year = {2022},
  month = apr,
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  volume = {33},
  number = {4},
  pages = {805--817},
  issn = {1045-9219, 1558-2183, 2161-9883},
  doi = {10.1109/TPDS.2021.3097283},
  url = {https://ieeexplore.ieee.org/document/9485033/},
  urldate = {2024-02-19},
  file = {/Users/caspar/Zotero/storage/I5UHN9QF/Trott et al. - 2022 - Kokkos 3 Programming Model Extensions for the Exa.pdf}
}

@article{vandenoordPythonInterfaceDutch2020,
  title = {A {{Python}} Interface to the {{Dutch Atmospheric Large-Eddy Simulation}}},
  author = {Van Den Oord, Gijs and Jansson, Fredrik and Pelupessy, Inti and Chertova, Maria and Gr{\"o}nqvist, Johanna H. and Siebesma, Pier and Crommelin, Daan},
  year = {2020},
  month = jul,
  journal = {SoftwareX},
  volume = {12},
  pages = {100608},
  issn = {23527110},
  doi = {10.1016/j.softx.2020.100608},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S2352711020303216},
  urldate = {2024-02-11},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/QX4GILCJ/Van Den Oord et al. - 2020 - A Python interface to the Dutch Atmospheric Large-.pdf}
}

@article{vanderdussenGASSEUCLIPSEModel2013,
  title = {The {{GASS}}/{{EUCLIPSE}} Model Intercomparison of the Stratocumulus Transition as Observed during {{ASTEX}}: {{LES}} Results},
  shorttitle = {The {{GASS}}/{{EUCLIPSE}} Model Intercomparison of the Stratocumulus Transition as Observed during {{ASTEX}}},
  author = {Van Der Dussen, J. J. and De Roode, S. R. and Ackerman, A. S. and Blossey, P. N. and Bretherton, C. S. and Kurowski, M. J. and Lock, A. P. and Neggers, R. A. J. and Sandu, I. and Siebesma, A. P.},
  year = {2013},
  month = jul,
  journal = {Journal of Advances in Modeling Earth Systems},
  volume = {5},
  number = {3},
  pages = {483--499},
  issn = {1942-2466, 1942-2466},
  doi = {10.1002/jame.20033},
  url = {https://agupubs.onlinelibrary.wiley.com/doi/10.1002/jame.20033},
  urldate = {2024-02-01},
  abstract = {Large-eddy simulations of a Lagrangian transition from a vertically well-mixed stratocumulus-topped boundary layer to a situation in which shallow cumuli penetrate an overlying layer of thin and broken stratocumulus are compared with aircraft observations collected during the Atlantic Stratocumulus Transition Experiment. Despite the complexity of the case and the long simulation period of 40 h, the six participating state-of-the-art models skillfully and consistently represent the observed gradual deepening of the boundary layer, a negative buoyancy flux at the top of the subcloud layer and the development of a double-peaked vertical velocity variance profile. The moisture flux from the subcloud to the stratocumulus cloud layer by cumulus convection exhibits a distinct diurnal cycle. During the night the moisture flux at the stratocumulus cloud base exceeds the surface evaporation flux, causing a net drying of the subcloud layer, and vice versa during daytime. The spread in the liquid water path (LWP) among the models is rather large during the first 12 h. From additional sensitivity experiments it is demonstrated that this spread is mainly attributable to differences in the parameterized precipitation rate. The LWP differences are limited through a feedback mechanism in which enhanced drizzle fluxes result in lower entrainment rates and subsequently a reduced drying at cloud top. The spread is furthermore reduced during the day as cloud layers with a greater LWP absorb more solar radiation and hence evaporate more.           ,              Key Points                                                                LES models successful in representing observed ASTEX stratocumulus transition                                                     Main intermodel spread in LWP attributable to microphysical parameterization                                                     Moisture flux from subcloud to stratocumulus layer shows strong diurnal cycle},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/SPF5ZLH9/Van Der Dussen et al. - 2013 - The GASSEUCLIPSE model intercomparison of the str.pdf}
}

@article{vanheerwaardenMicroHHComputationalFluid2017,
  title = {{{MicroHH}} 1.0: A Computational Fluid Dynamics Code for Direct Numerical Simulation and Large-Eddy Simulation of Atmospheric Boundary Layer Flows},
  shorttitle = {{{MicroHH}} 1.0},
  author = {Van Heerwaarden, Chiel C. and Van Stratum, Bart J. H. and Heus, Thijs and Gibbs, Jeremy A. and Fedorovich, Evgeni and Mellado, Juan Pedro},
  year = {2017},
  month = aug,
  journal = {Geoscientific Model Development},
  volume = {10},
  number = {8},
  pages = {3145--3165},
  issn = {1991-9603},
  doi = {10.5194/gmd-10-3145-2017},
  url = {https://gmd.copernicus.org/articles/10/3145/2017/},
  urldate = {2024-01-13},
  abstract = {Abstract. This paper describes MicroHH~1.0, a new and open-source (www.microhh.org) computational fluid dynamics code for the simulation of turbulent flows in the atmosphere. It is primarily made for direct numerical simulation but also supports large-eddy simulation (LES). The paper covers the description of the governing equations, their numerical implementation, and the parameterizations included in the code. Furthermore, the paper presents the validation of the dynamical core in the form of convergence and conservation tests, and comparison of simulations of channel flows and slope flows against well-established test cases. The full numerical model, including the associated parameterizations for LES, has been tested for a set of cases under stable and unstable conditions, under the Boussinesq and anelastic approximations, and with dry and moist convection under stationary and time-varying boundary conditions. The paper presents performance tests showing good scaling from 256 to 32\,768 processes. The graphical processing unit (GPU)-enabled version of the code can reach a speedup of more than an order of magnitude for simulations that fit in the memory of a single GPU.},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/W8B3R5QK/Van Heerwaarden et al. - 2017 - MicroHH 1.0 a computational fluid dynamics code f.pdf}
}

@article{verzijlberghAtmosphericFlowsLarge2021,
  title = {Atmospheric Flows in Large Wind Farms},
  author = {Verzijlbergh, R.A.},
  year = {2021},
  journal = {Europhysics News},
  volume = {52},
  number = {5},
  pages = {20--23},
  issn = {0531-7479, 1432-1092},
  doi = {10.1051/epn/2021502},
  url = {https://www.europhysicsnews.org/10.1051/epn/2021502},
  urldate = {2023-05-31},
  abstract = {As we are transitioning to an energy system based on renewable sources, the atmosphere is becoming one of our primary energy sources. Understanding atmospheric flows through wind farms has become an issue of large economic and societal concern.},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/H2QREFDG/Verzijlbergh - 2021 - Atmospheric flows in large wind farms.pdf}
}

@article{vila-gueraudearellanoRoleBoundaryLayer2011,
  title = {The Role of Boundary Layer Dynamics on the Diurnal Evolution of Isoprene and the Hydroxyl Radical over Tropical Forests},
  author = {{Vil{\`a}-Guerau De Arellano}, Jordi and Patton, Edward G. and Karl, Thomas and Van Den Dries, Kees and Barth, Mary C. and Orlando, John J.},
  year = {2011},
  month = apr,
  journal = {Journal of Geophysical Research},
  volume = {116},
  number = {D7},
  pages = {D07304},
  issn = {0148-0227},
  doi = {10.1029/2010JD014857},
  url = {http://doi.wiley.com/10.1029/2010JD014857},
  urldate = {2024-02-10},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/4I8EHFKG/Vilà-Guerau De Arellano et al. - 2011 - The role of boundary layer dynamics on the diurnal.pdf}
}

@article{Vila2014,
  title = {Shallow Cumulus Rooted in Photosynthesis},
  author = {{Vil{\`a}-Guerau de Arellano}, Jordi and Ouwersloot, Huug G. and Baldocchi, Dennis and Jacobs, Cor M. J.},
  year = {2014},
  journal = {Geophysical Research Letters},
  volume = {41},
  number = {5},
  eprint = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1002/2014GL059279},
  pages = {1796--1802},
  doi = {10.1002/2014GL059279},
  url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1002/2014GL059279},
  abstract = {AbstractWe study the interactions between plant evapotranspiration, controlled by photosynthesis (C3 and C4 grasses), and moist thermals responsible for the formation of shallow cumulus clouds (SCu). Our findings are based on a series of systematic numerical experiments at fine spatial and temporal scales using large eddy simulations explicitly coupled to a plant-physiology model. The shading provided by SCu leads to strong spatial variability in photosynthesis and the surface energy balance. This in turn results in SCu characterized by less extreme and less skewed values of liquid water path. The larger water use efficiency of C4 grass leads to two opposite effects that influence boundary layer clouds: more vigorous and deeper thermals due to the larger buoyancy surface flux (positive effect) characterized by less moisture content (negative). We find that under these midlatitude and well-watered soil conditions, SCu are characterized by a larger cloud cover and liquid water path over C4 grass fields.},
  keywords = {land-atmosphere interaction,photosynthesis,shallow cumulus}
}

@article{vremanEddyviscositySubgridscaleModel2004,
  title = {An Eddy-Viscosity Subgrid-Scale Model for Turbulent Shear Flow: {{Algebraic}} Theory and Applications},
  shorttitle = {An Eddy-Viscosity Subgrid-Scale Model for Turbulent Shear Flow},
  author = {Vreman, A. W.},
  year = {2004},
  month = oct,
  journal = {Physics of Fluids},
  volume = {16},
  number = {10},
  pages = {3670--3681},
  issn = {1070-6631, 1089-7666},
  doi = {10.1063/1.1785131},
  url = {https://pubs.aip.org/pof/article/16/10/3670/255289/An-eddy-viscosity-subgrid-scale-model-for},
  urldate = {2024-01-09},
  abstract = {An eddy-viscosity model is proposed and applied in large-eddy simulation of turbulent shear flows with quite satisfactory results. The model is essentially not more complicated than the Smagorinsky model, but is constructed in such a way that its dissipation is relatively small in transitional and near-wall regions. The model is expressed in first-order derivatives, does not involve explicit filtering, averaging, or clipping procedures, and is rotationally invariant for isotropic filter widths. Because of these highly desirable properties the model seems to be well suited for engineering applications. In order to provide a foundation of the model, an algebraic framework for general three-dimensional flows is introduced. Within this framework several types of flows are proven to have zero energy transfer to subgrid scales. The eddy viscosity is zero in the same cases; the theoretical subgrid dissipation and the eddy viscosity have the same algebraic structure. In addition, the model is based on a fundamental realizability inequality for the theoretical subgrid dissipation. Results are shown for a transitional and turbulent mixing layer at high Reynolds number and a turbulent channel flow. In both cases the present model is found to be more accurate than the Smagorinsky model and as good as the standard dynamic model. Unlike the Smagorinsky model, the present model is able to adequately handle not only turbulent but also transitional flow.},
  langid = {english}
}

@article{wangGPUAwareMPIRDMAEnabled2014,
  title = {{{GPU-Aware MPI}} on {{RDMA-Enabled Clusters}}: {{Design}}, {{Implementation}} and {{Evaluation}}},
  shorttitle = {{{GPU-Aware MPI}} on {{RDMA-Enabled Clusters}}},
  author = {Wang, Hao and Potluri, Sreeram and Bureddy, Devendar and Rosales, Carlos and Panda, Dhabaleswar K.},
  year = {2014},
  month = oct,
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  volume = {25},
  number = {10},
  pages = {2595--2605},
  issn = {1045-9219},
  doi = {10.1109/TPDS.2013.222},
  url = {http://ieeexplore.ieee.org/document/6587715/},
  urldate = {2023-11-02},
  file = {/Users/caspar/Zotero/storage/SUGQ3HW9/Wang et al. - 2014 - GPU-Aware MPI on RDMA-Enabled Clusters Design, Im.pdf}
}

@article{wilhelmsonDirectSolutionsPoisson1977,
  title = {Direct Solutions for {{Poisson}}'s Equation in Three Dimensions},
  author = {Wilhelmson, Robert B. and Ericksen, James H.},
  year = {1977},
  month = dec,
  journal = {Journal of Computational Physics},
  volume = {25},
  number = {4},
  pages = {319--331},
  issn = {00219991},
  doi = {10.1016/0021-9991(77)90001-8},
  url = {https://linkinghub.elsevier.com/retrieve/pii/0021999177900018},
  urldate = {2023-08-18},
  langid = {english}
}

@article{wyngaardNumericalModelingTerra2004,
  title = {Toward {{Numerical Modeling}} in the "{{Terra Incognita}}"},
  author = {Wyngaard, John C.},
  year = {2004},
  month = jan,
  journal = {Journal of the Atmospheric Sciences},
  volume = {61},
  number = {14},
  pages = {1816--1826},
  doi = {10.1175/1520-0469(2004)061%3C1816:TNMITT%3E2.0.CO;2},
  abstract = {In mesoscale modeling the scale l of the energy- and flux-containing turbulence is much smaller than the scale {$\Delta$} of the spatial filter used on the equations of motion, and in large-eddy simulation (LES) it is much larger. Since their models of the subfilter-scale (SFS) turbulence were not designed to be used when l and {$\Delta$} are of the same order, this numerical region can be called the ``terra incognita.'' The most common SFS model, a scalar eddy diffusivity acting on the filtered fields, emerges from the conservation equations for SFS fluxes when several terms, including all but one of the production terms, are neglected. Analysis of data from the recent Horizontal Array Turbulence Study (HATS) shows that the neglected production terms can be significant. Including them in the modeled SFS flux equations yields a more general SFS model, one with a tensor rather than a scalar eddy diffusivity. This more general SFS model is probably not necessary in fine-resolution LES or in coarse-resolution mesoscale modeling, but it could improve model performance in the terra incognita.},
  file = {/Users/caspar/Zotero/storage/QV8V5WQY/Wyngaard - 2004 - Toward Numerical Modeling in the Terra Incognita.pdf}
}

@article{xuArtificialIntelligencePowerful2021,
  title = {Artificial Intelligence: {{A}} Powerful Paradigm for Scientific Research},
  shorttitle = {Artificial Intelligence},
  author = {Xu, Yongjun and Liu, Xin and Cao, Xin and Huang, Changping and Liu, Enke and Qian, Sen and Liu, Xingchen and Wu, Yanjun and Dong, Fengliang and Qiu, Cheng-Wei and Qiu, Junjun and Hua, Keqin and Su, Wentao and Wu, Jian and Xu, Huiyu and Han, Yong and Fu, Chenguang and Yin, Zhigang and Liu, Miao and Roepman, Ronald and Dietmann, Sabine and Virta, Marko and Kengara, Fredrick and Zhang, Ze and Zhang, Lifu and Zhao, Taolan and Dai, Ji and Yang, Jialiang and Lan, Liang and Luo, Ming and Liu, Zhaofeng and An, Tao and Zhang, Bin and He, Xiao and Cong, Shan and Liu, Xiaohong and Zhang, Wei and Lewis, James P. and Tiedje, James M. and Wang, Qi and An, Zhulin and Wang, Fei and Zhang, Libo and Huang, Tao and Lu, Chuan and Cai, Zhipeng and Wang, Fang and Zhang, Jiabao},
  year = {2021},
  month = nov,
  journal = {The Innovation},
  volume = {2},
  number = {4},
  pages = {100179},
  issn = {26666758},
  doi = {10.1016/j.xinn.2021.100179},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S2666675821001041},
  urldate = {2024-02-21},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/ID2DB2IP/Xu et al. - 2021 - Artificial intelligence A powerful paradigm for s.pdf}
}
