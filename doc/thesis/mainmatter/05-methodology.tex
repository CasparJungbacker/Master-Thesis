\chapter{Validation and Benchmarking} \label{chap:val_bench}
This chapter delves into the validation and performance analysis of \acrshort{dales} on the \acrshort{gpu}. First, the test case used for validation is introduced and the results of the validation study are presented. Next, the systems and performance metrics used for examining the performance of the model are explained. Finally, the model performance is discussed.

\section{BOMEX}
The \acrfull{bomex} case has been used for benchmarking and validation of the accelerated version of \acrshort{dales}. This case originates from the field experiment carried out by \citet{hollandMeasurementsAtmosphericMass1973}. During this experiment, observations of horizontal wind components, temperature and specific humidity were made every 90 minutes over a $500 \times 500$ km area in the Atlantic Ocean, east of the island of Barbados. The meteorological conditions during the observed period of five days were relatively constant and gave rise to the development of shallow cumulus convection without the presence of precipitation. Because of these steady conditions, the \acrshort{bomex} observations formed a good base for the \acrshort{les} intercomparison study carried out by \citet{siebesmaLargeEddySimulation2003}. One of the main goals of this study was to compare the ability of different \acrshort{les} models to produce shallow cumulus clouds, including \acrshort{dales}. Since the original publication, the \acrshort{bomex} case has been widely used as a benchmark for new \acrshort{les} models (e.g. \citet{vanheerwaardenMicroHHComputationalFluid2017}, \todo{find another example}). The setup of the \acrshort{bomex} case is described by \citet{siebesmaEvaluationParametricAssumptions1995}. Because of the nature of the case, \acrshort{bomex} only stresses a relatively limited portion of the components of \acrshort{dales}: the dynamical core, which is occupied with solving the horizontal and vertical transport of momentum and scalars, and the moist thermodynamics scheme, responsible for cloud formation. Therefore, \acrshort{bomex} is a good starting point for validation, which can later be expanded on as more components of \acrshort{dales} are accelerated.

\section{Model validation}
Modifying the source code of an application carries the risk of introducing bugs. Therefore, the updated source code was validated against the original unmodified code to ensure that the application logic did not change as a result of the addition of \acrshort{acc} directives and associated optimizations. However, validation of an atmospheric code like \acrshort{dales} is complicated by the fact that weather is a chaotic phenomenon. As described by \citet{lorenzDeterministicNonperiodicFlow1963}, a small change in the initial conditions of a simulation can lead to vastly different outcomes. Furthermore, computers cannot work with infinitely long decimal numbers, which causes intermediate round-off errors during the simulation. These round-off errors can accumulate and influence the simulation. Therefore, the output of two model runs, both started from the same initial conditions, will generally not be identical. In this work, a statistical approach was used to validate the model instead. First, the original \acrshort{cpu} version of \acrshort{dales} was used to create an ensemble data set. For this data set, fifty model runs (also called \emph{ensemble members}) were used, each initialized with a random perturbation applied to the prognostic fields. Next, the ensemble mean and standard deviation were calculated. Similarly, a fifty-member ensemble was generated using the \acrshort{acc} version of \acrshort{dales}. In \Autoref{fig:validation}, the \acrshort{acc} ensemble mean is compared to the statistics of the original ensemble for the liquid water potential temperature $\theta_l$, total water specific humidity $q_t$ (both prognostic variables), total liquid water potential temperature flux $w^\prime \theta_l^\prime$ and the total moisture flux $w^\prime q_t^\prime$ (both diagnostic variables). The plotted quantities were calculated during the last hour of the case. It can be seen that the ensemble mean of the \acrshort{acc} version shows good correspondence with the original version \todo{TODO: bug for fluxes has been found, remake this plot with new ensemble}. Hence, there is little reason to believe that the model physics have changed as a result of the offloading.

\begin{figure}
    \centering
    \includesvg{../images/plots/verify_profiles.svg}
    \caption{Ensemble mean vertical profiles and standard deviation of (a) liquid water potential temperature, (b) total water specific humidity, (c) total liquid water potential temperature flux and (d) total moisture flux.}
    \label{fig:validation}
\end{figure} 

\section{Performance metrics}

\noindent \textbf{Wall clock time and speedup} \\
Wall clock time is the total time it takes for an application to finish executing. In other words, wall clock time is the real-world time that passes while the application is running. When speaking of the \emph{speedup} of an application, what is usually meant is that the wall clock time of that application has reduced. Speedup is often defined as the performance of the improved application relative to the old application for a constant problem size. Mathematically, this is equal to:

\begin{equation}
    \text{Speedup} = \frac{t_{\text{old}}}{t_{\text{new}}},
    \label{eq:speedup}
\end{equation}

in which $t_{\text{old}}$ and $t_{\text{new}}$ are the wall clock time needed to solve the same problem by the old application and improved application respectively. In the context of \acrshort{dales}, a constant problem size means that the size and resolution of the computational grid stay constant and that the same physical schemes are used.

\medskip

\noindent \textbf{Strong scaling} \\
A parallel application may still have sections that have to be executed serially which do not benefit from the addition of more processors. Hence, the speedup that an application obtains from parallelization is ultimately limited by the work that has to be executed serially. This limitation was first quantified by \citet{amdahlValiditySingleProcessor1967} in what is now known as Amdahl's law: 

\begin{equation}
    \text{Speedup} = \frac{1}{(1-f) + \displaystyle \frac{f}{N}}
    \label{eq:amdahls_law}
\end{equation}

\noindent in which $f$ denotes the fraction of the application that can be run in parallel, and $N$ is the number of processors. To illustrate the significance of Amdahl`s law, assume an application that can be fully parallelized. In this case, the fraction $f$ is equal to 1. Notice that under the assumption that $f=1$, \autoref{eq:amdahls_law} reduces to: $\text{Speedup} = N$ . In other words: if the application is executed on, for example, 512 processors, it theoretically runs 512 times faster compared to a single processor. Now, assume that only 99\% of the application can be parallelized. Using $f=0.99$, it follows from \autoref{eq:amdahls_law} that the application only runs 83.8 times faster when using 512 processors. Therefore, to create an efficient parallel application, the sequential portion of the application should be kept as small as possible. The speedup of an application as a function of the number of processors is also called \emph{strong scaling}. To measure the strong scaling performance of an application, the problem size (i.e., the number of grid points) is kept constant and the number of processors is increased. The strong scaling performance helps to determine the optimal amount of resources to use for an application.

\medskip

\noindent \textbf{Weak scaling} \\
\cite{gustafsonReevaluatingAmdahlLaw1988} voiced some skepticism regarding Amdahl's law. He stated that it is not realistic to assume that the problem size stays constant when more processors are available for the task. Instead, it should be assumed that the problem size tends to increase with the number of processors. Gustafson proposed that the speedup should be calculated as:

\begin{equation}
    \text{Speedup} = (1-f) + f \times N.
    \label{eq:gustafsons_law}
\end{equation}

\noindent Now, for the same parallel fraction as above ($f=0.99$), using 512 processors would result in a speedup of 506.89. This is a much more promising number than the upper bound of 83.3 obtained from \autoref{eq:amdahls_law}. The performance of a parallel application with regards to Gustafson's law is called \emph{weak scaling}. Weak scaling performance is a useful metric to assess whether an application can efficiently tackle larger problem sizes for which more computational resources are needed.

\section{System configuration} \label{sec:system_configuration}
Performance testing of \acrshort{dales} was done on systems of two scales: a high-end desktop workstation, and the Snellius supercomputer. The specifications of all tested systems can be found in \Autoref{tab:node_types}. On Snellius, instead of comparing a single \acrshort{gpu} to a (multicore) \acrshort{cpu}, a more fair comparison is to compare one \acrshort{gpu} to a full \acrshort{cpu} node which contains two (multicore) \acrshortpl{cpu}. The reason for this is that on supercomputers like Snellius, accounting of the used resources happens through \acrfullpl{sbu}. When a proposal for a certain project is granted computational resources, an amount of \acrshortpl{sbu} is obtained. As computing resources are being used, \acrshortpl{sbu} are subtracted from the granted budget depending on the type and amount of systems and the run time of the application. On Snellius, one \acrshort{gpu} costs the same amount of \acrshortpl{sbu} per hour as a full \acrshort{cpu} node.

\begin{table}[H]
    \caption{Specification of the computer systems used for performance testing of the \acrshort{cpu} and \acrshort{gpu} versions of \acrshort{dales}.}
    \label{tab:node_types}
    \begin{tabular}{l|c|c|c}
     & \textbf{Desktop system} & \textbf{Snellius CPU node} & \textbf{Snellius GPU node} \\ \hline
    \textbf{CPU} & Intel Core i9 11900K & AMD EPYC 7H12 & Intel Xeon Platinum 8360Y \\
    \textbf{CPU clock speed} & \begin{tabular}[c]{@{}c@{}}3.50 GHz\\ (5.40 GHz Turbo)\end{tabular} & \begin{tabular}[c]{@{}c@{}}2.60 GHz\\ (3.30 GHz Turbo)\end{tabular} & \begin{tabular}[c]{@{}c@{}}2.40 GHz\\ (3.50 GHz Turbo)\end{tabular} \\
    \textbf{CPU cores} & 8 & \begin{tabular}[c]{@{}c@{}}64 per socket\\ (2 sockets per node)\end{tabular} & \begin{tabular}[c]{@{}c@{}}36 per socket\\ (2 sockets per node)\end{tabular} \\
    \textbf{Memory} & 64 GB & 256 GB & 512 GB \\
    \textbf{GPU} & NVIDIA RTX 3090 & - & NVIDIA A100 SXM \\
    \textbf{GPU memory} & 24 GB & - & 40 GB
    \end{tabular}
\end{table}

\section{Speedup} \label{sec:speedup}
Wall-clock time and speedup of the \acrshort{gpu}-accelerated \acrshort{dales} were measured on both the desktop system and the Snellius supercomputer. This was done for multiple horizontal grid configurations, starting from 128 $\times$ 128 grid points up to 1024 $\times$ 512 on the desktop system and 1024 $\times$ 1024 grid points on Snellius. Each run was limited to 100 time steps, and only the wall-clock time of the time loop itself (i.e., excluding the startup phase of the model) was measured.

\autoref{fig:speedup_desktop} shows the wall-clock time and speedup as measured on the desktop system. Speedup was calculated according to \Autoref{eq:speedup}, where $t_{old}$ denotes the \acrshort{cpu} run and $t_{new}$ denotes the \acrshort{gpu} run, and was only considered for the multi-\acrshort{cpu} case. Speedup was found to increase with domain size until the maximum domain size of 1024 $\times$ 512 grid points was reached. This trend indicates that the overhead related to \acrshort{gpu} execution (sending data, sending instructions) is relatively large for small problem sizes, but decreases with problem size.

As discussed in \Autoref{sec:system_configuration}, on Snellius, a single \acrshort{gpu} was compared to a full \acrshort{cpu} node, which hosts two 64-core \acrshortpl{cpu} (\Autoref{tab:node_types}). Hence, in this case ``speedup'' can also be interpreted as ``cost savings''. Similar to the desktop system, speedup increased with domain size. The absolute value of the speedup was lower compared to the desktop system, however, as the Snellius \acrshort{cpu} node features more cores in total. For the smallest two problem sizes, the \acrshort{cpu} node was faster than the \acrshort{gpu}.

Comparing \Autoref{fig:speedup_desktop} to \Autoref{fig:speedup_snellius}, it can be seen that the A100 \acrshort{gpu} in the Snellius system only marginally outperformed the RTX 3090 \acrshort{gpu}. This is an interesting observation, as the A100 offers more than 17 times the performance in double-precision calculations compared to the RTX 3090 \citep{nvidiaNVIDIAA100Tensor2020,nvidiaNVIDIAAmpereGA1022021}. This indicates that the main factor preventing speedup is not the amount of cores in a \acrshort{gpu}. \citet{janssonCloudBotanyShallow2023} mention that \acrshort{dales} was found to be limited by memory bandwidth. In other words, \acrshort{dales} performs relatively few arithmetic operations to the amount of data it has to read from and write to memory. 

Not related to \acrshortpl{gpu} but curious nonetheless is that a single desktop \acrshort{cpu} significantly outperformed a single \acrshort{cpu} of a Snellius \acrshort{cpu} node. This can be explained by the fact that the Core i9 processor in the desktop system has a higher turbo clock speed. \acrshort{cpu} clock speed denotes how many instructions a \acrshort{cpu} can process in a second \citep{rauberParallelProgrammingMulticore2023}. In the context of scientific computing, a higher clock speed generally leads to more computations per second but also increases power usage and heat generation by the \acrshort{cpu}, which is why the turbo clock speed cannot be sustained for long periods. Overheating is generally not a problem when only a single core of the \acrshort{cpu} is performing at the turbo clock speed, however, from which the desktop \acrshort{cpu} can benefit in this test.

\begin{figure}[h!]
    \centering
    \includesvg{../images/plots/speedup_wiske.svg}
    \caption{Speedup for different horizontal grid sizes on the desktop system (\Autoref{tab:node_types}).}
    \label{fig:speedup_desktop}
\end{figure}

\begin{figure}[h!]
    \centering
    \includesvg{../images/plots/speedup_snellius.svg}
    \caption{Speedup for different horizontal grid sizes on Snellius.}
    \label{fig:speedup_snellius}
\end{figure}

\section{Scaling}
Strong and weak scaling tests were performed exclusively on Snellius, as the desktop system only features a single \acrshort{gpu}. For strong scaling, a horizontal grid size of 1024 $\times$ 1024 points was used, which nearly fully utilized the available 40Gb of memory of a single NVIDIA A100 \acrshort{gpu}. Then, while keeping the domain size constant, the number of \acrshortpl{gpu} was doubled for each subsequent simulation until a maximum of 64 \acrshortpl{gpu} was reached. The task of choosing a suitable domain decomposition was left to \acrshort{mpi}, leading to a slab decomposition along the $y$-direction in all cases. The wall-clock time of the time loop itself (i.e., excluding the start-up phase of the model) was measured during the first 100 time steps and averaged. \autoref{fig:strong_scaling} shows the average wall-clock time per timestep as a function of the number of \acrshortpl{gpu} used. 

To measure weak scaling performance, a horizontal domain size of 1024 $\times$ 1024 grid points with a grid spacing of 100 m in both directions was used per \acrshort{gpu}. Next, the number of \acrshortpl{gpu} was doubled per simulation, therefore also doubling the total domain size. The domain was extended alternating in the $x$ and $y$ direction. Again, the wall-clock time of the time loop was averaged over the first 100 times steps. Both a slab decomposition along the $y$-direction and a pencil decomposition of the domain were tested. In \Autoref{fig:weak_scaling}, the ratio of the wall-clock time measured for multi-\acrshort{gpu} runs to the single-\acrshort{gpu} run is plotted. A wall-clock time ratio greater than one indicates a slowdown. In the ideal case, indicated by the horizontal black line, the runtime would not increase with the number of \acrshortpl{gpu} as the workload per \acrshort{gpu} stays constant. In reality, however, inter-\acrshort{gpu} communication introduces a performance penalty that can grow quite significant. From \Autoref{fig:weak_scaling}, it can be seen that this performance penalty is dependent on the number of \acrshortpl{gpu} used. This is expected, as the amount of data that has to be communicated also increases with the number of \acrshort{gpu}. Finally, the pencil decomposition showed strikingly worse performance than the slab decomposition, which also worsened with the number of \acrshortpl{gpu}. This could be explained by the fact that in the setting of a slab decomposition, each \acrshort{gpu} contains the full $x$ and $z$ directions of the domain, thus requiring two less collective inter-\acrshort{gpu} communications in the pressure solver.

\Autoref{fig:speedup_per_module} provides further evidence towards the hypothesis that communications hinder performance in multi-\acrshort{gpu} simulations. Here, the speedup for different computationally expensive modules of \acrshort{dales} is plotted as measured in the strong scaling test with 4 \acrshort{gpu}. It can be seen that the advection, thermodynamics and subgrid modules experienced a speedup that was close to the ideal value of 4. The Poisson equation solver that contains the global transpose operations as described in \Autoref{sec:poisson_solver} and therefore heavily relies on communication between \acrshortpl{gpu} showed a \emph{slowdown} compared to the single-\acrshort{gpu} case. The bar labeled ``Other'' contains, among other things, the routines that manage the exchange of data at the boundaries of the \acrshort{mpi} sub-domains. Again, the absence of inter-\acrshort{gpu} communication in a single-\acrshort{gpu} simulation explains why a sub-optimal speedup was observed for this category.

\begin{figure}[h!]
    \centering
    \includesvg{../images/plots/strong_scaling.svg}
    \caption{Strong scaling on Snellius for a horizontal domain size of 1024 $\times$ 1024 grid points.}
    \label{fig:strong_scaling}
\end{figure}

\begin{figure}[h!]
    \centering
    \includesvg{../images/plots/speedup_per_module.svg}
    \caption{Speedup of the computationally most expensive modules of \acrshort{dales} when using 4 \acrshortpl{gpu} on a horizontal domain of 1024 $\times$ 1024 grid points.}
    \label{fig:speedup_per_module}
\end{figure}

\begin{figure}[h!]
    \centering
    \includesvg{../images/plots/weak_scaling.svg}
    \caption{Weak scaling on Snellius for a horizontal domain size of 1024 $\times$ 1024 grid points per \acrshort{gpu}}
    \label{fig:weak_scaling}
\end{figure}

\newpage

\section{Single precision calculations}
\acrshort{dales} offers the option to store the prognostic variables as single-precision floating point numbers \citep{janssonCloudBotanyShallow2023}. As a single-precision floating point number consists of half as many bits as a double-precision floating point number (the default precision in \acrshort{dales}), it can be read from and written to memory much quicker, offering possibly a very significant performance enhancement for a memory bandwidth-bound application like \acrshort{dales}. Moreover, \acrshortpl{gpu}, specifically \acrshortpl{gpu} that are produced for the consumer market, often contain significantly more cores for single-precision calculations than for double-precision calculations. For example, the NVIDIA RTX 3090 \acrshort{gpu} present in the tested desktop system (\autoref{tab:node_types}) features just two double-precision cores per \acrshort{sm} for a total of 164 cores, versus 128 single-precision cores per \acrshort{sm} for a total of 10,496 cores \citep{nvidiaNVIDIAAmpereGA1022021}. Therefore, switching from double-precision calculations to single-precision calculations should, in theory, lead to a 64-fold speedup on the RTX 3090. 

To assess whether or not the usage of single-precision for the prognostic variables leads to any significant performance increase, \acrshort{dales} was recompiled with single-precision enabled and the simulations described in \Autoref{sec:speedup} were repeated on the \acrshort{gpu} of the desktop system. The speedup of the single-precision simulations over the double-precision simulations can be found in \Autoref{fig:single_vs_double_precision} for different horizontal grid configurations. Interestingly enough, single-precision simulations were only found to be about 1.5 times faster than double-precision simulations, with a very slight dependency on grid size.

\begin{figure}[h!]
    \centering
    \includesvg{../images/plots/single_vs_double_precision.svg}
    \caption{Obtained speedup when using single-precision floating-point numbers for the prognostic variables.}
    \label{fig:single_vs_double_precision}
\end{figure}

Why did the usage of single-precision floating-point numbers for the prognostic variables not lead to a speedup of 64 compared to double-precision? First, such a speedup can only arise in the absence of any memory considerations. As has been mentioned several times, \acrshort{dales} is very likely a memory-bound application and a speedup of the order of 64 was therefore already highly unlikely. A closer inspection of the source code of \acrshort{dales} revealed another issue. Most Fortran compilers, including the GFortran and NVFortran compilers used in this work, use single-precision floating-point numbers for any real-valued variable (e.g. velocity, temperature, grid spacing, et cetera). For compatibility reasons, however, \acrshort{dales} has to be compiled with the option \texttt{-fdefaultreal8} (GFortran) or \texttt{-Mr8} (NVFortran), which increases the default precision of real values to double precision. This means that all calculations in \acrshort{dales} are done in double precision. When the single-precision option for prognostic variables as described by \citet{janssonCloudBotanyShallow2023} is used, \emph{exclusively} the prognostic variables are stored in single-precision format. All other real-valued variables and parameters are nevertheless compiled with double precision. When a single-precision variable encounters a double-precision variable in a calculation, it has to be temporarily converted to a double-precision floating-point number for the computer to be able to execute the calculation. This means that all calculations in \acrshort{dales} are executed in double precision, regardless of the precision used for the prognostic variables. For this reason, the single-precision cores of the RTX 3090 \acrshort{gpu} were still not utilized in this test. The observed speedup in \Autoref{fig:single_vs_double_precision} can therefore be attributed to savings in memory bandwidth by the reduced precision of the prognostic variables.