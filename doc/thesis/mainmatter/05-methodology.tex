\chapter{Validation and Benchmarking}

\section{BOMEX}
... the \acrfull{bomex} case has been used for benchmarking and validation of the accelerated version of \acrshort{dales}. The \acrshort{bomex} case originates from the field experiment carried out by \citet{hollandMeasurementsAtmosphericMass1973}. During this experiment, observations of horizontal wind components, temperature and specific humidity were made every 90 minutes over a $500 \times 500$ km area in the Atlantic Ocean, east of the island of Barbados. The meteorological conditions during the observed period of five days were relatively constant and gave rise to the development of shallow cumulus convection without the presence of precipitation. Because of these steady conditions, the \acrshort{bomex} observations formed a good base for the \acrshort{les} intercomparison study carried out by \citet{siebesmaLargeEddySimulation2003}. One of the main goals of this study was to compare the ability of different \acrshort{les} models to produce shallow cumulus clouds, including \acrshort{dales}. Since the original publication, the \acrshort{bomex} case has been widely used as a benchmark for new \acrshort{les} models (e.g. \citet{vanheerwaardenMicroHHComputationalFluid2017}, \todo{find another example}). The setup of the \acrshort{bomex} case is described by \citet{siebesmaEvaluationParametricAssumptions1995}. Because of the nature of the case, \acrshort{bomex} only stresses a relatively limited portion of the components of \acrshort{dales}. Therefore, \acrshort{bomex} is a good starting point for validation, which can later be expanded as more components of \acrshort{dales} are accelerated.

\section{Model validation}
Modifying the source code of an application carries the risk of introducing bugs. Therefore, the updated source code needs to be validated against the original unmodified code to ensure that the program logic stays intact. However, validation is complicated by the fact that weather is a chaotic phenomeon. As described by \citet{lorenzDeterministicNonperiodicFlow1963}, a small change of the initial conditions of a simulation can lead to vastly different outcomes. Intermediate rounding, as done by a computer, can have the same effect, as often the order of arithmetic operations is not constant. Therefore, directly comparing the output of two different model runs, both started from the exact same initial conditions, will generally not be useful towards validation. In this work, a statistical approach was used to validate the model. First, \acrshort{dales} version 4.4 was used to create an ensemble data set. For this data set, twenty model runs (also called \emph{ensemble members}) were used, each initialized with random perturbations of the prognostic fields. Next, the ensemble mean and standard deviation were calculated. 

\section{Performance metrics}

\noindent \textbf{Wall clock time and speedup} \\
Wall clock time is the total time it takes for an application to finish executing. In other words, wall clock time is the real-world time that passes while the application is running. When speaking of the \emph{speedup} of an application, what is usually meant is that the wall clock time of that application has reduced. Speedup is often defined as the performance of the improved application relative to the old application for a constant problem size. Mathematically, this is equal to:

\begin{equation}
    \text{Speedup} = \frac{t_{\text{old}}}{t_{\text{new}}},
\end{equation}

in which $t_{\text{old}}$ and $t_{\text{new}}$ are the wall clock time needed to solve the same problem by the old application and improved application respectively. In the context of \acrshort{dales}, a constant problem size means that the size and resolution of the computational grid stay constant and that the same physical schemes are used.

\medskip

\noindent \textbf{Strong scaling} \\
A parallel program may still have sections that have to be executed serially which do not benefit from the addition of more processors. Hence, the speedup that a program obtains from parallelization is ultimately limited by the work that has to be executed serially. This limitation was first quantified by \citet{amdahlValiditySingleProcessor1967} in what is now known as Amdahl's law: 

\begin{equation}
    \text{Speedup} = \frac{1}{(1-f) + \displaystyle \frac{f}{N}}
    \label{eq:amdahls_law}
\end{equation}

\noindent in which $f$ denotes the fraction of the program that is parallelizable, and $N$ is the number of processors. To illustrate the significance of Amdahl`s, assume a program that can be fully parallelized. In this case, the fraction $f$ is equal to 1. Notice that under the assumption that $f=1$, \autoref{eq:amdahls_law} reduces to: $\text{Speedup} = N$ . In other words: if the program is executed on, for example, 512 processors, it theoretically runs 512 times faster compared to a single processor. Now, assume that only 99\% of the program can be parallelized. Using $f=0.99$, it follows from \autoref{eq:amdahls_law} that the program only runs 83.8 times faster when using 512 processors. Therefore, to create an efficient parallel program, the sequential portion of the program should be kept as small as possible. The speedup of a program as a function of number of processors is also called \emph{strong scaling}. To measure the strong scaling performance of a program, the problem size (i.e., the number of grid points) is kept constant and the number of processors is increased. The strong scaling performance helps to determine the optimal amount of resources to use for a program.

\medskip

\noindent \textbf{Weak scaling} \\
\cite{gustafsonReevaluatingAmdahlLaw1988} voiced some skepticism regarding Amdahl's law. He stated that it is not realistic to assume that the problem size stays constant when more processors are available for the task. Instead, it should be assumed that the problem size tends to increase with the number of processors. Gustafson proposed that the speedup should be calculated as:

\begin{equation}
    \text{Speedup} = (1-f) + f \times N.
    \label{eq:gustafsons_law}
\end{equation}

\noindent Now, for the same parallel fraction as above ($f=0.99$), using 512 processors would result in a speedup of 506.89. This is a much more promising number than the upper bound of 83.3 obtained from \autoref{eq:amdahls_law}. 

\medskip

\noindent \textbf{Energy consumption} \\
In the age of climate change, energy consumption is an important consideration for HPC applications. \todo{mention how GPUs are more energy efficient than CPUs, refer back to section on GPU architecture} On the Snellius supercomputer, the energy consumption of a job can be measured using the Energy Aware Runtime (EAR) package. When enabled, EAR keeps track of multiple energy-related metrics such as average node power, consumed energy and gigaFLOPS per Watt \citep{corbalanEnergyOptimizationAnalysis2020}. Because EAR monitors power consumption at the node level, the contributions of both CPU and GPU are included in these metrics.

\section{System configuration}
Performance testing of \acrshort{dales} was done on systems of two scales: a high-end dekstop workstation, and the Snellius supercomputer. 
\begin{table}[H]
    \caption{}
    \label{tab:node_types}
    \begin{tabular}{l|c|c|c}
     & \textbf{Desktop system} & \textbf{Snellius CPU node} & \textbf{Snellius GPU node} \\ \hline
    \textbf{CPU} & Intel Core i9 11900K & AMD EPYC 7H12 & Intel Xeon Platinum 8360Y \\
    \textbf{CPU clock speed} & \begin{tabular}[c]{@{}c@{}}3.50 GHz\\ (5.40 GHz Turbo)\end{tabular} & \begin{tabular}[c]{@{}c@{}}2.60 GHz\\ (3.30 GHz Turbo)\end{tabular} & \begin{tabular}[c]{@{}c@{}}2.40 GHz\\ (3.50 GHz Turbo)\end{tabular} \\
    \textbf{CPU cores} & 8 & \begin{tabular}[c]{@{}c@{}}64 per socket\\ (2 sockets per node)\end{tabular} & \begin{tabular}[c]{@{}c@{}}36 per socket\\ (2 sockets per node)\end{tabular} \\
    \textbf{Memory} & 64 GB & 256 GB & 512 GB \\
    \textbf{GPU} & NVIDIA RTX 3090 & - & NVIDIA A100 SXM \\
    \textbf{GPU memory} & 24 GB & - & 40 GB
    \end{tabular}
\end{table}
\section{Single precision calculations}
\acrshort{dales} offers the option to store the prognostic variables as single-precision floating point numbers \citep{janssonCloudBotanyShallow2023}. As a single-precision floating point number consists of half as many bits as a double-precision floating point number (the default precision in \acrshort{dales}), it can be read from and written to memory much quicker, offering possibly a very significant performance enhancement for a memory bandwidth-bound application like \acrshort{dales}. Moreover, \acrshortpl{gpu}, specifically \acrshortpl{gpu} that are produced for the consumer market, often contain significantly more cores for single-precision calculations than for double-precision calculations. For example, the NVIDIA RTX 3090 \acrshort{gpu} present in the tested desktop system (\autoref{tab:node_types}) features just two double-precision cores per \acrshort{sm} for a total of 164 cores, versus 128 single-precision cores per \acrshort{sm} for a total of 10,496 cores \citep{nvidiaIntroductionNVIDIAAmpere2021}. Therefore, switching from double-precision calculations to single-precision calculations should, in theory, lead to a 64-fold speedup on the RTX 3090. 

To assess whether or not the usage of single-precision for the prognostic variables leads to any significant performance increase, \acrshort{dales} was recompiled with single-precision enabled and the simulations described in \Autoref{sec:speedup} were repeated on the \acrshort{gpu} of the desktop system. The speedup of the single-precision simulations over the double-precision simulations can be found in \Autoref{fig:single_vs_double_precision} for different horizontal grid configurations. Interestingly enough, single-precision simulations were only found to be about 1.5 times faster than double-precision simulations, with a very slight dependency on grid size.

Why did the usage of single-precision floating-point numbers for the prognostic variables not lead to a speedup of 64 compared to double-precision? First, such a speedup can only arise in the absence of any memory considerations. As has been mentioned several times, \acrshort{dales} is very likely a memory-bound application and a speedup of the order of 64 was therefore already highly unlikely. A closer inspection of the source code of \acrshort{dales} revealed another issue. Most Fortran compilers, including the GFortran and NVFortran compilers used in this work, use single-precision floating-point numbers for any real-valued variable (e.g. velocity, temperature, grid spacing, et cetera). For compatibility reasons, however, \acrshort{dales} has to be compiled with the option \texttt{-fdefaultreal8} (GFortran) or \texttt{-Mr8} (NVFortran), which increases the default precision of real values to double precision. This means that all calculations in \acrshort{dales} are done in double precision. When the single-precision option for prognostic variables as described by \citet{janssonCloudBotanyShallow2023} is used, \emph{exclusively} the prognostic variables are stored in single-precision format. All other real-valued variables and parameters are nevertheless compiled with double precision. When a single-precision variable encounters a double-precision variable in a calculation, it has to be temporarily converted to a double-precision floating-point number for the computer to be able to execute the calculation. This means that all calculations in \acrshort{dales} are executed in double precision, regardless of the precision used for the prognostic variables. For this reason, the single-precision cores of the RTX 3090 \acrshort{gpu} were still not utilized in this test. The observed speedup in \Autoref{fig:single_vs_double_precision} can therefore be attributed to savings in memory bandwidth by the reduced precision of the prognostic variables.

\begin{figure}[h!]
    \centering
    \includesvg{../images/plots/single_vs_double_precision.svg}
    \caption{Speedup of }
    \label{fig:single_vs_double_precision}
\end{figure}
