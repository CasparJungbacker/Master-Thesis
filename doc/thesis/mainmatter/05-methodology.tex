\chapter{Validation and Benchmarking Methodology}

\section{Performance metrics}

\noindent \textbf{Wall clock time} \\
The time that it takes for a program to finish executing is an intuitive metric to assess performance. 

\medskip

\noindent \textbf{Strong scaling} \\
A parallel program may still have sections that have to be executed serially which do not benefit from the addition of more processors. Hence, the speedup that a program obtains from parallelization is ultimately limited by the work that has to be executed serially. This limitation was first quantified by \citet{amdahlValiditySingleProcessor1967} in what is now known as Amdahl's law: 

\begin{equation}
    \text{Speedup} = \frac{1}{(1-f) + \displaystyle \frac{f}{N}}
    \label{eq:amdahls_law}
\end{equation}

\noindent in which $f$ denotes the fraction of the program that is parallelizable, and $N$ is the number of processors. To illustrate the significance of Amdahl`s, assume a program that can be fully parallelized. In this case, the fraction $f$ is equal to 1. Notice that under the assumption that $f=1$, \autoref{eq:amdahls_law} reduces to: $\text{Speedup} = N$ . In other words: if the program is executed on, for example, 512 processors, it theoretically runs 512 times faster compared to a single processor. Now, assume that only 99\% of the program can be parallelized. Using $f=0.99$, it follows from \autoref{eq:amdahls_law} that the program only runs 83.8 times faster when using 512 processors. Therefore, to create an efficient parallel program, the sequential portion of the program should be kept as small as possible. The speedup of a program as a function of number of processors is also called \emph{strong scaling}. To measure the strong scaling performance of a program, the problem size (i.e., the number of grid points) is kept constant and the number of processors is increased. The strong scaling performance helps to determine the optimal amount of resources to use for a program.

\medskip

\noindent \textbf{Weak scaling} \\
\cite{gustafsonReevaluatingAmdahlLaw1988} voiced some skepticism regarding Amdahl's law. He stated that it is not realistic to assume that the problem size stays constant when more processors are available for the task. Instead, it should be assumed that the problem size tends to increase with the number of processors. Gustafson proposed that the speedup should be calculated as:

\begin{equation}
    \text{Speedup} = (1-f) + f \times N.
    \label{eq:gustafsons_law}
\end{equation}

\noindent Now, for the same parallel fraction as above ($f=0.99$), using 512 processors would result in a speedup of 506.89. This is a much more promising number than the upper bound of 83.3 obtained from \autoref{eq:amdahls_law}. 

\medskip

\noindent \textbf{Energy consumption} \\
In the age of climate change, energy consumption is an important consideration for HPC applications. \todo{mention how GPUs are more energy efficient than CPUs, refer back to section on GPU architecture} On the Snellius supercomputer, the energy consumption of a job can be measured using the Energy Aware Runtime (EAR) package. When enabled, EAR keeps track of multiple energy-related metrics such as average node power, consumed energy and gigaFLOPS per Watt \citep{corbalanEnergyOptimizationAnalysis2020}. Because EAR monitors power consumption at the node level, the contributions of both CPU and GPU are included in these metrics.