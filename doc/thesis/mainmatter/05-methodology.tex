\chapter{Validation and Benchmarking}

\section{BOMEX}
... the \acrfull{bomex} case has been used for benchmarking and validation of the accelerated version of \acrshort{dales}. The \acrshort{bomex} case originates from the field experiment carried out by \citet{hollandMeasurementsAtmosphericMass1973}. During this experiment, observations of horizontal wind components, temperature and specific humidity were made every 90 minutes over a $500 \times 500$ km area in the Atlantic Ocean, east of the island of Barbados. The meteorological conditions during the observed period of five days were relatively constant and gave rise to the development of shallow cumulus convection without the presence of precipitation. Because of these steady conditions, the \acrshort{bomex} observations formed a good base for the \acrshort{les} intercomparison study carried out by \citet{siebesmaLargeEddySimulation2003}. One of the main goals of this study was to compare the ability of different \acrshort{les} models to produce shallow cumulus clouds, including \acrshort{dales}. Since the original publication, the \acrshort{bomex} case has been widely used as a benchmark for new \acrshort{les} models (e.g. \citet{vanheerwaardenMicroHHComputationalFluid2017}, \todo{find another example}). The setup of the \acrshort{bomex} case is described by \citet{siebesmaEvaluationParametricAssumptions1995}. Because of the nature of the case, \acrshort{bomex} only stresses a relatively limited portion of the components of \acrshort{dales}. Therefore, \acrshort{bomex} is a good starting point for validation, which can later be expanded as more components of \acrshort{dales} are accelerated.

\section{Model validation}
Modifying the source code of an application carries the risk of introducing bugs. Therefore, the updated source code needs to be validated against the original unmodified code to ensure that the program logic stays intact. However, validation is complicated by the fact that weather is a chaotic phenomeon. As described by \citet{lorenzDeterministicNonperiodicFlow1963}, a small change of the initial conditions of a simulation can lead to vastly different outcomes. Intermediate rounding, as done by a computer, can have the same effect, as often the order of arithmetic operations is not constant. Therefore, directly comparing the output of two different model runs, both started from the exact same initial conditions, will generally not be useful towards validation. In this work, a statistical approach was used to validate the model. First, \acrshort{dales} version 4.4 was used to create an ensemble data set. For this data set, twenty model runs (also called \emph{ensemble members}) were used, each initialized with random perturbations of the prognostic fields. Next, the ensemble mean and standard deviation were calculated. 

\section{Performance metrics}

\noindent \textbf{Wall clock time and speedup} \\
Wall clock time is the total time it takes for an application to finish executing. In other words, wall clock time is the real-world time that passes while the application is running. When speaking of the \emph{speedup} of an application, what is usually meant is that the wall clock time of that application has reduced. Speedup is often defined as the performance of the improved application relative to the old application for a constant problem size. Mathematically, this is equal to:

\begin{equation}
    \text{Speedup} = \frac{t_{\text{old}}}{t_{\text{new}}},
\end{equation}

in which $t_{\text{old}}$ and $t_{\text{new}}$ are the wall clock time needed to solve the same problem by the old application and improved application respectively. In the context of \acrshort{dales}, a constant problem size means that the size and resolution of the computational grid stay constant and that the same physical schemes are used.

\medskip

\noindent \textbf{Strong scaling} \\
A parallel program may still have sections that have to be executed serially which do not benefit from the addition of more processors. Hence, the speedup that a program obtains from parallelization is ultimately limited by the work that has to be executed serially. This limitation was first quantified by \citet{amdahlValiditySingleProcessor1967} in what is now known as Amdahl's law: 

\begin{equation}
    \text{Speedup} = \frac{1}{(1-f) + \displaystyle \frac{f}{N}}
    \label{eq:amdahls_law}
\end{equation}

\noindent in which $f$ denotes the fraction of the program that is parallelizable, and $N$ is the number of processors. To illustrate the significance of Amdahl`s, assume a program that can be fully parallelized. In this case, the fraction $f$ is equal to 1. Notice that under the assumption that $f=1$, \autoref{eq:amdahls_law} reduces to: $\text{Speedup} = N$ . In other words: if the program is executed on, for example, 512 processors, it theoretically runs 512 times faster compared to a single processor. Now, assume that only 99\% of the program can be parallelized. Using $f=0.99$, it follows from \autoref{eq:amdahls_law} that the program only runs 83.8 times faster when using 512 processors. Therefore, to create an efficient parallel program, the sequential portion of the program should be kept as small as possible. The speedup of a program as a function of number of processors is also called \emph{strong scaling}. To measure the strong scaling performance of a program, the problem size (i.e., the number of grid points) is kept constant and the number of processors is increased. The strong scaling performance helps to determine the optimal amount of resources to use for a program.

\medskip

\noindent \textbf{Weak scaling} \\
\cite{gustafsonReevaluatingAmdahlLaw1988} voiced some skepticism regarding Amdahl's law. He stated that it is not realistic to assume that the problem size stays constant when more processors are available for the task. Instead, it should be assumed that the problem size tends to increase with the number of processors. Gustafson proposed that the speedup should be calculated as:

\begin{equation}
    \text{Speedup} = (1-f) + f \times N.
    \label{eq:gustafsons_law}
\end{equation}

\noindent Now, for the same parallel fraction as above ($f=0.99$), using 512 processors would result in a speedup of 506.89. This is a much more promising number than the upper bound of 83.3 obtained from \autoref{eq:amdahls_law}. 

\medskip

\noindent \textbf{Energy consumption} \\
In the age of climate change, energy consumption is an important consideration for HPC applications. \todo{mention how GPUs are more energy efficient than CPUs, refer back to section on GPU architecture} On the Snellius supercomputer, the energy consumption of a job can be measured using the Energy Aware Runtime (EAR) package. When enabled, EAR keeps track of multiple energy-related metrics such as average node power, consumed energy and gigaFLOPS per Watt \citep{corbalanEnergyOptimizationAnalysis2020}. Because EAR monitors power consumption at the node level, the contributions of both CPU and GPU are included in these metrics.

\section{System configuration}
Performance testing of \acrshort{dales} was done on systems of two scales: a high-end dekstop workstation, and the Snellius supercomputer. 
\begin{table}[H]
    \caption{}
    \label{tab:node_types}
    \begin{tabular}{l|c|c|c}
     & \textbf{Desktop system} & \textbf{Snellius CPU node} & \textbf{Snellius GPU node} \\ \hline
    \textbf{CPU} & Intel Core i9 11900K & AMD EPYC 7H12 & Intel Xeon Platinum 8360Y \\
    \textbf{CPU clock speed} & \begin{tabular}[c]{@{}c@{}}3.50 GHz\\ (5.40 GHz Turbo)\end{tabular} & \begin{tabular}[c]{@{}c@{}}2.60 GHz\\ (3.30 GHz Turbo)\end{tabular} & \begin{tabular}[c]{@{}c@{}}2.40 GHz\\ (3.50 GHz Turbo)\end{tabular} \\
    \textbf{CPU cores} & 8 & \begin{tabular}[c]{@{}c@{}}64 per socket\\ (2 sockets per node)\end{tabular} & \begin{tabular}[c]{@{}c@{}}36 per socket\\ (2 sockets per node)\end{tabular} \\
    \textbf{Memory} & 64 GB & 256 GB & 512 GB \\
    \textbf{GPU} & NVIDIA RTX 3090 & - & NVIDIA A100 SXM \\
    \textbf{GPU memory} & 24 GB & - & 40 GB
    \end{tabular}
\end{table}
