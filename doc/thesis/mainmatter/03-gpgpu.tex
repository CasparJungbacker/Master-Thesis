\chapter{Graphics Processing Units} \label{chap:gpgpu}
This chapter starts with an introduction to the technology behind \acrfullpl{gpu} and how it compares to \acrfullpl{cpu}. Next, different methods of \acrshort{gpu} programming are presented. Third, some examples of how \acrshortpl{gpu} are applied to accelerate \acrshort{cfd} applications are given. Finally, some trends in the field of \acrshort{gpu} computing are discussed. 


\section{\acrshort{cpu} and \acrshort{gpu} architecture}
The main component of a \acrshort{cpu} is the \emph{core}, which performs arithmetic operations. Nowadays, \acrshortpl{cpu} often consist of multiple cores, from which the term \emph{multicore processor} arises \citep{rauberParallelProgrammingMulticore2023}. These cores each have private access to a small amount of fast memory called \emph{cache memory}. Cores are connected via an \emph{interconnection network} to form a \acrshort{cpu}. The main memory is not part of the \acrshort{cpu} and is physically located somewhere else in the computer. A schematic of a multicore \acrshort{cpu} can be found in \Autoref{fig:cpu_diagram}. In a \acrshort{cpu}, each core can be controlled individually, which makes the \acrshort{cpu} a \acrfull{mimd} system; each core can fetch instructions by itself at any time and it does not have to wait until all other cores in the \acrshort{cpu} have finished executing \citep{flynnVeryHighspeedComputing1966}. In practice, the \acrshort{mimd} design of \acrshortpl{cpu} means that each core can be occupied with a different application. A modern high-end CPU can have in the order of tens of cores.

\begin{figure}
    \centering
    \includegraphics[scale=0.3]{../images/drawings/cpu_diagram.png}
    \caption{Diagram of a modern multicore \acrshort{cpu} \citep{rauberParallelProgrammingMulticore2023}. The arrow indicates a connection to other components of the computer, such as the main memory.}
    \label{fig:cpu_diagram}
\end{figure}

The \acrshort{gpu} was originally invented to accelerate computations revolving around 3D computer graphics \citep{aamondtGeneralPurposeGraphicsProcessing2018}. In the early 2000s, it was discovered that \acrshort{gpu}s could also be used for general calculations. Specifically, \acrshort{gpu}s were found to offer enormous performance benefits over the traditional \acrshort{cpu} for highly parallel computations, such as the matrix-matrix product \citep{larsenFastMatrixMultiplies2001}. 

\acrshortpl{gpu} are made up of \acrfullpl{sm}, and each \acrshort{sm} contains multiple cores that can perform calculations \citep{rauberParallelProgrammingMulticore2023}. A schematic representation of an \acrshort{sm} can be found in \Autoref{fig:gpu_sm_diagram}. Unlike the cores inside a \acrshort{cpu}, the cores in an \acrshort{sm} cannot function independently; each core has to execute the same instruction, and a new instruction can only be executed once all cores have finished executing the previous instruction. This execution model is called \acrfull{simd} \citep{flynnVeryHighspeedComputing1966}. The NVIDIA A100, a modern high-end data center \acrshort{gpu}, consists of 108 \acrshortpl{sm}, each containing 64 single-precision cores for a total of 6,912 cores and 32 double-precision cores for a total of 3,456 cores \citep{nvidiaNVIDIAA100Tensor2020}. Despite their massive amount of cores, \acrshortpl{gpu} do not outperform \acrshortpl{cpu} in every workload. To fully utilize the potential performance of a \acrshort{gpu}, an algorithm has to be suited for \acrshort{simd} execution. I.e., an algorithm has to consist of a (very) large amount of independent calculations that can be executed in parallel. This is the reason why \acrshortpl{gpu} can never fully replace \acrshortpl{cpu}. The majority of the tasks related to the operating system of a computer are sequential by nature, for which the \acrshort{cpu} is much better suited.

\begin{figure}
    \centering
    \includegraphics[scale=0.6]{../images/drawings/gpu_sm_diagram.png}
    \caption{Diagram of a \acrfull{sm}. The boxes indicated by ``INT32'', ``FP32'' and ``FP64'' are the computational cores for integers, single-precision floating-point and double-precision floating-point calculations, respectively. \citep{rauberParallelProgrammingMulticore2023}}
    \label{fig:gpu_sm_diagram}
\end{figure}

Another area where \acrshort{gpu} design is different from traditional \acrshortpl{cpu} is the memory configuration. \acrshortpl{gpu} generally do not share memory with \acrshortpl{cpu}. Rather, \acrshortpl{gpu} store data in memory that is located on the \acrshort{gpu} itself. This memory is optimized for high throughput, meaning that a large amount of data can flow from or to memory in a given time interval \citep{aamondtGeneralPurposeGraphicsProcessing2018}. \acrshort{cpu} memory, on the other hand, is located somewhere else in the computer and is optimized for providing low latency. This means that the time between the \acrshort{cpu} requesting data from memory and receiving the requested data is low. The high memory bandwidth makes the \acrshort{gpu} also a suitable device for memory-intensive applications.

The last important component in a \acrshort{gpu} system is the interconnection between \acrshort{cpu} and \acrshort{gpu}. This connection is often quite slow compared to the memory bandwidth of both \acrshortpl{cpu} and \acrshortpl{gpu}. Because the \acrshort{cpu} and \acrshort{gpu} do not share memory, data transfers between \acrshort{cpu} and \acrshort{gpu} are needed during runtime, which can significantly slow down an application.

\newpage
\section{Programming GPUs} \label{sec:programming_gpus}
To exploit the capabilities of GPUs, an application has to be written using a specialized programming model. There are numerous programming models available, each with different characteristics, which can be categorized as follows:

\textbf{Native kernel-based models} \\
Kernel-based methods involve the writing of special functions (also referred to as kernels) that describe the computations that have to be done by a single thread of the GPU.  At runtime, this kernel is then \emph{launched} across multiple threads, which all perform the kernel on their own piece of data. The word \emph{native} in this context means that the model is designed for use with a specific family of GPUs. Perhaps the most well-known example of a native kernel-based model is NVIDIA's Compute Unified Device Architecture (CUDA) \citep{dallyEvolutionGraphicsProcessing2021}. Native kernel-based programming models provide very fine-grained control over the GPU, thereby allowing for a great degree of optimization and consequently, the best performance. A drawback, however, is that these methods require (partial) rewriting of existing code. If a CPU version of the program is required in addition to a GPU version, two separate versions of the code have to be maintained, which is expensive and can introduce bugs.

\textbf{Portable kernel-based models} \\
As opposed to native kernel-based models, portable kernel-based models are designed to work on hardware from a variety of vendors. The program still consists of kernels, but these kernels can be launched with different backends of different accelerator vendors. The great benefit is that a program can run on different hardware without the need to rewrite the code, hence the name \emph{portable}. When the application is compiled, the target hardware is selected and the kernels are translated into code that can run natively on the selected hardware. Examples of portable kernel-based programming models include Kokkos \citep{trottKokkosProgrammingModel2022} and SYCL \citep{khronosgroupSYCL2020Specification2023}.

\textbf{Directive-based models} \\
A fundamentally different approach to GPU programming is the use of compiler directives. Compiler directives are instructions to the compiler to handle a section of code differently. These compiler directives are placed near computationally expensive sections of the code that can benefit from \acrshort{gpu} acceleration. When the code is compiled, the annotated sections of code are compiled into \acrshort{gpu}-compatible code. Because directive-based methods do not require the writing of kernels, an application can be ported to GPUs with relatively low effort. An additional benefit of compiler directives is that they are added to the original code, meaning that only one version of the source code has to be maintained. When the application has to be built for \acrshortpl{cpu}, the directives are simply ignored by the compiler. Two popular directive-based programming models are OpenACC \citep{farberParallelProgrammingOpenACC2017} and OpenMP \citep{antaoOffloadingSupportOpenMP2016}.

\textbf{Standard language parallelism} \\
A more recent trend has been the implementation of parallel programming features directly in programming languages. An example is the Fortran \texttt{do concurrent} construct. Its usage is similar to directive-based models: parallelizing loops. With the right compiler, \texttt{do concurrent} loops can be executed on GPUs \citep{kedwardStateFortran2022}. A great benefit of standard language parallelism is that it is part of the syntax of the language, thus offering good usability for developers already familiar with the language. 

\section{Usage of \acrshortpl{gpu} for \acrfull{cfd}}
\citet{niemeyerRecentProgressChallenges2014} have examined the status of \acrshort{gpu} computing in the field of \acrshort{cfd}. To demonstrate the potential benefits of using \acrshortpl{gpu} for \acrshort{cfd}, two case studies were performed: a 2D Laplace equation solver, resembling the Poisson equation that is often found in CFD codes, and a lid-driven cavity flow. Four implementations were tested: single-core \acrshort{cpu}, multi-core \acrshort{cpu} with OpenMP, \acrshort{gpu} with \acrshort{cuda}, and \acrshort{gpu} with \acrshort{acc}. For mesh sizes up to $512^2$, the wall-clock time for the \acrshort{gpu} implementations exceeded that of the multi-core \acrshort{gpu} implementation. While the authors do not explicitly explore the possible causes of this behavior, it can be argued that the increase in wall-clock time is due to data transfers between the \acrshort{cpu} and \acrshort{gpu}. For larger mesh sizes, the \acrshort{gpu} implementations outperformed the \acrshort{cpu} implementations. Specifically, for the Laplace equation, the \acrshort{gpu} solver showed a speedup of about 4.6, while for the lid-driven cavity flow, the speedup was about 2.8. Remarkably, \citet{niemeyerRecentProgressChallenges2014} showed that as the mesh size increases, the wall-clock time of the \acrshort{acc} implementation converges to that of the \acrshort{cuda} implementation, indicating that the benefits of the low-level optimizations that \acrshort{cuda} offers are not as important for large problem sizes.

\citet{costaFFTbasedFinitedifferenceSolver2018} has developed a tool for \acrshort{dns} of turbulent flows, called \acrshort{cans}. The dynamical core of \acrshort{cans} is very similar to that of \acrshort{dales}: both use finite-difference discretization on a structured, staggered grid, an \acrshort{fft}-based solver for the pressure, and third-order Runge Kutta time integration. Parallelization of \acrshort{cans} is achieved through domain decomposition with \acrshort{mpi}, with further fine-grained (i.e., at the level of loops) parallelization via OpenMP. \acrshort{cans} was later adapted for \acrshortpl{gpu} using \acrshort{cuda} Fortran \citep{costaGPUAccelerationCaNS2021}, but this was later switched out in favor of \acrshort{acc}. NVIDIA's cuFFT library was used to perform the \acrshort{fft} calculations on GPUs. Performance analysis was done on two systems: an NVIDIA DGX Station, a system comparable in size to a modern desktop PC and containing 4 Tesla V100 \acrshortpl{gpu}, and an NVIDIA DGX-2, a system that is more comparable to something that one would find in a supercomputer. The NVIDIA DGX-2 contains 16 of the same Tesla V100 \acrshortpl{gpu}. \citet{costaGPUAccelerationCaNS2021} found that for a constant problem size, one would need about 6100 to 11200 CPU cores to match the wall-clock time per time step of the 16 Tesla V100s in the NVIDIA DGX-2. This is still a conservative estimate, as linear scaling was assumed for the CPU code, whereas in reality, performance often scales sublinearly for a given problem size due to overhead introduced by communication between \acrshortpl{cpu}.

\acrshort{dales} itself has been ported to \acrshortpl{gpu} before by \citet{schalkwijkHighPerformanceSimulationsTurbulent2012}. To this end, the original Fortran code of \acrshort{dales} was translated to C++, and calculations were moved to the \acrshort{gpu} using \acrshort{cuda}, resulting in the \acrshort{gpu}-resident Atmospheric Large-Eddy Simulation (GALES) model. \citet{schalkwijkHighPerformanceSimulationsTurbulent2012} found that GALES was able to reduce the wall-clock time per time step by a factor of 2 compared to \acrshort{dales}. Since then, the company Whiffle has adopted GALES and further developed it into the GPU-Resident Atmospheric Simulation Platform (GRASP). GRASP is often used for very accurate simulations of windfarms \citep{verzijlberghAtmosphericFlowsLarge2021}.

Another \acrshort{les} model that shares characteristics with \acrshort{dales} is MicroHH, as described by \citet{vanheerwaardenMicroHHComputationalFluid2017}. In terms of the physical assumptions, \acrshort{dales} and MicroHH are almost identical, but MicroHH also features the option to perform \acrshort{dns} of atmospheric flows. MicroHH is written in C++ and simulations on the \acrshort{gpu} are made possible through \acrshort{cuda}. Again, significant performance benefits were found when running the model on the \acrshort{gpu}. A limitation of MicroHH, however, is that it cannot utilize more than one \acrshort{gpu} at a time.

\section{Trends in \acrshort{gpu} computing}
As can be seen in \Autoref{fig:top500_gpus}, the number of supercomputers with hardware accelerators (of which \acrshortpl{gpu} are an example) installed has grown steadily over the last decade. With the launch of Frontier in 2022, supercomputers have reached a significant milestone: the ability to perform $1\times10^{18}$ floating-point operations per second \citep{choiBeatingHartWorld2022}. To reach this level of performance, Frontier too makes heavy use of \acrshortpl{gpu}. This trend can be partially attributed to the rapid advances in the field of \acrfull{ai}, whose algorithms greatly benefit from the large number of cores offered by \acrshortpl{gpu} \citep{mittalSurveyTechniquesOptimizing2019}. As the development of \acrshort{ai} algorithms does not show any sign of slowing down \citep{xuArtificialIntelligencePowerful2021}, \acrshort{gpu} technology will continue to improve for the foreseeable future from which the \acrshort{cfd} community can profit too. 

\begin{figure}[h!]
    \centering
    \includesvg{../images/plots/gpu_in_top500.svg}
    \caption{Total number of systems in the TOP500 (list of the 500 most powerful supercomputers worldwide) that feature hardware accelerators over time. Data from \citet{top500TOP500Lists2023}}
    \label{fig:top500_gpus}
\end{figure}
