\chapter{Conclusions and Recommendations} \label{chap:conclusion_rec}

\section{Conclusions}
The objective of this thesis was to accelerate the \acrshort{dales} model by using \acrshortpl{gpu}. This has been achieved through the use of the \acrshort{acc} \acrshort{gpu} programming model. The scope of the research was limited to the \acrshort{bomex} model-intercomparison case which uses a limited subset of the physical schemes offered by \acrshort{dales}. Validation of the offloaded code was done using ensembles. For both the unmodified and \acrshort{gpu}-accelerated versions of \acrshort{dales}, fifty-member ensembles were constructed and compared. The ensemble mean and standard deviation of the \acrshort{gpu}-accelerated version of \acrshort{dales} did not show any significant deviations from the original version.

On a high-end desktop workstation, featuring an 8-core \acrshort{cpu} and an NVIDIA RTX 3090 \acrshort{gpu}, \acrshort{dales} was found to be up to 11.6 times faster when running on the \acrshort{gpu} compared to the 8 \acrshort{cpu} cores. On the Snellius supercomputer, the wall-clock time of \acrshort{dales} running on a single NVIDIA A100 \acrshort{gpu} was compared to that on a full \acrshort{cpu} node, because these two configurations cost the same number of \acrshortpl{sbu} per hour. The \acrshort{gpu} was up to 3.9 times faster than the \acrshort{cpu} node. 

One of the goals of this thesis was to enable \acrshort{dales} to make use of multiple \acrshortpl{gpu} for a single simulation, which was achieved through the use of \acrshort{gpu}-aware \acrshort{mpi}. The implementation was tested on the Snellius supercomputer. Strong scaling tests showed near-linear strong scaling up to 4 \acrshortpl{gpu}. From 8 \acrshortpl{gpu} onward, diminishing returns were observed. This performance drop is most likely caused by the relatively slow interconnection between nodes. Weak scaling tests show that multiple \acrshortpl{gpu} can be used relatively efficiently for large domain simulations. Up to 4 \acrshortpl{gpu}, near-linear weak scaling was observed. Similar to the strong scaling tests, a performance decrease was observed going from a single node to multiple nodes.

In general, the performance of \acrshort{dales} seemed to be limited by memory bandwidth. This is supported by the observation that the NVIDIA A100 \acrshort{gpu}, as present in Snellius \acrshort{gpu} nodes, only slightly outperformed the NVIDIA RTX 3090 \acrshort{gpu} of the desktop workstation, despite the A100 offering 17 times the theoretical computational performance. The use of single-precision floating-point numbers for the prognostic variables did not yield a significant speedup either, because most of the calculations in \acrshort{dales} are still executed in double precision.

\section{Recommendations}

\subsection{Accelerating more components}
Since only a limited portion of \acrshort{dales} was accelerated in this work, a clear direction for future work is to add \acrshort{acc} directives to other components of \acrshort{dales}. For example, the Cloud Botany case as described by \citet{janssonCloudBotanyShallow2023} requires, in addition to the components used by the \acrshort{bomex} case, a radiation scheme and a precipitation scheme. Offloading more components of the model to the \acrshort{gpu} will accelerate these more realistic cases, and can, through longer simulations, bigger domains and/or bigger ensembles, help with the development of new scientific insights.
  
\subsection{Performance tuning}

For improving performance when using a large number of \acrshortpl{gpu} ($\mathcal{O}(10^2)$ to $\mathcal{O}(10^3)$), the use of a specialized domain decomposition library could be explored. \citet{romeroDistributedmemorySimulationsTurbulent2022} developed such a library, called cuDecomp. cuDecomp automatically chooses the most efficient domain decomposition and inter-\acrshort{gpu} communication backend at runtime. This is done through the same transposition algorithm as encountered in the Poisson solver of \acrshort{dales}. While the slab decomposition outperformed the pencil decomposition in all cases of the weak scaling benchmarks of \acrshort{dales}, \citet{romeroDistributedmemorySimulationsTurbulent2022} found, for an application similar to \acrshort{dales}, that the slab decomposition is not necessarily the most efficient option when hundreds of \acrshortpl{gpu} are used. Moreover, \citet{romeroDistributedmemorySimulationsTurbulent2022} found that \acrshort{mpi} is not always the most efficient backend for inter-\acrshort{gpu} communication between high numbers of \acrshortpl{gpu}.

As \acrshort{dales} appears to be severely memory bandwidth bound, the model's performance could be improved by attempting to reduce memory usage. In practice, this can be done by storing fewer variables and computing more ``on the fly''. This strategy will, however, only yield a performance benefit in cases where the additional computations save on memory operations (loading and storing data).

\acrshortpl{gpu} are well suited for calculations in single precision, especially consumer-grade \acrshortpl{gpu} targeted for computer gaming like the RTX 3090 used in this work. Since the current \acrshort{gpu} port of \acrshort{dales} is not yet fully configured for using single-precision floating-point numbers, performance can still be gained in this regard. Full single-precision support can be achieved by converting more variables to variable precision, as is the case for the prognostic variables, such that no conversion to double-precision has to be done at runtime. During this process, care has to be taken to make sure that the loss of precision does not result in any numerical instabilities.

\subsection{Exploring portability}
The current \acrshort{gpu}-implementation of \acrshort{dales} has been tested exclusively on NVIDIA \acrshortpl{gpu}. However, newer supercomputers are starting to feature accelerators made by different vendors. For example, the Frontier and LUMI supercomputers both use \acrshortpl{gpu} made by AMD, while the \acrshortpl{gpu} of the upcoming Aurora supercomputer are supplied by Intel \citep{traderHowArgonnePreparing2021}. Therefore, it would be worthwhile to examine how \acrshort{dales} performs on \acrshortpl{gpu} other than NVIDIA. While \acrshort{acc} code can be compiled for AMD \acrshortpl{gpu}, it cannot for Intel \acrshortpl{gpu}. One approach to reaching portability is to translate the \acrshort{acc} directives to OpenMP. While this strategy appears to be very involved, tools exist that can perform this translation automatically \citep{servatIntelApplicationMigration2022}. With the use of such tools, the only task left to the developer is to validate the translated code.

Another component that requires extra care when exploring execution on \acrshortpl{gpu} from other vendors is the \acrshort{fft} library. In this work, the cuFFT library has been used, which only supports execution on NVIDIA \acrshortpl{gpu}. As discussed in \Autoref{sec:poisson_solver}, multiple other \acrshort{fft} libraries exist that support execution on AMD and/or Intel \acrshortpl{gpu}. Out of these libraries, rocFFT supports both NVIDIA and AMD \acrshortpl{gpu} and features bindings for the Fortran programming language. Furthermore, rocFFT has a similar interface as cuFFT and would therefore be relatively straightforward to implement in \acrshort{dales}. Alternatively, vkFFT could be used. vkFFT offers multiple benefits over rocFFT: it supports NVIDIA, AMD and Intel \acrshortpl{gpu} and features discrete cosine transforms for non-periodic boundary conditions. 

\acrshort{acc} can also be compiled for multi-core \acrshortpl{cpu}. While \acrshort{dales} can already make use of multiple \acrshort{cpu} cores via the existing \acrshort{mpi} parallelization, switching to \acrshort{acc} may improve efficiency for multi-core \acrshort{cpu} runs. The reasoning behind this line of thought is that \acrshort{acc} is a \emph{shared memory} programming model, meaning that all \acrshort{cpu} cores have access to the same memory space. In practice, this means that no domain decomposition is needed (unlike with \acrshort{mpi}) to make use of multiple \acrshort{cpu} cores, which also removes the need for communication between cores and thus can speed up simulations. For simulations with multiple \acrshortpl{cpu} and/or multiple nodes in a supercomputer, \acrshort{mpi} would still be needed as only the cores \emph{inside} a \acrshort{cpu} can share memory. Once again, the FFT library requires some care, as it would also need to operate in a shared memory configuration. The performance of \acrshort{dales} on multi-core \acrshortpl{cpu} using \acrshort{acc} is particularly interesting for supercomputers that rely on massive amounts of \acrshortpl{cpu} for their computational power, such as the Fugaku supercomputer \citep{satoCoDesignA64FXManycore2020}.
