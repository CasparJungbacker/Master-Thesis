@inproceedings{amdahlValiditySingleProcessor1967,
  title = {Validity of the Single Processor Approach to Achieving Large Scale Computing Capabilities},
  booktitle = {Proceedings of the {{April}} 18-20, 1967, Spring Joint Computer Conference on - {{AFIPS}} '67 ({{Spring}})},
  author = {Amdahl, Gene M.},
  year = {1967},
  pages = {483},
  publisher = {{ACM Press}},
  address = {{Atlantic City, New Jersey}},
  doi = {10.1145/1465482.1465560},
  url = {http://portal.acm.org/citation.cfm?doid=1465482.1465560},
  urldate = {2023-09-05},
  langid = {english}
}

@techreport{amdAMDEPYC7H12,
  title = {{{AMD EPYC}}{\texttrademark} {{7H12}} Specifications},
  author = {{AMD}},
  url = {https://www.amd.com/en/product/9131}
}

@misc{amdHipFFT2023,
  title = {{{hipFFT}}},
  author = {{AMD}},
  year = {2023},
  url = {https://hipfft.readthedocs.io/en/rocm-5.7.0/}
}

@misc{amdRocFFT2023,
  title = {{{rocFFT}}},
  author = {{AMD}},
  year = {2023},
  url = {https://rocfft.readthedocs.io/en/rocm-5.7.0/index.html}
}

@incollection{arakawaComputationalDesignBasic1977,
  title = {Computational {{Design}} of the {{Basic Dynamical Processes}} of the {{UCLA General Circulation Model}}},
  booktitle = {Methods in {{Computational Physics}}: {{Advances}} in {{Research}} and {{Applications}}},
  author = {Arakawa, Akio and Lamb, Vivian R.},
  year = {1977},
  volume = {17},
  pages = {173--265},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-0-12-460817-7.50009-4},
  url = {https://linkinghub.elsevier.com/retrieve/pii/B9780124608177500094},
  urldate = {2023-11-28},
  isbn = {978-0-12-460817-7},
  langid = {english}
}

@inproceedings{bardinaImprovedSubgridscaleModels1980,
  title = {Improved Subgrid-Scale Models for Large-Eddy Simulation},
  booktitle = {13th {{Fluid}} and {{PlasmaDynamics Conference}}},
  author = {Bardina, J. and Ferziger, J. and Reynolds, W.},
  year = {1980},
  month = jul,
  publisher = {{American Institute of Aeronautics and Astronautics}},
  address = {{Snowmass,CO,U.S.A.}},
  doi = {10.2514/6.1980-1357},
  url = {https://arc.aiaa.org/doi/10.2514/6.1980-1357},
  urldate = {2024-01-09},
  langid = {english}
}

@book{chandrasekaranOpenACCProgrammersConcepts2018,
  title = {{{OpenACC}} for Programmers: Concepts and Strategies},
  shorttitle = {{{OpenACC}} for Programmers},
  editor = {Chandrasekaran, Sunita and Juckeland, Guido},
  year = {2018},
  publisher = {{Addison-Wesley}},
  address = {{Boston}},
  isbn = {978-0-13-469434-4},
  langid = {english},
  annotation = {OCLC: 1003645622}
}

@article{choquetteNVIDIAA100Tensor2021,
  title = {{{NVIDIA A100 Tensor Core GPU}}: {{Performance}} and {{Innovation}}},
  shorttitle = {{{NVIDIA A100 Tensor Core GPU}}},
  author = {Choquette, J. and Gandhi, W. and Giroux, O. and Stam, N. and Krashinsky, R.},
  year = {2021},
  journal = {IEEE Micro},
  volume = {41},
  number = {2},
  pages = {29--35},
  issn = {0272-1732},
  doi = {10.1109/MM.2021.3061394},
  abstract = {NVIDIA A100 Tensor Core GPU is NVIDIA's latest flagship GPU. It has been designed with many new innovative features to provide performance and capabilities for HPC, AI, and data analytics workloads. Feature enhancements include a Third-Generation Tensor Core, new asynchronous data movement and programming model, enhanced L2 cache, HBM2 DRAM, and third-generation NVIDIA NVLink I/O. {\textcopyright} 1981-2012 IEEE.},
  langid = {english},
  keywords = {A100,C++20,CUDA,Deep Learning,GPU,NVLink,Tensor Core},
  file = {/Users/caspar/Zotero/storage/R42XCV4P/display.html}
}

@article{chorinNumericalSolutionNavierStokes1967,
  title = {The Numerical Solution of the {{Navier-Stokes}} Equations for an Incompressible Fluid},
  author = {Chorin, Alexandre Joel},
  year = {1967},
  journal = {Bulletin of the American Mathematical Society},
  volume = {73},
  number = {6},
  pages = {928--931},
  issn = {0273-0979, 1088-9485},
  doi = {10.1090/S0002-9904-1967-11853-6},
  url = {https://www.ams.org/bull/1967-73-06/S0002-9904-1967-11853-6/},
  urldate = {2023-12-06},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/UB4U6L6U/Chorin - 1967 - The numerical solution of the Navier-Stokes equati.pdf}
}

@inproceedings{corbalanEnergyOptimizationAnalysis2020,
  title = {Energy {{Optimization}} and {{Analysis}} with {{EAR}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Cluster Computing}} ({{CLUSTER}})},
  author = {Corbalan, Julita and Alonso, Lluis and Aneas, Jordi and Brochard, Luigi},
  year = {2020},
  month = sep,
  pages = {464--472},
  publisher = {{IEEE}},
  address = {{Kobe, Japan}},
  doi = {10.1109/CLUSTER49012.2020.00067},
  url = {https://ieeexplore.ieee.org/document/9229570/},
  urldate = {2023-09-05},
  isbn = {978-1-72816-677-3},
  file = {/Users/caspar/Zotero/storage/9R2PK5MA/Corbalan et al. - 2020 - Energy Optimization and Analysis with EAR.pdf;/Users/caspar/Zotero/storage/G5UWX5XX/Corbalan et al. - 2020 - Energy Optimization and Analysis with EAR.pdf}
}

@article{costaFFTbasedFinitedifferenceSolver2018,
  title = {A {{FFT-based}} Finite-Difference Solver for Massively-Parallel Direct Numerical Simulations of Turbulent Flows},
  author = {Costa, Pedro},
  year = {2018},
  month = oct,
  journal = {Computers \& Mathematics with Applications},
  volume = {76},
  number = {8},
  pages = {1853--1862},
  issn = {0898-1221},
  doi = {10.1016/j.camwa.2018.07.034},
  url = {https://www.sciencedirect.com/science/article/pii/S089812211830405X},
  urldate = {2023-05-25},
  abstract = {We present an efficient solver for massively-parallel direct numerical simulations of incompressible turbulent flows. The method uses a second-order, finite-volume pressure-correction scheme, where the pressure Poisson equation is solved with the method of eigenfunction expansions. This approach allows for very efficient FFT-based solvers in problems with different combinations of homogeneous pressure boundary conditions. Our algorithm explores all combinations of pressure boundary conditions valid for such a solver, in a single, general framework. The method is implemented in a 2D pencil-like domain decomposition, which enables efficient massively-parallel simulations. The implementation was validated against different canonical flows, and its computational performance was examined. Excellent strong scaling performance up to 104 cores is demonstrated for a domain with 109 spatial degrees of freedom, corresponding to a very small wall-clock time/time step. The resulting tool, CaNS, has been made freely available and open-source.},
  langid = {english},
  keywords = {Direct numerical simulations,Fast Poisson solver,High-performance computing,Turbulent flows},
  file = {/Users/caspar/Zotero/storage/WCD9NJJK/Costa - 2018 - A FFT-based finite-difference solver for massively.pdf;/Users/caspar/Zotero/storage/K8TVR7Q5/S089812211830405X.html}
}

@article{costaGPUAccelerationCaNS2021,
  title = {{{GPU}} Acceleration of {{CaNS}} for Massively-Parallel Direct Numerical Simulations of Canonical Fluid Flows},
  author = {Costa, Pedro and Phillips, Everett and Brandt, Luca and Fatica, Massimiliano},
  year = {2021},
  month = jan,
  journal = {Computers \& Mathematics with Applications},
  volume = {81},
  pages = {502--511},
  issn = {08981221},
  doi = {10.1016/j.camwa.2020.01.002},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0898122120300092},
  urldate = {2023-05-26},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/BP4P5UHX/Costa et al. - 2021 - GPU acceleration of CaNS for massively-parallel di.pdf}
}

@article{deardorffStratocumuluscappedMixedLayers1980,
  title = {Stratocumulus-Capped Mixed Layers Derived from a Three-Dimensional Model},
  author = {Deardorff, James W.},
  year = {1980},
  month = jun,
  journal = {Boundary-Layer Meteorology},
  volume = {18},
  number = {4},
  pages = {495--527},
  issn = {0006-8314, 1573-1472},
  doi = {10.1007/BF00119502},
  url = {http://link.springer.com/10.1007/BF00119502},
  urldate = {2024-01-05},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/Z4PRF4VU/Deardorff - 1980 - Stratocumulus-capped mixed layers derived from a t.pdf}
}

@article{elsterNvidiaHopperGPU2022,
  title = {Nvidia {{Hopper GPU}} and {{Grace CPU Highlights}}},
  author = {Elster, Anne C. and Haugdahl, Tor A.},
  year = {2022},
  month = mar,
  journal = {Computing in Science \& Engineering},
  volume = {24},
  number = {2},
  pages = {95--100},
  issn = {1558-366X},
  doi = {10.1109/MCSE.2022.3163817},
  abstract = {At GTC 2022, Nvidia announced a new product family that aims to cover from small enterprise workloads through exascale high performance computing (HPC) and trillion-parameter AI models. This column highlights the most interesting features of their new Hopper graphical processing unit (GPU) and Grace central processing unit (CPU) computer chips and the Hopper product family. We also discuss some of the history behind Nvidia technologies and their most useful features for computational scientists, such as the Hopper DPX dynamic programming (DP) instruction set, increased number of SMs, and FP 8 tensor core availability. Also included are descriptions of the new Hopper Clustered SMs architecture and updated NVSwitch technologies that integrate their new ARM-based Grace CPU.},
  keywords = {Central processing units,Companies,Graphics processing units,Product design,Product development},
  file = {/Users/caspar/Zotero/storage/SXV5H73R/Elster and Haugdahl - 2022 - Nvidia Hopper GPU and Grace CPU Highlights.pdf;/Users/caspar/Zotero/storage/KL3DKPXC/stamp.html}
}

@book{farberParallelProgrammingOpenACC2017,
  title = {Parallel {{Programming}} with {{OpenACC}}},
  author = {Farber, Rob},
  year = {2017},
  publisher = {{Elsevier}},
  isbn = {978-0-12-410397-9},
  langid = {english}
}

@techreport{FFTW97,
  title = {The Fastest {{Fourier}} Transform in the West},
  author = {Frigo, Matteo and Johnson, Steven G.},
  year = {1997},
  month = sep,
  number = {MIT-LCS-TR-728},
  institution = {{Massachusetts Institute of Technology}}
}

@article{flynnVeryHighspeedComputing1966,
  title = {Very High-Speed Computing Systems},
  author = {Flynn, M.J.},
  year = {1966},
  journal = {Proceedings of the IEEE},
  volume = {54},
  number = {12},
  pages = {1901--1909},
  issn = {0018-9219},
  doi = {10.1109/PROC.1966.5273},
  url = {http://ieeexplore.ieee.org/document/1447203/},
  urldate = {2023-06-04},
  abstract = {Very high-speed computers maybe clnssified as follows: 1) Single Jktmction S t r d i n g l e Data Stream (SISD) 2) S i l e Imbnctioa Stream-Multiple Data Stream(SIMD) 3) Multiplehstmcth StrePntSingle Data Stream(MSD) 4) Mnltiple Instroctioo Stream-Multiple Data Stream (''D).},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/RMSPUSJH/Flynn - 1966 - Very high-speed computing systems.pdf}
}

@inproceedings{frigoFFTWAdaptiveSoftware1998a,
  title = {{{FFTW}}: An Adaptive Software Architecture for the {{FFT}}},
  shorttitle = {{{FFTW}}},
  booktitle = {Proceedings of the 1998 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}, {{ICASSP}} '98 ({{Cat}}. {{No}}.{{98CH36181}})},
  author = {Frigo, M. and Johnson, S.G.},
  year = {1998},
  volume = {3},
  pages = {1381--1384},
  publisher = {{IEEE}},
  address = {{Seattle, WA, USA}},
  doi = {10.1109/ICASSP.1998.681704},
  url = {http://ieeexplore.ieee.org/document/681704/},
  urldate = {2023-10-03},
  isbn = {978-0-7803-4428-0}
}

@article{gustafsonReevaluatingAmdahlLaw1988,
  title = {Reevaluating {{Amdahl}}'s Law},
  author = {Gustafson, John L.},
  year = {1988},
  month = may,
  journal = {Communications of the ACM},
  volume = {31},
  number = {5},
  pages = {532--533},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/42411.42415},
  url = {https://dl.acm.org/doi/10.1145/42411.42415},
  urldate = {2023-09-14},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/3N2W5BTG/Gustafson - 1988 - Reevaluating Amdahl's law.pdf}
}

@inproceedings{hanReducingBranchDivergence2011,
  title = {Reducing Branch Divergence in {{GPU}} Programs},
  booktitle = {Proceedings of the {{Fourth Workshop}} on {{General Purpose Processing}} on {{Graphics Processing Units}}},
  author = {Han, Tianyi David and Abdelrahman, Tarek S.},
  year = {2011},
  month = mar,
  pages = {1--8},
  publisher = {{ACM}},
  address = {{Newport Beach California USA}},
  doi = {10.1145/1964179.1964184},
  url = {https://dl.acm.org/doi/10.1145/1964179.1964184},
  urldate = {2023-11-30},
  isbn = {978-1-4503-0569-3},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/HJVTCRUS/Han and Abdelrahman - 2011 - Reducing branch divergence in GPU programs.pdf}
}

@book{hennessyComputerArchitectureQuantitative2012,
  title = {Computer Architecture: A Quantitative Approach},
  shorttitle = {Computer Architecture},
  author = {Hennessy, John L. and Patterson, David A. and Asanovi{\'c}, Krste},
  year = {2012},
  edition = {5th ed},
  publisher = {{Morgan Kaufmann/Elsevier}},
  address = {{Waltham, MA}},
  isbn = {978-0-12-383872-8},
  lccn = {QA76.9.A73 P377 2012},
  keywords = {Computer architecture},
  annotation = {OCLC: ocn755102367}
}

@article{herdmanAcceleratingHydrocodesOpenACC2012,
  title = {Accelerating Hydrocodes with {{OpenACC}}, {{OpenCL}} and {{CUDA}}},
  author = {Herdman, J. A. and Gaudin, W. P. and {McIntosh-Smith}, S. and Boulton, M. and Beckingsale, D. A. and Mallinson, A. C. and Jarvis, S. A.},
  year = {2012},
  journal = {Proceedings - 2012 SC Companion: High Performance Computing, Networking Storage and Analysis, SCC 2012},
  pages = {465--471},
  doi = {10.1109/SC.COMPANION.2012.66},
  abstract = {Hardware accelerators such as GPGPUs are becoming increasingly common in HPC platforms and their use is widely recognised as being one of the most promising approaches for reaching exascale levels of performance.Large HPC centres, such as AWE, have made huge investments in maintaining their existing scientific software codebases, the vast majority of which were not designed to effectively utilise accelerator devices.Consequently, HPC centres will have to decide how to develop their existing applications to take best advantage of future HPC system architectures.Given limited development and financial resources, it is unlikely that all potential approaches will be evaluated for each application. We are interested in how this decision making can be improved, and this work seeks to directly evaluate three candidate technologies - -OpenACC, OpenCL and CUDA - -in terms of performance, programmer productivity, and portability using a recently developed Lagrangian-Eulerian explicit hydrodynamics mini-Application. We find that OpenACC is an extremely viable programming model for accelerator devices, improving programmer productivity and achieving better performance than OpenCL and CUDA. {\textcopyright} 2012 IEEE.},
  isbn = {9780769549569},
  keywords = {CUDA,High Performance Computing,Hydrodynamics,OpenACC,OpenCL},
  file = {/Users/caspar/Zotero/storage/KFESNJ6F/Herdman et al. - 2012 - Accelerating hydrocodes with OpenACC, OpenCL and C.pdf}
}

@article{heusFormulationDutchAtmospheric2010,
  title = {Formulation of the {{Dutch Atmospheric Large-Eddy Simulation}} ({{DALES}}) and Overview of Its Applications},
  author = {Heus, T. and Van Heerwaarden, C. C. and Jonker, H. J. J. and Pier Siebesma, A. and Axelsen, S. and Van Den Dries, K. and Geoffroy, O. and Moene, A. F. and Pino, D. and De Roode, S. R. and {Vil{\`a}-Guerau De Arellano}, J.},
  year = {2010},
  month = sep,
  journal = {Geoscientific Model Development},
  volume = {3},
  number = {2},
  pages = {415--444},
  issn = {1991-9603},
  doi = {10.5194/gmd-3-415-2010},
  url = {https://gmd.copernicus.org/articles/3/415/2010/},
  urldate = {2023-05-23},
  abstract = {Abstract. The current version of the Dutch Atmospheric Large-Eddy Simulation (DALES) is presented. DALES is a large-eddy simulation code designed for studies of the physics of the atmospheric boundary layer, including convective and stable boundary layers as well as cloudy boundary layers. In addition, DALES can be used for studies of more specific cases, such as flow over sloping or heterogeneous terrain, and dispersion of inert and chemically active species. This paper contains an extensive description of the physical and numerical formulation of the code, and gives an overview of its applications and accomplishments in recent years.},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/B4CER9JR/Heus et al. - 2010 - Formulation of the Dutch Atmospheric Large-Eddy Si.pdf}
}

@article{hockneyFastDirectSolution1965,
  title = {A {{Fast Direct Solution}} of {{Poisson}}'s {{Equation Using Fourier Analysis}}},
  author = {Hockney, R. W.},
  year = {1965},
  month = jan,
  journal = {Journal of the ACM},
  volume = {12},
  number = {1},
  pages = {95--113},
  issn = {0004-5411, 1557-735X},
  doi = {10.1145/321250.321259},
  url = {https://dl.acm.org/doi/10.1145/321250.321259},
  urldate = {2023-12-19},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/JUZPLBBU/Hockney - 1965 - A Fast Direct Solution of Poisson's Equation Using.pdf}
}

@article{hollandMeasurementsAtmosphericMass1973,
  title = {Measurements of the {{Atmospheric Mass}}, {{Energy}}, and {{Momentum Budgets Over}} a 500-{{Kilometer Square}} of {{Tropical Ocean}}},
  author = {Holland, Joshua Z. and Rasmusson, Eugene M.},
  year = {1973},
  month = jan,
  journal = {Monthly Weather Review},
  volume = {101},
  pages = {44},
  issn = {0027-0644},
  doi = {10.1175/1520-0493(1973)101<0044:MOTAME>2.3.CO;2},
  url = {https://ui.adsabs.harvard.edu/abs/1973MWRv..101...44H},
  urldate = {2023-09-19},
  annotation = {ADS Bibcode: 1973MWRv..101...44H},
  file = {/Users/caspar/Zotero/storage/NQE9M6MX/Holland and Rasmusson - 1973 - Measurements of the Atmospheric Mass, Energy, and .pdf}
}

@article{janssonCloudBotanyShallow2023,
  title = {Cloud {{Botany}}: {{Shallow Cumulus Clouds}} in an {{Ensemble}} of {{Idealized Large}}-{{Domain Large}}-{{Eddy Simulations}} of the {{Trades}}},
  shorttitle = {Cloud {{Botany}}},
  author = {Jansson, Fredrik and Janssens, Martin and Gr{\"o}nqvist, Johanna H. and Siebesma, A. Pier and Glassmeier, Franziska and Attema, Jisk and Azizi, Victor and Satoh, Masaki and Sato, Yousuke and Schulz, Hauke and K{\"o}lling, Tobias},
  year = {2023},
  month = nov,
  journal = {Journal of Advances in Modeling Earth Systems},
  volume = {15},
  number = {11},
  pages = {e2023MS003796},
  issn = {1942-2466, 1942-2466},
  doi = {10.1029/2023MS003796},
  url = {https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2023MS003796},
  urldate = {2023-11-23},
  abstract = {Abstract                            Small shallow cumulus clouds ({$<$}1~km) over the tropical oceans appear to possess the ability to self-organize into mesoscale (10{\textendash}100~km) patterns. To better understand the processes leading to such self-organized convection, we present Cloud Botany, an ensemble of 103 large-eddy simulations on domains of 150~km, produced by the Dutch Atmospheric Large Eddy Simulation model on supercomputer Fugaku. Each simulation is run in an idealized, fixed, larger-scale environment, controlled by six free parameters. We vary these over characteristic ranges for the winter trades, including parameter combinations observed during the EUREC               4               A (Elucidating the role of clouds{\textendash}circulation coupling in climate) field campaign. In contrast to simulation setups striving for maximum realism, Cloud Botany provides a platform for studying idealized, and therefore more clearly interpretable causal relationships between conditions in the larger-scale environment and patterns in mesoscale, self-organized shallow convection. We find that any simulation that supports cumulus clouds eventually develops mesoscale patterns in their cloud fields. We also find a rich variety in these patterns as our control parameters change, including cold pools lined by cloudy arcs, bands of cross-wind clouds and aggregated patches, sometimes topped by thin anvils. Many of these features are similar to cloud patterns found in nature. The published data set consists of raw simulation output on full 3D grids and 2D cross-sections, as well as post-processed quantities aggregated over the vertical (2D), horizontal (1D) and all spatial dimensions (time-series). The data set is directly accessible from Python through the use of the EUREC               4               A intake catalog.                        ,              Plain Language Summary             The organization of shallow cumulus clouds over the tropical ocean has recently received a lot of attention. This type of organization is potentially important for how the clouds are affected by a changing climate and also for how they modulate further warming. We present a collection of 103 detailed simulations of shallow cumulus clouds in idealized atmospheric environments. These environments are described by six parameters, and our collection is formed by systematically simulating different parameter combinations. This way an ensemble is created that spans up a multidimensional phase space of environmental conditions typical for the wintertime subtropical Atlantic Ocean. This approach allows us to form a picture of how the environmental conditions relate to the cloud organization that develops in the simulations. At a glance, most simulations evolve similarly: They quickly form small cumulus clouds, which then grow in size and organize into patterns. Often this leads to rainfall, which then causes further heterogeneity and pattern formation. The data is openly available online, and will serve future studies of cumulus clouds, their organization, and how they interact with the climate.           ,              Key Points                                                                We present Cloud Botany, an ensemble of idealized large-eddy simulations of the winter trade wind regions, controlled by six varied parameters                                                     The parameter ranges are chosen to match the climatology of the trade wind region                                                     The simulations show a variety of cloud organization patterns: small cumulus, stripes, cold pools, cloud arcs, and anvils},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/WDVJLKI5/Jansson et al. - 2023 - Cloud Botany Shallow Cumulus Clouds in an Ensembl.pdf}
}

@article{kedwardStateFortran2022,
  title = {The {{State}} of {{Fortran}}},
  author = {Kedward, Laurence J. and Aradi, Balint and Certik, Ondrej and Curcic, Milan and Ehlert, Sebastian and Engel, Philipp and Goswami, Rohit and Hirsch, Michael and {Lozada-Blanco}, Asdrubal and Magnin, Vincent and Markus, Arjen and Pagone, Emanuele and Pribec, Ivan and Richardson, Brad and Snyder, Harris and Urban, John and Vandenplas, Jeremie},
  year = {2022},
  month = mar,
  journal = {Computing in Science \& Engineering},
  volume = {24},
  number = {2},
  pages = {63--72},
  issn = {1521-9615, 1558-366X},
  doi = {10.1109/MCSE.2022.3159862},
  url = {https://ieeexplore.ieee.org/document/9736688/},
  urldate = {2023-10-06},
  file = {/Users/caspar/Zotero/storage/B9R5HI9W/Kedward et al. - 2022 - The State of Fortran.pdf}
}

@techreport{liquilungOpenBoundaryConditions2023,
  type = {Preprint},
  title = {Open {{Boundary Conditions}} for {{Atmospheric Large Eddy Simulations}} and the {{Implementation}} in {{DALES4}}.4},
  author = {Liqui Lung, Franciscus P. A. and Jakob, Christian and Siebesma, A. Pier and Jansson, Fredrik R.},
  year = {2023},
  month = nov,
  institution = {{Atmospheric sciences}},
  doi = {10.5194/gmd-2023-196},
  url = {https://gmd.copernicus.org/preprints/gmd-2023-196/},
  urldate = {2023-11-15},
  abstract = {Abstract. Open boundary conditions were developed for atmospheric large eddy simulation (LES) models and implemented into the Dutch Atmospheric Large Eddy simulation model. The implementation was tested in a "Big Brother"-like setup, in which the simulation with open boundary conditions was forced by an identical control simulation with periodic boundary conditions. The results show that the open boundary implementation has minimal influence on the solution. Both the mean state and the turbulent structures are close to the control simulation and disturbances at the in- and outflow boundaries are negligible. To emulate a setup in which the LES is coupled to a coarser model, the influence of coarse boundary input was tested by smoothing the output of the periodic control simulation both temporally and spatially before feeding it as input to the simulation with open boundary conditions. The results show that when the ratio between input and model resolution increases, disturbances start to form at the inflow boundary and an area exists where turbulence needs to develop. Adding synthetic turbulence to the smoothed input reduces the size of this area and the magnitude of the disturbances.},
  file = {/Users/caspar/Zotero/storage/DIX8W5ZS/Liqui Lung et al. - 2023 - Open Boundary Conditions for Atmospheric Large Edd.pdf}
}

@article{masonLargeEddySimulationConvective1989,
  title = {Large-{{Eddy Simulation}} of the {{Convective Atmospheric Boundary Layer}}},
  author = {Mason, P. J.},
  year = {1989},
  month = jun,
  journal = {Journal of the Atmospheric Sciences},
  volume = {46},
  number = {11},
  pages = {1492--1516},
  publisher = {{American Meteorological Society}},
  issn = {0022-4928, 1520-0469},
  doi = {10.1175/1520-0469(1989)046<1492:LESOTC>2.0.CO;2},
  url = {https://journals.ametsoc.org/view/journals/atsc/46/11/1520-0469_1989_046_1492_lesotc_2_0_co_2.xml},
  urldate = {2023-05-26},
  abstract = {Abstract Large-eddy simulations of a free convective atmospheric boundary layer with an overlying capping inversion are considered. Attention is given to the dependence of the results upon the various factors influencing the simulation: the subgrid model, the domain size, and the mesh resolution. By providing artificial constraints upon the convection the results also provide extra insight into the underlying dynamics. The gross features of the boundary layer, such as the overall energy budget, are not sensitive to the details of the simulations but a number of important factors are revealed. It has been found that near the surface the subgrid diffusivity must be larger than is usually supposed, in order for the vertical velocity skewness to have the correct sign. This region of the flow has a significant subgrid-scale heat flux and it seems that the subgrid model requires improvement in such cases. A revised model which under statically unstable conditions allows the mixing-length of the subgrid-scale turbulence to depend on the flow stability is found to give improved results. The domain size and mesh spacings have a significant influence upon the results and need a setting which allows resolution of the main, freely occurring scales of motion. The entrainment at the capping inversion is remarkable in its insensitivity to all factors. Finally, the higher resolution simulations provide a detailed view of the flow structure of the convective boundary layer. Downdrafts cover a large fraction of the surface area, and near the surface the flow converges into smaller areas comprising long narrow regions of updrafts. The plumes which penetrate through the depth of the boundary layer to the inversion mainly occur over the inter-sections of these long narrow regions of updrafts.},
  chapter = {Journal of the Atmospheric Sciences},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/E65ZN4TT/Mason - 1989 - Large-Eddy Simulation of the Convective Atmospheri.pdf}
}

@incollection{moengNUMERICALMODELSLargeEddy2015,
  title = {{{NUMERICAL MODELS}} | {{Large-Eddy Simulation}}},
  booktitle = {Encyclopedia of {{Atmospheric Sciences}}},
  author = {Moeng, C.-H. and Sullivan, P.P.},
  year = {2015},
  pages = {232--240},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-0-12-382225-3.00201-2},
  url = {https://linkinghub.elsevier.com/retrieve/pii/B9780123822253002012},
  urldate = {2023-05-26},
  isbn = {978-0-12-382225-3},
  langid = {english}
}

@article{mullerSelfAggregationConvection2020,
  title = {Self-{{Aggregation}} of {{Convection}} in {{Spatially Varying Sea Surface Temperatures}}},
  author = {M{\"u}ller, Sebastian K. and Hohenegger, Cathy},
  year = {2020},
  month = jan,
  journal = {Journal of Advances in Modeling Earth Systems},
  volume = {12},
  number = {1},
  pages = {e2019MS001698},
  issn = {1942-2466, 1942-2466},
  doi = {10.1029/2019MS001698},
  url = {https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2019MS001698},
  urldate = {2023-10-18},
  abstract = {Abstract             The phenomenon of self-aggregation of convection was first identified in convection-permitting simulations of radiative convective equilibrium, characterized by homogeneous boundary conditions and in the absence of planetary rotation. In this study, we expose self-aggregation of convection to more complex, nonhomogeneous boundary conditions and investigate its interaction with convective aggregation, as forced by large-scale variations in sea surface temperatures (SSTs). We do this by conducting radiative convective equilibrium simulations on a spherical domain, with SST patterns that are zonally homogeneous but meridionally varying. Due to the meridional contrast in SST, a convergence line first forms, mimicking the Intertropical Convergence Zone. We nevertheless find that the convergence line breaks up and contracts zonally as a result of the self-aggregation of convection. The contraction is significant, being here more than 50\% of the original extent. The stability of the convergence line is controlled by the strength of the meridional circulation, which depends upon the imposed SST contrast. However, the process of self-aggregation, once it is initiated, is insensitive to the strength of the SST contrast. The zonal contraction is accompanied by a slight meridional expansion and a moistening of the high latitudes, where SSTs are low. The moistening of the high latitudes can be understood from the fact that the convective cluster intensifies and expands its moist meridional low-level outflow when it self-aggregates zonally. Overall, our results suggest that the Intertropical Convergence Zone may be unstable to the self-aggregation of convection, that self-aggregation may serve as a precursor to the formation of atmospheric rivers, and that longer convergence lines are more likely to exist in regimes with strong SST gradients.           ,              Plain Language Summary             Storm clouds over the tropical oceans are found to organize in large systems. They tend to form where the temperatures of the sea surface (SSTs) are highest and that is most often along the equator. With idealized computer model experiments we investigate how different sea surface temperature patterns, along with the natural tendency of clouds to aggregate, control the properties of the storm cloud system. We find that a contrast in SSTs immediately acts to organize the storm clouds. In our simulations, this means the formation of a long line of clouds, oriented along the equator. With time however, this line breaks up and then contracts just by itself. Here we find that the spatial difference in SSTs controls the stability of the cloud system against the break up and contraction: The stronger the difference in SST, the more stable is the line of clouds. Also we find that the storm clouds export moisture only in one direction: from the equator to the higher latitudes.           ,              Key Points                                                                A convergence line, forced by an SST contrast and comprising deep convection, is unstable to the self-aggregation of convection                                                     The zonally elongated convective clusters export moisture only meridionally and only between 1 and 5~km height},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/9W28M57R/Müller and Hohenegger - 2020 - Self‐Aggregation of Convection in Spatially Varyin.pdf;/Users/caspar/Zotero/storage/B3ZNYBUK/Müller and Hohenegger - 2020 - Self‐Aggregation of Convection in Spatially Varyin.pdf}
}

@article{niemeyerRecentProgressChallenges2014,
  title = {Recent Progress and Challenges in Exploiting Graphics Processors in Computational Fluid Dynamics},
  author = {Niemeyer, Kyle E. and Sung, Chih-Jen},
  year = {2014},
  month = feb,
  journal = {The Journal of Supercomputing},
  volume = {67},
  number = {2},
  pages = {528--564},
  issn = {1573-0484},
  doi = {10.1007/s11227-013-1015-7},
  url = {https://doi.org/10.1007/s11227-013-1015-7},
  urldate = {2023-05-29},
  abstract = {The progress made in accelerating simulations of fluid flow using GPUs, and the challenges that remain, are surveyed. The review first provides an introduction to GPU computing and programming, and discusses various considerations for improved performance. Case studies comparing the performance of CPU- and GPU-based solvers for the Laplace and incompressible Navier{\textendash}Stokes equations are performed in order to demonstrate the potential improvement even with simple codes. Recent efforts to accelerate CFD simulations using GPUs are reviewed for laminar, turbulent, and reactive flow solvers. Also, GPU implementations of the lattice Boltzmann method are reviewed. Finally, recommendations for implementing CFD codes on GPUs are given and remaining challenges are discussed, such as the need to develop new strategies and redesign algorithms to enable GPU acceleration.},
  langid = {english},
  keywords = {Computational fluid dynamics (CFD),CUDA,Graphics processing unit (GPU),Laminar flows,Reactive flow,Turbulent flow},
  file = {/Users/caspar/Zotero/storage/324IWT3H/Niemeyer and Sung - 2014 - Recent progress and challenges in exploiting graph.pdf}
}

@misc{nvidiaCUDAProgrammingGuide2023,
  title = {{{CUDA C}}++ {{Programming Guide}}},
  author = {{NVIDIA}},
  year = {2023},
  month = jul,
  url = {https://docs.nvidia.com/cuda/pdf/CUDA_C_Programming_Guide.pdf}
}

@misc{nvidiaCuFFT,
  title = {{{cuFFT}}},
  author = {{Nvidia}},
  url = {https://docs.nvidia.com/cuda/cufft/index.html}
}

@misc{nvidiadeveloperCuFFTMp,
  title = {{{cuFFTMp}}},
  author = {{NVIDIA Developer}},
  url = {https://docs.nvidia.com/hpc-sdk/cufftmp/index.html}
}

@misc{nvidiadeveloperNVSHMEM,
  title = {{{NVSHMEM}}},
  author = {{NVIDIA Developer}},
  url = {https://docs.nvidia.com/nvshmem/api/index.html}
}

@misc{nvidiaNVIDIANsightSystems,
  title = {{{NVIDIA Nsight Systems}}},
  author = {{NVIDIA}},
  url = {https://developer.nvidia.com/nsight-systems},
  urldate = {2023-05-25}
}

@article{ouwerslootLargeEddySimulationComparison2017,
  title = {Large-{{Eddy Simulation Comparison}} of {{Neutral Flow Over}} a {{Canopy}}: {{Sensitivities}} to {{Physical}} and {{Numerical Conditions}}, and {{Similarity}} to {{Other Representations}}},
  shorttitle = {Large-{{Eddy Simulation Comparison}} of {{Neutral Flow Over}} a {{Canopy}}},
  author = {Ouwersloot, H. G. and Moene, A. F. and Attema, J. J. and De Arellano, J. Vil{\`a}-Guerau},
  year = {2017},
  month = jan,
  journal = {Boundary-Layer Meteorology},
  volume = {162},
  number = {1},
  pages = {71--89},
  issn = {0006-8314, 1573-1472},
  doi = {10.1007/s10546-016-0182-5},
  url = {http://link.springer.com/10.1007/s10546-016-0182-5},
  urldate = {2023-11-24},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/KQWI42AV/Ouwersloot et al. - 2017 - Large-Eddy Simulation Comparison of Neutral Flow O.pdf}
}

@article{owensGPUComputing2008,
  title = {{{GPU Computing}}},
  author = {Owens, J.D. and Houston, M. and Luebke, D. and Green, S. and Stone, J.E. and Phillips, J.C.},
  year = {2008},
  month = may,
  journal = {Proceedings of the IEEE},
  volume = {96},
  number = {5},
  pages = {879--899},
  issn = {0018-9219, 1558-2256},
  doi = {10.1109/JPROC.2008.917757},
  url = {http://ieeexplore.ieee.org/document/4490127/},
  urldate = {2023-06-04},
  abstract = {The graphics processing unit (GPU) has become an integral part of today's mainstream computing systems. Over the past six years, there has been a marked increase in the performance and capabilities of GPUs. The modern GPU is not only a powerful graphics engine but also a highly parallel programmable processor featuring peak arithmetic and memory bandwidth that substantially outpaces its CPU counterpart. The GPU's rapid increase in both programmability and capability has spawned a research community that has successfully mapped a broad range of computationally demanding, complex problems to the GPU. This effort in generalpurpose computing on the GPU, also known as GPU computing, has positioned the GPU as a compelling alternative to traditional microprocessors in high-performance computer systems of the future. We describe the background, hardware, and programming model for GPU computing, summarize the state of the art in tools and techniques, and present four GPU computing successes in game physics and computational biophysics that deliver order-of-magnitude performance gains over optimized CPU applications.},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/TQKRD2TY/Owens et al. - 2008 - GPU Computing.pdf}
}

@book{popeTurbulentFlows2000,
  title = {Turbulent Flows},
  author = {Pope, S. B.},
  year = {2000},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge ; New York}},
  isbn = {978-0-521-59125-6 978-0-521-59886-6},
  lccn = {QA913 .P64 2000},
  keywords = {Turbulence}
}

@inproceedings{potluriEfficientInternodeMPI2013,
  title = {Efficient {{Inter-node MPI Communication Using GPUDirect RDMA}} for {{InfiniBand Clusters}} with {{NVIDIA GPUs}}},
  booktitle = {2013 42nd {{International Conference}} on {{Parallel Processing}}},
  author = {Potluri, Sreeram and Hamidouche, Khaled and Venkatesh, Akshay and Bureddy, Devendar and Panda, Dhabaleswar K.},
  year = {2013},
  month = oct,
  pages = {80--89},
  publisher = {{IEEE}},
  address = {{Lyon, France}},
  doi = {10.1109/ICPP.2013.17},
  url = {http://ieeexplore.ieee.org/document/6687341/},
  urldate = {2023-11-03},
  isbn = {978-0-7695-5117-3},
  file = {/Users/caspar/Zotero/storage/FGDQTQSN/Potluri et al. - 2013 - Efficient Inter-node MPI Communication Using GPUDi.pdf}
}

@misc{RefactoringOptimizingWRF,
  title = {Refactoring and {{Optimizing WRF Model}} on {{Sunway TaihuLight}} | {{Proceedings}} of the 48th {{International Conference}} on {{Parallel Processing}}},
  url = {https://dl.acm.org/doi/10.1145/3337821.3337923},
  urldate = {2023-05-10},
  file = {/Users/caspar/Zotero/storage/KMETDULS/Refactoring and Optimizing WRF Model on Sunway Tai.pdf;/Users/caspar/Zotero/storage/453T3UG2/3337821.html}
}

@inproceedings{romeroDistributedmemorySimulationsTurbulent2022,
  title = {Distributed-Memory Simulations of Turbulent Flows on Modern {{GPU}} Systems Using an Adaptive Pencil Decomposition Library},
  booktitle = {Proceedings of the {{Platform}} for {{Advanced Scientific Computing Conference}}},
  author = {Romero, Joshua and Costa, Pedro and Fatica, Massimiliano},
  year = {2022},
  month = jul,
  series = {{{PASC}} '22},
  pages = {1--11},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3539781.3539797},
  url = {https://dl.acm.org/doi/10.1145/3539781.3539797},
  urldate = {2023-06-02},
  abstract = {This paper presents a performance analysis of pencil domain decomposition methodologies for three-dimensional Computational Fluid Dynamics (CFD) codes for turbulence simulations, on several large GPU-accelerated clusters. The performance was assessed for the numerical solution of the Navier-Stokes equations in two codes which require the calculation of Fast-Fourier Transforms (FFT): a tri-periodic pseudo-spectral solver for isotropic turbulence, and a finite-difference solver for canonical turbulent flows, where the FFTs are used in its Poisson solver. Both codes use a newly developed transpose library that automatically determines the optimal domain decomposition and communication backend on each system. We compared the performance across systems with very different node topologies and available network bandwidth, to show how these characteristics impact decomposition selection for best performance. Additionally, we assessed the performance of several communication libraries available on these systems, such as Open-MPI, IBM Spectrum MPI, Cray MPI, the NVIDIA Collective Communication Library (NCCL), and NVSHMEM. Our results show that the optimal combination of communication backend and domain decomposition is highly system-dependent, and that the adaptive decomposition library is key in ensuring efficient resource usage with minimal user effort.},
  isbn = {978-1-4503-9410-9},
  keywords = {computational fluid dynamics,direct numerical simulation,GPU accelerated systems,parallel transpose},
  file = {/Users/caspar/Zotero/storage/XENEL2NG/Romero et al. - 2022 - Distributed-memory simulations of turbulent flows .pdf}
}

@article{schalkwijkHighPerformanceSimulationsTurbulent2012,
  title = {High-{{Performance Simulations}} of {{Turbulent Clouds}} on a {{Desktop PC}}: {{Exploiting}} the {{GPU}}},
  shorttitle = {High-{{Performance Simulations}} of {{Turbulent Clouds}} on a {{Desktop PC}}},
  author = {Schalkwijk, Jer{\^o}me and Griffith, Eric J. and Post, Frits H. and Jonker, Harm J. J.},
  year = {2012},
  month = mar,
  journal = {Bulletin of the American Meteorological Society},
  volume = {93},
  number = {3},
  pages = {307--314},
  issn = {1520-0477},
  doi = {10.1175/BAMS-D-11-00059.1},
  url = {https://journals.ametsoc.org/doi/10.1175/BAMS-D-11-00059.1},
  urldate = {2023-05-22},
  abstract = {No abstract available.},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/VI465BX5/Schalkwijk et al. - 2012 - High-Performance Simulations of Turbulent Clouds o.pdf}
}

@article{schumannFastFourierTransforms1988,
  title = {Fast {{Fourier}} Transforms for Direct Solution of Poisson's Equation with Staggered Boundary Conditions},
  author = {Schumann, Ulrich and Sweet, Roland A},
  year = {1988},
  month = mar,
  journal = {Journal of Computational Physics},
  volume = {75},
  number = {1},
  pages = {123--137},
  issn = {0021-9991},
  doi = {10.1016/0021-9991(88)90102-7},
  url = {https://www.sciencedirect.com/science/article/pii/0021999188901027},
  urldate = {2023-05-24},
  abstract = {This paper describes pre- and postprocessing algorithms used to incorporate the fast Fourier transform (FFT) into the solution of finite difference approximations to multi-dimensional Poisson's equation on a staggered grid where the boundary is located midway between two grid points. All frequently occurring boundary conditions (Neumann, Dirichlet, or cyclic) are considered including the combination of staggered Neumann boundary condition on one side with nonstaggered Dirichlet boundary condition on the other side. Experiences from implementing these algorithms in vectorized coding in Fortran subroutines are reported.},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/MZZ46CWY/Schumann and Sweet - 1988 - Fast Fourier transforms for direct solution of poi.pdf;/Users/caspar/Zotero/storage/PE69MKIZ/0021999188901027.html}
}

@article{schumannFastFourierTransforms1988a,
  title = {Fast {{Fourier}} Transforms for Direct Solution of Poisson's Equation with Staggered Boundary Conditions},
  author = {Schumann, Ulrich and Sweet, Roland A},
  year = {1988},
  month = mar,
  journal = {Journal of Computational Physics},
  volume = {75},
  number = {1},
  pages = {123--137},
  issn = {0021-9991},
  doi = {10.1016/0021-9991(88)90102-7},
  url = {https://www.sciencedirect.com/science/article/pii/0021999188901027},
  urldate = {2023-06-08},
  abstract = {This paper describes pre- and postprocessing algorithms used to incorporate the fast Fourier transform (FFT) into the solution of finite difference approximations to multi-dimensional Poisson's equation on a staggered grid where the boundary is located midway between two grid points. All frequently occurring boundary conditions (Neumann, Dirichlet, or cyclic) are considered including the combination of staggered Neumann boundary condition on one side with nonstaggered Dirichlet boundary condition on the other side. Experiences from implementing these algorithms in vectorized coding in Fortran subroutines are reported.},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/CVI4T8SZ/Schumann and Sweet - 1988 - Fast Fourier transforms for direct solution of poi.pdf;/Users/caspar/Zotero/storage/62JJFTLM/0021999188901027.html}
}

@article{siebesmaLargeEddySimulation2003,
  title = {A {{Large Eddy Simulation Intercomparison Study}} of {{Shallow Cumulus Convection}}},
  author = {Siebesma, A. Pier and Bretherton, Christopher S. and Brown, Andrew and Chlond, Andreas and Cuxart, Joan and Duynkerke, Peter G. and Jiang, Hongli and Khairoutdinov, Marat and Lewellen, David and Moeng, Chin-Hoh and Sanchez, Enrique and Stevens, Bjorn and Stevens, David E.},
  year = {2003},
  month = may,
  journal = {Journal of the Atmospheric Sciences},
  volume = {60},
  number = {10},
  pages = {1201--1219},
  doi = {10.1175/1520-0469(2003)60<1201:ALESIS>2.0.CO;2},
  url = {https://journals.ametsoc.org/view/journals/atsc/60/10/1520-0469_2003_60_1201_alesis_2.0.co_2.xml}
}

@article{smagorinskyGeneralCirculationExperiments1963,
  title = {General {{Circulation Experiments}} with the {{Primitive Equations}}},
  author = {Smagorinsky, Joseph},
  year = {1963},
  month = jan,
  journal = {Monthly Weather Review},
  volume = {91},
  pages = {99--164},
  doi = {10.1175/1520-0493(1963)091%3C0099:GCEWTP%3E2.3.CO;2}
}

@article{spiegel1960boussinesq,
  title = {On the {{Boussinesq}} Approximation for a Compressible Fluid.},
  author = {Spiegel, Edward A and Veronis, G},
  year = {1960},
  journal = {Astrophysical Journal, vol. 131, p. 442},
  volume = {131},
  pages = {442}
}

@misc{surfSnelliusHardwareFile,
  title = {Snellius Hardware and File Systems},
  author = {{SURF}},
  url = {https://servicedesk.surf.nl/wiki/display/WIKI/Snellius+hardware+and+file+systems}
}

@incollection{swarztrauberVectorizingFFTs1982,
  title = {Vectorizing the {{FFTs}}},
  booktitle = {Parallel {{Computations}}},
  author = {Swarztrauber, Paul N.},
  year = {1982},
  pages = {51--83},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-0-12-592101-5.50007-5},
  url = {https://linkinghub.elsevier.com/retrieve/pii/B9780125921015500075},
  isbn = {978-0-12-592101-5},
  langid = {english}
}

@article{tolmachevVkFFTAPerformantCrossPlatform2023,
  title = {{{VkFFT-A Performant}}, {{Cross-Platform}} and {{Open-Source GPU FFT Library}}},
  author = {Tolmachev, Dmitrii},
  year = {2023},
  journal = {IEEE Access},
  volume = {11},
  pages = {12039--12058},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2023.3242240},
  url = {https://ieeexplore.ieee.org/document/10036080/},
  urldate = {2023-10-03},
  file = {/Users/caspar/Zotero/storage/SRKCLCWM/Tolmachev - 2023 - VkFFT-A Performant, Cross-Platform and Open-Source.pdf}
}

@article{verzijlberghAtmosphericFlowsLarge2021,
  title = {Atmospheric Flows in Large Wind Farms},
  author = {Verzijlbergh, R.A.},
  year = {2021},
  journal = {Europhysics News},
  volume = {52},
  number = {5},
  pages = {20--23},
  issn = {0531-7479, 1432-1092},
  doi = {10.1051/epn/2021502},
  url = {https://www.europhysicsnews.org/10.1051/epn/2021502},
  urldate = {2023-05-31},
  abstract = {As we are transitioning to an energy system based on renewable sources, the atmosphere is becoming one of our primary energy sources. Understanding atmospheric flows through wind farms has become an issue of large economic and societal concern.},
  langid = {english},
  file = {/Users/caspar/Zotero/storage/H2QREFDG/Verzijlbergh - 2021 - Atmospheric flows in large wind farms.pdf}
}

@article{vremanEddyviscositySubgridscaleModel2004,
  title = {An Eddy-Viscosity Subgrid-Scale Model for Turbulent Shear Flow: {{Algebraic}} Theory and Applications},
  shorttitle = {An Eddy-Viscosity Subgrid-Scale Model for Turbulent Shear Flow},
  author = {Vreman, A. W.},
  year = {2004},
  month = oct,
  journal = {Physics of Fluids},
  volume = {16},
  number = {10},
  pages = {3670--3681},
  issn = {1070-6631, 1089-7666},
  doi = {10.1063/1.1785131},
  url = {https://pubs.aip.org/pof/article/16/10/3670/255289/An-eddy-viscosity-subgrid-scale-model-for},
  urldate = {2024-01-09},
  abstract = {An eddy-viscosity model is proposed and applied in large-eddy simulation of turbulent shear flows with quite satisfactory results. The model is essentially not more complicated than the Smagorinsky model, but is constructed in such a way that its dissipation is relatively small in transitional and near-wall regions. The model is expressed in first-order derivatives, does not involve explicit filtering, averaging, or clipping procedures, and is rotationally invariant for isotropic filter widths. Because of these highly desirable properties the model seems to be well suited for engineering applications. In order to provide a foundation of the model, an algebraic framework for general three-dimensional flows is introduced. Within this framework several types of flows are proven to have zero energy transfer to subgrid scales. The eddy viscosity is zero in the same cases; the theoretical subgrid dissipation and the eddy viscosity have the same algebraic structure. In addition, the model is based on a fundamental realizability inequality for the theoretical subgrid dissipation. Results are shown for a transitional and turbulent mixing layer at high Reynolds number and a turbulent channel flow. In both cases the present model is found to be more accurate than the Smagorinsky model and as good as the standard dynamic model. Unlike the Smagorinsky model, the present model is able to adequately handle not only turbulent but also transitional flow.},
  langid = {english}
}

@article{wangGPUAwareMPIRDMAEnabled2014,
  title = {{{GPU-Aware MPI}} on {{RDMA-Enabled Clusters}}: {{Design}}, {{Implementation}} and {{Evaluation}}},
  shorttitle = {{{GPU-Aware MPI}} on {{RDMA-Enabled Clusters}}},
  author = {Wang, Hao and Potluri, Sreeram and Bureddy, Devendar and Rosales, Carlos and Panda, Dhabaleswar K.},
  year = {2014},
  month = oct,
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  volume = {25},
  number = {10},
  pages = {2595--2605},
  issn = {1045-9219},
  doi = {10.1109/TPDS.2013.222},
  url = {http://ieeexplore.ieee.org/document/6587715/},
  urldate = {2023-11-02},
  file = {/Users/caspar/Zotero/storage/SUGQ3HW9/Wang et al. - 2014 - GPU-Aware MPI on RDMA-Enabled Clusters Design, Im.pdf}
}

@article{wilhelmsonDirectSolutionsPoisson1977,
  title = {Direct Solutions for {{Poisson}}'s Equation in Three Dimensions},
  author = {Wilhelmson, Robert B. and Ericksen, James H.},
  year = {1977},
  month = dec,
  journal = {Journal of Computational Physics},
  volume = {25},
  number = {4},
  pages = {319--331},
  issn = {00219991},
  doi = {10.1016/0021-9991(77)90001-8},
  url = {https://linkinghub.elsevier.com/retrieve/pii/0021999177900018},
  urldate = {2023-08-18},
  langid = {english}
}
