\section{Background}
The atmospheric boundary layer (ABL) is a highly turbulent flow consisting of a wide range of turbulent eddies (whirling motions). Solving the Navier-Stokes equations explicitly for all scales is called direct numerical simulation (DNS). However, because the scales of these eddies ranges from millimeters to tens of kilometers, DNS is computationally very expensive, making simulations of the ABL with a reasonable domain size unfeasable \citep{moengNUMERICALMODELSLargeEddy2015}. Instead of resolving all scales, one could settle on resolving only the largest, most energetic scales explicitly and model the smaller scales. Such a simulation is called large-eddy simulation (LES). The Dutch Atmospheric Large-Eddy Simulation (DALES) model developed by \citet{heusFormulationDutchAtmospheric2010} is an LES model for atmospheric applications that has proven itself in a variety of LES model inter-comparison studies.

While LES is computationally cheaper than DNS for a given domain size, it is still quite costly. Most of the computing time arises from loops that perform some calculation on all grid points, for example, calculating the derivative of a variable. Usually, these calculations are independent from one to another, which is a case of Single Instruction Multiple Data (SIMD). Graphics Processing Units (GPUs) are particularly well suited for SIMD applications; unlike Central Processing Units (CPUs), which consist of up to tens of processing cores, a modern GPU can have thousands of processing cores performing calculations in parallel \citep{elsterNvidiaHopperGPU2022}, offering huge potential speed up for applications. 

Moving calculations from CPUs to GPUs is no trivial exercise. A programmer has to modify their program such that it can take advantage of the GPU in a system. GPUs can be programmed in standard programming languages like C/C++, Fortran, Python, et cetera. To this end, specialized programming models are available that allow the programmer to manage a GPU. Examples of such programming models are: Compute Unified Device Architecture (CUDA) from NVIDIA, Open Computing Language (OpenCL) and Heterogeneous-Compute Interface for Portability from AMD. Using any of these involves rewriting the original CPU code into code that can run on GPUs, so called \emph{kernels} .

OpenACC provides a range of \emph{compiler directives}, which can be used to offload loops to the GPU. An example of such a loop, decorated with an OpenACC directive, can be found in \autoref{listing:accloop}. This directive-based approach offers multiple benefits over the traditional, lower-level programming models such as CUDA and OpenCL \citep{herdmanAcceleratingHydrocodesOpenACC2012}:

\begin{itemize}
    \item Productivity: OpenACC requires minimal addition of lines of code to an existing codebase. This makes it possible for a programmer to accelerate large sections of a program in a relatively short amount of time. 
    \item Single source code: CUDA and OpenCL require the programmer to rewrite existing code into ``\emph{compute kernels}'' that are executed on the GPU. If it is desired that a program is still able to run on CPU's, two versions of the same program have to be maintained: one version for CPU's and one version for GPU's. Needless to say, this is more error-prone than a single source code. As OpenACC directives are essentially just comments in the code, the programmer can add them to the existing CPU code. The same code can then be compiled for execution on both GPU's and CPU's, and the OpenACC directives will simply be ignored for the latter.
\end{itemize}

% As opposed to the Reynolds Averaged Navier Stokes (RANS) equations, which does not resolve any turbulence, and Direct Numerical Simulation (DNS), which resolves all turbulent scales, LES only resolves the largest turbulent scales which carry the most energy, while the small scales are parameterized \citep{masonLargeEddySimulationConvective1989}. 