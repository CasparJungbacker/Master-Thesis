\section{Related work}



The proposed work will not be the first attempt at accelerating an LES model with GPU's. In fact, DALES itself has been ported to GPU's before by \citet{schalkwijkHighPerformanceSimulationsTurbulent2012}. To this end, the original Fortran code of DALES was translated to C++ and calculations were moved to the GPU using NVIDIA's Compute Unified Device Architecture (CUDA), resulting in the GPU-resident Atmospheric Large-Eddy Simulation (GALES) model. \citet{schalkwijkHighPerformanceSimulationsTurbulent2012} found that GALES was able to reduce the wall clock time per time step by a factor of 2 compared to DALES, though it should be noted that GALES uses single precision floating point numbers in most parts of the simulation, while DALES uses double precision. GPU's are especially well tuned for single precision calculations. 

\citet{costaFFTbasedFinitedifferenceSolver2018} has developed a tool for DNS of turbulent flows, called CaNS (Canonical Navier-Stokes solver). The dynamical core of CaNS is very similar to that of DALES: both use finite-difference discretization on a structured, staggered grid, an FFT-based solver for the pressure and third-order Runge Kutta time integration. Unlike DALES, CaNS does not include a subgrid scale model. Parallelization of CaNS is achieved by domain decomposition into pencils along two directions, with further shared-memory parellelization via OpenMP.
CaNS was later adapted for GPU's using CUDA Fortran \citep{costaGPUAccelerationCaNS2021}. CUDA Fortran provides a Fortran interface to the CUDA programming model and includes CUF Kernels. CUF Kernels are compiler directives that can be placed above loops to tell the compiler that the loop can be executed on the GPU. NVIDIA's cuFFT library was used to perform the FFT calculations on GPUs. Performance analysis was done on two systems: an NVIDIA DGX Station, a system comparable in size to a modern desktop PC and containing 4 Tesla V100 32 Gb GPUs, and an NVIDIA DGX-2, a standard 19 inch server chassis containing 16 Tesla V100 32 Gb GPUs. \citet{costaGPUAccelerationCaNS2021} found that for the same problem size, one would need about 6100 to 11200 CPU cores to match the wall-clock time per time step of the 16 Tesla V100s in the NVIDIA DGX-2. This is still a conservative estimate, as linear scaling was assumed for the CPU code, whereas in reality, performance often scales sublinearly for a given problem size due to overhead introduced by communication between processes. For reference, the first phase of the new TU Delft DelftBlue supercomputer has around 11,000 CPU cores, indicating that GPUs can offer a siginificant reduction of energy consumption.

