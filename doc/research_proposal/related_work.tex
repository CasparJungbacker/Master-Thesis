\section{Related work}

\citet{niemeyerRecentProgressChallenges2014} have examined the status of GPU computing in the field of computational fluid dynamics (CFD). To demonstrate the potential benefits of using GPUs for CFD, two case studies were performed: a 2D Laplace equation, resembling the Poisson equation that is often found in CFD codes, and a lid-driven cavity flow. Four implementations were tested: single-core CPU, multi-core CPU with OpenMP, GPU with CUDA and GPU with OpenACC. For mesh sizes up to $512^2$, the wall-clock time for the GPU implementations exceeded that of the multi core CPU implementation. While the authors do not explicitly explore the possible causes of this behavior, it can be argued that the increase in wall-clock time is due to data transfer between the CPU and GPU. For larger mesh sizes, the GPU implementations outperformed the CPU implementations. Specifically, for the Laplace equation, the GPU solver showed a speedup of about 4.6, while for the lid-driven cavity flow, the speedup was about 2.8. Remarkably, \citet{niemeyerRecentProgressChallenges2014} showed that as the mesh size increases, the wall-clock time of the OpenACC implementation converged to that of the CUDA implementation.

DALES itself has been ported to GPU's before by \citet{schalkwijkHighPerformanceSimulationsTurbulent2012}. To this end, the original Fortran code of DALES was translated to C++ and calculations were moved to the GPU using CUDA, resulting in the GPU-resident Atmospheric Large-Eddy Simulation (GALES) model. \citet{schalkwijkHighPerformanceSimulationsTurbulent2012} found that GALES was able to reduce the wall-clock time per time step by a factor of 2 compared to DALES, although it should be noted that GALES uses single precision floating point numbers in most parts of the simulation, while DALES uses double precision. This is an important distinction to make, as GPUs are particularly well-optimized for single precision floating point arithmetics. Since then, the company Whiffle has adopted GALES and further developed it into the GPU-Resident Atmospheric Simulation Platform (GRASP). GRASP is often used for very accurate simulations of windfarms

\citet{costaFFTbasedFinitedifferenceSolver2018} has developed a tool for DNS of turbulent flows, called CaNS (Canonical Navier-Stokes solver). The dynamical core of CaNS is very similar to that of DALES: both use finite-difference discretization on a structured, staggered grid, an FFT-based solver for the pressure and third-order Runge Kutta time integration. Unlike DALES, CaNS does not include a subgrid scale model. Parallelization of CaNS is achieved by domain decomposition into pencils along two directions, with further shared-memory parellelization via OpenMP.
CaNS was later adapted for GPU's using CUDA Fortran \citep{costaGPUAccelerationCaNS2021}. CUDA Fortran provides a Fortran interface to the CUDA programming model and includes CUF Kernels. CUF Kernels are compiler directives that can be placed above loops to tell the compiler that the loop can be executed on the GPU. NVIDIA's cuFFT library was used to perform the FFT calculations on GPUs. Performance analysis was done on two systems: an NVIDIA DGX Station, a system comparable in size to a modern desktop PC and containing 4 Tesla V100 32 Gb GPUs, and an NVIDIA DGX-2, a standard 19 inch server chassis containing 16 Tesla V100 32 Gb GPUs. \citet{costaGPUAccelerationCaNS2021} found that for the same problem size, one would need about 6100 to 11200 CPU cores to match the wall-clock time per time step of the 16 Tesla V100s in the NVIDIA DGX-2. This is still a conservative estimate, as linear scaling was assumed for the CPU code, whereas in reality, performance often scales sublinearly for a given problem size due to overhead introduced by communication between processes. For reference, the first phase of the new TU Delft DelftBlue supercomputer has around 11,000 CPU cores, indicating that GPUs can offer a siginificant reduction of energy consumption.



CFD codes often contain parallel problems: a certain piece of work on some dataset can be split up in multiple tasks, each working on an independent piece of the dataset. A good example of a parallel problem is a finite difference stencil: the finite difference in every grid point can be calculated independent of the finite difference in all other grid points. 